(Intro SFX)

Okay, what exactly is happening with the iPhone's camera? We've conducted years of blind smartphone camera tests, and the iPhone, supposedly one of the premium cameras in the entire smartphone industry, consistently loses in the first round. Then, we do a scientific version with over 20 million votes, and it finishes in the middle of the pack. "And yet, Marques, you named it the best overall smartphone camera system for the fourth time running in 2022 and gave it a trophy. What's up with that?" A concerning number of people have started to notice that the iPhone’s camera feels like it has taken a few steps back lately, and I agree with them. I think we should take a closer look at this.

(relaxed music)

First of all, cameras have come a long way. Smartphone cameras aren't just cameras anymore. Back in the day, a camera was basically a sensor that would travel around covered all the time. When you wanted to take a photo, you would expose that sensitive bit to the environment around it, collecting light. The photo would then represent how much light hit each part of the sensor. The better the sensor, the better the image, and the more light information you could capture—super simple.

These days though, it's turned into a whole computational event. Your smartphone sensor is sampling the environment not just once, but often several times in rapid succession at different speeds. It's merging exposures together, doing tone mapping, noise reduction, and HDR processing—all to create what it thinks is the best-looking image. This, of course, is a very different definition of a picture. Now, it’s not just about having the best sensor that gathers the most light information; software makes a much bigger difference in how the image looks at the end of the day. 

Next time you watch a smartphone reveal event, take note of all the new additions, and notice how many of them are pure software. Google really struck gold when they first started using the IMX363 sensor with the Pixel 3's camera. Their software tuning was just right, making it an instant smash hit. They continued using that camera combo in every subsequent Pixel model: the 3, 3a, 4, 4a, 5, 5a, and even the Pixel 6a. Year after year, same sensor, same software tuning combo—if it ain’t broke, don’t fix it.

So when the Pixel 6a won December's scientific blind smartphone camera test, it was with a four-year-old sensor and software tuning combination that was still performing exceptionally well. In side-by-side comparisons of compressed images, where you can't really judge sharpness or depth of field much, this combo nailed the basics better than anyone else.

When the Pixel 6 came along, Google updated their design and branding and changed to a new sensor with this new camera system, moving from the tried-and-true 12-megapixel to a massive new 50-megapixel sensor. This change threw a wrench into things.

"It looks to me like the Pixel is over-sharpening. The one on the left looks too crunchy." The camera on the Pixel 6 does have a habit of making things look overly HDR. I'm not sure if there’s a technical term for that. "If you look at all the photos, it's clear the Pixel is still doing Pixel things." I think Google is still running all their camera algorithms at maximum, even when they don’t need to anymore.

New phones with much larger sensors are still processing images like their smaller predecessors. The basic principle is that they were doing all this processing with old sensors as if they weren’t getting much light, but suddenly they had this massive new sensor that could capture way more light. Yet, they continued running all the same processing. They would do high-sensitivity processing and then apply noise reduction, but since noise reduction requires sharpening afterward, the photos ended up overprocessed. 

This fancy new phone comes out with a new camera system, but you could argue that the older Pixel still took better-looking photos. Google had to work hard behind the scenes to make some adjustments to the software and dial in this new sensor. It took a while, but now with the Pixel 7 out, a full year later with the same huge 50-megapixel sensor, they’re back on track. 

Interestingly, the Pixel 7 finished just behind the Pixel 6a in the blind camera test. When I see iPhone 14 Pro photos looking inconsistent and a bit overprocessed, I spot a lot of the same issues that Google faced with the Pixel. The iPhone story is somewhat similar. They used a small 12-megapixel sensor for years, then the 13 Pro sensor got slightly bigger, but this year, the iPhone 14 Pro is the first to bump up to a significantly larger 48-megapixel sensor. As a result, some iPhone photos are looking a little too processed. It’s not extreme, but it's noticeable, and they’ll have to work on this.

I suspect that by the time we get to the iPhone 15 Pro, they will have new software adjustments. I imagine there will be some new wording they use on stage—like they introduced Deep Fusion and pixel-binning—perhaps to explain some software improvements to the camera. I think this will keep improving with software updates over time, and they’ll get it dialed in. I believe it will be fine.

However, this doesn’t explain why all the previous 12-megapixel iPhones also lost in the first round of our tests. This is a separate issue that I’m a bit more curious about. As you may recall, all of our testing photos featured me intentionally. We designed the tests to incorporate as many potential factors as possible to judge a photo. If it were just a picture of a figurine in front of a plain background, the winner would likely be just whichever one was brighter or had better color. 

But with the figurine placed in a more complex background, we’re judging both color and background blur. Add a sky to the mix, and now we’re also testing dynamic range and HDR. Our latest photo included lots of factors: two different skin tones, two differently colored shirts, textures for sharpness, a sky for dynamic range, short-range falloff on one side, and long-range falloff on the other. With all these factors, whichever photo people pick as the winner could be seen as closer to the best overall photo.

I also wanted the pictures to feature humans since the most important photos people typically take are of other humans. Using my own face revealed a lot about how different smartphones handle photographing a human face. As I’ve already mentioned, smartphone cameras rely heavily on software, meaning the photo you get when you hit the shutter isn’t so much a capture of reality as it is a computer's best interpretation of what it thinks you want reality to look like. 

Each company has different ways of optimizing their pictures. They used to be more transparent about it. Some phones would identify when you were taking a landscape photo and enhance the greens in the grass or boost the blues in a sky shot to make it look nicer. I did a video on smartphone cameras versus reality, which I’ll link below the Like button if you want to check it out. But the point is, when you snap a photo, you’re not always getting an accurate capture of what was in front of you—many adjustments are being made.

The iPhone, for instance, identifies faces and tries to evenly light them. It tries every time. This seems pretty harmless, right? If you ask people what they think should look good in a photo, the idea of evenly lighting all the faces sounds reasonable. Often, it ends up looking fine, but it can lead to issues. For example, in our low-light photo test from the blind camera test, the Pixel 7 on the left looks standard, while the iPhone 14 Pro, which finished in the middle of the pack, displays some oddities. The iPhone completely removed the shadow from half of my face. 

I am clearly lit from a side source, but in the iPhone’s interpretation, it’s impossible to tell where the light is coming from. Sometimes, you encounter bizarre results like this, which can be traced back to software choices. Another issue is skin tones. I’ve mentioned that I often prefer photos from the Pixel’s camera, and we’ve done plenty of tests featuring me. The results look great on the Pixel.

Google has implemented a feature called Real Tone in the Pixel camera over the past few years. Although it doesn’t get much attention, it makes a significant difference. Historically, film cameras tended to be calibrated for lighter skin tones, leaving people with darker skin underexposed. Today, smartphone cameras are software-based, meaning they can make adjustments for various skin tones, but they do this to varying degrees. 

You might have noticed that many phones sold in China brighten faces uniformly since that’s a popular aesthetic in that region. Google goes the extra mile to train their camera software with diverse datasets that accurately represent various skin tones. Apple’s cameras, from what I’ve observed, tend to just aim to evenly light faces without accounting for the different white balances and exposures necessary for accurately representing diverse skin tones.

Essentially, this is a substantial factor in the Pixel’s success, with many smartphones that accurately depict my skin tone finishing higher in our blind voting. People consider how well a camera performs with different tones when casting their votes. I haven’t mentioned this often, but one reason I’ve really liked RED cameras is their accurate representation of skin tones compared to other brands like Sony, ARRI, and Canon.

All of this software complexity explains why photo comparisons between modern smartphones are so challenging. There are many channels that conduct side-by-side photo tests quite well, but as you try to pick one over another, you might prefer one phone's landscape rendering over another’s, while a different phone does a superior job with your own skin tone or perhaps your pet’s photo.

I’m sure Apple will defend all their current camera capabilities, but I’m also certain they’re working to fine-tune these new cameras and improve them with the iPhone 15 and 15 Pro.

Now, back to the original question we set out to answer: "Marques, you like the Pixel photos, the Pixel 6a won the blind scientific camera test, but you still awarded best overall camera system to the iPhone 14 Pro. Why?" If you listen closely, you’ve already gotten the answer. That scientific test measured one specific thing: small postage-stamp-sized images and general color metrics, but sharpness and detail under compression weren’t assessed. 

We didn’t test the speed or reliability of autofocus either, or the open-close time of the camera app, or the speed and reliability of capturing a shot. Video quality wasn’t tested either, including microphone quality, video sharpness, HDR, and all of that. Maybe someday we’ll test those factors comprehensively, but for now, the lesson learned is that the visually appealing photos that come from the Pixel—or whatever phone you're using—are composed of photons but are primarily the result of processing.

(relaxed music)

Thanks for watching. Catch you guys in the next one. Peace.

(record crackling)