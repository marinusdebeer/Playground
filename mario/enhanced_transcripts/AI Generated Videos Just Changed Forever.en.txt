- [Will] That's hot.

(bright upbeat music)

- All right, so this is simultaneously really impressive and really frightening. It's hitting me in ways that I didn’t really expect. Do you remember Will Smith eating spaghetti? Back when we said, “Okay, this AI stuff is cool and all, but clearly there’s a long way to go before there’s any need for concern”? Well, welcome to the future, people, because this video is also AI-generated. So is this one—completely synthesized out of thin air by computers. And this one too—this is not real. It’s absolutely ridiculous how far we’ve come in just one year. This feels like another ChatGPT, DALL·E moment for AI. 

Maybe I’m overreacting because, okay, I am a video creator, so an AI that’s actually doing my job feels a bit more threatening; I’m particularly impressed by it. But also, this stuff is really good. Today, Sam Altman and OpenAI announced a new model called Sora, and it can generate full video clips up to one minute long from just text input. Just like DALL·E was able to understand our text input and turn it into photo-realistic or stylized images, Sora does the same with videos. Now, since it’s handling videos, it must also understand how reflections, textures, materials, and physics interact over time to create a believable video. 

Right away, there are a bunch of crazy examples on their website. Before I show you these, I need you to keep this in mind: you’re about to watch a bunch of AI-generated videos, and you know you’re watching AI-generated content. Your brain will already be looking for imperfections, and while they exist, not everyone who sees AI-generated content online is looking for those flaws. Also, this is the worst that this technology will be from here on out.

Okay, here’s one of the videos. There’s no audio to any of these clips, but the prompt for this one is: “A stylish woman walks down a Tokyo street filled with warm, glowing neon and animated city signage. She wears a black leather jacket, a long red dress, and black boots.” This video is already miles ahead of where we were. It has accurate lighting, materials, skin tones, movements, and even reflections everywhere. Of course, if you scrutinize it closely for more than about ten seconds, you’ll find lots of giveaways. For instance, a guy in the background seems to be gliding oddly. The frame rates and the reflections in the water are lower than in the rest of the video, and the camera movement feels a bit inconsistent. Overall, everything just feels a little off—yet just keep in mind how far we’ve come in just a year.

How about this one? This is another video with a long prompt about a camera following behind a white vintage SUV with a black roof rack as it speeds up a steep dirt road. This one is also really good. It looks a bit more video game-like due to the rock-solid drone footage, but it's clearly usable.

Here’s another one featuring a litter of golden retriever puppies playing in the snow. Their heads pop in and out of the snow, and it’s so good. The physics of the fur, the ears, and the snow flying around in slow motion are incredible. I’ve gone through all the sample videos on OpenAI’s website, and clearly, these are the handpicked best ones they chose to share, where they just input text and then got a video without any modifications. But there are some genuinely impressive results, both realistic and stylized, some featuring people and some not. A lot of them are in slow motion. I have to say, the speed at which these models are improving is genuinely shocking. 

Just months ago, DALL·E 3 produced really high-end results, but you could still find something off about it. For example, it struggled with realistic human features—something about hands or ears always seemed a little off; not to mention the physics. But even this video is insane at first glance. The prompt for this AI-generated video is: “A young man in his 20s is sitting on a piece of cloud in the sky reading a book.” This one feels about 90% of the way there for me. It’s beyond the uncanny valley of say, Apple’s personas, which are based on real humans. This is a made-up person. His eyes look somewhat strange, the motion of the pages in the book is odd, and yes, he’s sitting on a cloud, which is a giveaway. But the lighting, shadows, skin tones, and the realism of the textures on his shirt and the way his shirt, pants, and hair move are all very impressive.

For another example, they typed in a movie trailer featuring the adventures of a 30-year-old spaceman wearing a red wool motorcycle helmet against a blue sky and salt desert in a cinematic style, shot on 35mm film. The close-ups of his face, the fabrics of the helmet, the film grain—this is one of the most convincing AI-generated videos I’ve ever seen, minus maybe the strange physics with the character walking in fast motion. 

If you follow Sam Altman on Twitter, he’s been posting more videos generated from people's requests. So if you want to see more, check out his profile. Here’s the thing about these AI-generated videos: as good as they’ve gotten, they can and will pass as real videos to people who are not looking for AI content. This is obviously sketchy during an election year in the U.S. and terrifying for a variety of other reasons, but it’s also perfect for stock footage. There are all kinds of presentations, advertisements, and PowerPoints that desperately need oddly specific stock videos, and these AI-generated clips are good enough to 100% pass for that purpose. 

Take this video of waves at Big Sur, for example. If I saw this on Twitter, I wouldn’t even think twice; I’d just think, “Oh, nice drone shot.” I wouldn’t suspect it was generated by AI unless I scrutinized the way the water was moving. It’s perfectly usable in an ad for a California-based product, which has significant implications for drone pilots who may no longer need to be hired and photographers and videographers whose footage no longer requires licensing for that ad being produced. It’s already that impressive.

There are other visuals, like this wall of TVs, which would be expensive and complicated to shoot with a camera and all those costly props. But if you could generate it with reflections and the surrounding environment this well, why wouldn’t you? It’s also capable of producing historical-themed footage. For example, this is AI-generated footage of California during the Gold Rush. It could easily pass for the opening scene of an old Western with the right music overlay.

How long will it be until an entire ad consists of completely AI-generated shots? Or an entire YouTube video? Or even a whole movie? I’m tempted to say we’re a long way from that because these still have noticeable flaws, and they lack sound. There’s still a long way to go with prompt engineering to sort these issues out. But then again, the spaghetti video was just a year ago.

OpenAI also showcases some of the shortcomings of this model, and who would know better than the people using it? It's a very private tool at the moment, with limited access only for testers—essentially red teamers pushing the limits—and a few trusted creators. They’ve found some strange quirks in certain videos. For instance, this clip of gray wolf pups looks normal initially, but it’s evident something is off as they appear from nowhere and walk through each other. 

Then there’s this bizarre clip of a guy running on a treadmill. I don’t even need to elaborate on why this is odd.

But here's my favorite example. Just imagine you’re scrolling through Facebook or Twitter, not expecting to see AI content, and you suddenly see this video of a grandma celebrating her birthday. You might wonder, “I wonder what birthday she’s celebrating. How old do you think she is? 60? 65? Maybe it’s the big 70.” She seems to really enjoy that cake. Now, did you catch that? I’ll play it again, but this time, pay attention, knowing that AI-generated photos and videos often struggle with hands. As I replay it, you’re likely to notice the inconsistency. Watch it multiple times, and you’ll see different sets of hands, making it feel weirder every time. The inconsistencies just become more pronounced. Not to mention the strange variations in the wind’s direction on the candles.

Even as I point all this out, I can’t help but remember that just twelve months ago, we were critiquing much less sophisticated outputs.

So what does all this mean? Well, there’s the current impact and what it implies for the future. Sora, the tool they’ve developed, is clearly an impressive AI video generation tool. It will both deceive viewers and be incredibly useful. Every video it generates has a watermark in the bottom corner, serving as a clear indicator that it’s an AI product—if it hasn’t been cropped out, of course. 

Yet, I think they’ll need to exercise extreme caution with this technology. They’ll have to implement stringent safety measures, perhaps even more so than with DALL·E. For instance, you shouldn’t be able to generate someone’s likeness without their consent, especially this election year. You probably won’t be able to recreate something like Will Smith eating spaghetti, but it definitely means there will be a significant impact on stock video licensing. Logistically, why would anyone producing content pay for footage of a house on the cliffs when they can generate one for free or for a small subscription fee? That is the real concern that this tool raises.

Looking ahead, it gets existential. If this model is trained on all videos ever created by humans, can it be innovative or creative in ways humans haven't already achieved? I don't know. Regardless, I’ll include all the relevant links below for Sora and OpenAI’s work. I guess I’ll talk to you next year when we look back and think, “Remember that first version of Sora?” Just keep in mind, this is the worst this technology will be from now on. 

Thanks for watching. Catch you in the next one. Peace.

(bright upbeat music)