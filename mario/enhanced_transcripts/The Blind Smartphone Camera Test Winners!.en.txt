(upbeat music)  
(logo whirring)  

- All right, everyone, you have waited patiently for this. Once again this year, we ran our blind smartphone camera test, scientific style. We always think we know what the absolute best smartphone cameras are, but do we actually know? What if we wanted an objective answer straight from real-world data? Every time I review a phone, there are always comments saying, "iPhone photos are the only good ones," or, "I only like Pixel photos," or, "Samsung is the only one who does good portrait mode." But what if we put that to the test? What if we took the best smartphone cameras on the planet, took them out into the real world, and captured the exact same photo with each one, stripped of all their labels, and had you, the public, vote on which photos you liked best? That should reveal the best actual smartphone camera.

So that’s exactly what we did. This year, we took 20 smartphones, updated all their software, charged them all up, and got their batteries to 100%. We then took the same three photos with each one: one daytime photo, one low-light photo, and one portrait-mode photo. Yes, it is surprisingly hard to stay perfectly still for 20 identical photos taken in a row, but I did it for you. Then we had to import all the photos from every single smartphone camera. This was harder than you think. We stripped them of all their EXIF data and uploaded them to a site we built that lets us display them side by side for you to vote on head-to-head, millions and millions of times, which you did. This should give us a statistically significant winner—and a loser. We’ll get to that.

The results this year were kind of interesting. I almost didn’t make this video, but people have been waiting, the competitive tension is high, and the actual results might be surprising to some of you. So let’s get into it. 

The data from our site comes in text files with matchup ratings for every single combination of smartphones. We actually coded them with letters of the alphabet, from A through T. That little number you see is the most important—it's the Elo rating, the system that ranks chess players, tennis players, and basically anything head-to-head. With enough matchup information, we can sort through all of these letters based on who they beat and who they lost to, creating a power ranking that will give us our winners.

So, you probably want to know who the winners are. We'll go category by category, since we had three different scenarios: daytime, low light, and portrait mode. Let’s start with daytime photos. We had it set up with me sitting in front of a window, but there’s a lot more to this photo. The strongest light comes from inside the room, while the window light is a good test of dynamic range. There’s also a variety of colors—my skin tone, the orange chair next to the blue pillow, and a few other things. This setup was intentional; there’s no individual variable that can dominate the test. 

For every phone, we tried to frame the shot the same way from the same spot, using one phone as a reference to get as close as possible. Everything was fully auto; we just opened the camera app, made sure the lens was clean, and hit the shutter button. We didn’t even tap to focus—just point and shoot. It takes about six minutes to go through all 20 phones. Like I said, it’s kind of hard to sit still for six minutes straight, but we did it.

So, for this regular daylight photo scenario, the winner with the highest Elo rating is the Pixel 7A. That’s pretty impressive! This photo was voted as the winner in most of its matchups. It’s a pretty neutral photo—not too bright, not too dark—and it has excellent dynamic range. This wasn’t a fluke, as the second-highest Elo rating came from the Pixel Fold, which took a very similar photo. Finishing up on the podium was the OnePlus Open, one of those high-end folding phones. My personal favorite photo, which actually won for me in my blind test, was the OnePlus 11. It’s definitely a bit contrasty, but it confidently nails the exposure and dynamic range.

A lot of people pointed out that the sun behind me looked a little different in each photo. It might have been setting, which impacted the background a bit, but it wasn’t late enough in the day for that to affect the photo. The fact that the 7A has this little lens flare is just a coincidence. Fun fact: dead last, with the absolute lowest Elo rating for this daytime photo, is the iPhone 15 Pro. It’s not a terrible photo; you can’t really get a horrible photo in normal-looking lighting with most smartphones. However, up against the others, it’s mostly just the darkest one. People chose the brighter photos almost every single time. 

Interestingly, you can absolutely overexpose for this competition, as the second-lowest Elo rating belongs to the Galaxy S23 Ultra, which produced an overexposed masterpiece. Here are all the photos side by side with their Elo ratings. Feel free to pause if you’d like to make more observations, but let’s move on to the nighttime photos.

Low-light photos are much more challenging for a smartphone camera, especially with a tiny sensor and optics trying to take in as much light as possible. However, computational photography has come a long way. Even in this pretty dark rooftop setting where the only light source was about 40 feet away, these cameras still managed to perform well. Here’s the plot twist: the highest overall Elo rating for the low-light photo comes from the same phone that finished dead last in daylight—the iPhone 15 Pro, with another decent photo. 

Following closely in second place was the Pixel 8 Pro, with the Pixel 7A coming in third. To be honest, I agree with these rankings. Some smartphone cameras started to overdo the light, so outside of the top four, you dive straight into bad HDR territory. The fifth-place photo, from the S23 Ultra, is definitely too bright and shows an HDR halo effect around my head. When we look down at the Oppo Find X6 Pro’s photo, it seems as if they literally painted over me and dragged up the exposure. The Zenfone did something similar, along with a few others. 

The OnePlus Open not only made me brighter, but it also darkened the sky, creating a ridiculous effect—real life looked nothing like that. Here are all the nighttime photos with their Elo ratings. Pause if needed. 

Now let’s move to the last but not least category: portrait mode. Portrait mode is often the hardest test to run because different cameras handle it differently. Some do 2X, some do 3X, and some say 1X but are actually around 1.5X; it’s confusing. We tried to keep it simple by switching to portrait mode and adjusting our distance to match the focal length. This worked to some degree, but the bokeh levels varied and some are adjustable while others aren’t. We used the default settings, which most people would choose. 

The portrait-mode winner with the highest Elo rating is the Pixel 8 Pro. It was closely followed by the Samsung Z Fold 5, which I think had a better cutout. The third-highest Elo was the iPhone 15 Pro, which showed the most natural-looking blur. All of these podium finishers have relatively subtle portrait modes, with good detail and balance in the rest of the photo. 

In contrast, the losers in this category produced some of the weirdest photos in the entire competition. The absolute lowest Elo rating for portrait mode—and indeed for any category—was the Sony Xperia 1 V with a photo that looked like a mistake. We genuinely thought it was a mistake while taking it. We cleaned the lens and took it again, but it kept happening. For whatever reason, this $1,200 Android flagship in auto portrait mode just could not handle the lighting and had some strange issues.

Here are all of the portrait-mode photos with their Elo ratings. You’re welcome. Now, I’d like to present some awards. We have all of our matchup data, voting results, and Elo ratings from over 20 million total votes—pretty interesting stuff. We have our winners and losers in each category, which is great. But if we average it all together, we get the highest overall average Elo rating, which we’ll call the People’s Champ. With the highest average of 1,660, the Pixel 7A claimed this title. 

This phone not only won the daytime photo but also came in third in low light and fourth in portrait mode out of all 20 phones. That’s pretty strong. Here’s the kicker: the second-highest average Elo rating was the Pixel 8 Pro, followed by the Pixel Fold in third. So, it’s a perfect Pixel podium!

Here’s the entire list from top to bottom, with the three Pixels at the top in reverse price order, surprisingly. Bringing up the rear in total Elo is the Sony Xperia 1 V and the Xiaomi 13 Ultra. I also brought back the Bang for the Buck Award, given to the most votes per MSRP at launch. The winner for this is again the Pixel 7A. This phone launched around $500 and performed excellently in tests. 

In second for votes per dollar is the Nothing Phone 2, a solid upper mid-range phone. In 3A and 3B, we have the OnePlus 11 and the Zenfone 10. Surprisingly, the Sony Xperia didn’t finish dead last in this category; while it was expensive and performed poorly, it did better bang for buck than the more expensive folding phone, the Z Flip 5.

So, what did we learn? First of all, the Pixels dominated with a one-two-three finish and are great for bang for buck. It was odd that they were in reverse price order, but that’s fine. There’s a lot more to a smartphone camera than just a staged photo. We learned a bit about how it captures my skin tones, but there are countless factors to consider—auto-focus speed, UI, file format, editability, video quality, etc. At the end of the day, it wasn’t shocking that slightly brighter photos beat slightly less bright ones. 

Maybe we’ll run this test again next year and learn something new. Either way, thanks for watching! Catch you next time. Peace.  
(upbeat music)