And I don't think you can just, I don't think we'll have the luxury to see humans as disjoint from the technology we've created for much longer. We are an organism that's... Yeah, I mean I agree with you but we come really with this to consciousness. And is there a distinction there? Because what you're saying, the natural end point says we are indistinguishable. That if you are capable of building an AI which is sufficiently close and similar that we merge with it, then to all intents and purposes, that AI is conscious as we know it. And I don't have a strong view but I have a view and I wrote about it in the epilogue to my last book because 10 years ago, I wrote a chapter in a book called Life Ascending about consciousness. And the subtitle of Life Ascending was the 10 Great Inventions of Evolution and I couldn't possibly write a book with a subtitle like that that did not include consciousness and specifically consciousness as one of the great inventions. And it was in part because I was just curious to know more and I read more for that chapter. I never worked on it but I've always, how can anyone not be interested in the question? And I was left with the feeling that A, nobody knows and B, there are two main schools of thought out there with a big kind of skew in distribution. One of them says, oh, it's a property of matter. It's an unknown law of physics, panpsychism. Everything is conscious. The sun is conscious. It's just a matter or a rock is conscious. It's just a matter of how much. And I find that very unpersuasive. I can't say that it's wrong. It's just that I think we somehow can tell the difference between something that's living and something that's not. And then the other end is it's an emergent property of a very complex central nervous system and I never quite understand what people mean by words like emergence. I mean, there are genuine examples but I think we very often tend to use it to plaster over ignorance. As a biochemist, the question for me then was, okay, it's a concoction of a central nervous system. A depolarizing neuron gives rise to a feeling, to a feeling of pain or to a feeling of love or anger or whatever it may be. So what is then a feeling in biophysical terms in the central nervous system? Which bit of the wiring gives rise to... And I've never seen anyone answer that question in a way that makes sense to me. And that's an important question to answer. I think if we want to understand consciousness, that's the only question to answer. Because certainly an AI is capable of out thinking and it's only a matter of time. Maybe it's already happened. In terms of just information processing and computational skill, I don't think we have any problem in designing a mind which is at least the equal of the human mind. But in terms of what we value the most as humans, which is to say our feelings, our emotions, our sense of what the world is in a very personal way, that I think means as much or more to people than their information processing. And that's where I don't think that AI necessarily will become conscious because I think it's the property of life. Well, let's talk about it more. You're an incredible writer, one of my favorite writers. So let me read from your latest book, Transformers, what you write about consciousness. I think therefore I am, said Descartes, is one of the most celebrated lines ever written. But what am I exactly? An artificial intelligence can think too by definition and therefore is, yet few of us could agree whether AI is capable in principle of anything resembling human emotions, of love or hate, fear and joy, of spiritual yearnings for oneness or oblivion, or corporeal pangs of thirst and hunger. The problem is we don't know what emotions are, as you were saying. What is the feeling in physical terms? How does a discharging neuron give rise to a feeling of anything at all? This is the heart problem of consciousness, the seeming duality of mind and matter, the physical makeup of our innermost self. We can understand in principle how an extremely sophisticated parallel processing system could be capable of wondrous feats of intelligence, but we can't answer in principle whether such a supreme intelligence would experience joy or melancholy. What is the quantum of solace? Speaking to the question of emergence, there's just technical, there's an excellent paper on this recently about this kind of phase transition emergence of performance in neural networks on the problem of NLP, natural language processing. Language models. There seems to be this question of size. At some point, there is a phase transition as you grow the size of the neural network. The question is, this is somewhat of a technical question that you can philosophize over. The technical question is, is there a size of a neural network that starts to be able to form the kind of representations that can capture a language, and therefore be able to not just language, but linguistically capture knowledge that's sufficient to solve a lot of problems in language, like be able to have a conversation. There seems to be not a gradual increase, but a phase transition. They're trying to construct the science of where that is. What is a good size of a neural network, and why does such a phase transition happen? Anyway, that sort of points to emergence, that there could be stages where a thing goes from being, oh, you're a very intelligent toaster, to a toaster that's feeling sad today and turns away and looks out the window, sighing, having an existential crisis. I was thinking of Marvin, the paranoid android. Well, no, Marvin is simplistic, because Marvin is just cranky. Yes, so easily programmed. Yeah, easily programmed, nonstop existential crisis. You're almost basically, what is it, notes from underground by Dostoevsky. It's just constantly complaining about life. No, they're capturing the full rollercoaster of human emotion, the excitement, the bliss, the connection, the empathy, and all that kind of stuff, and then the selfishness, the anger, the depression, all that kind of stuff. They're capturing all of that and be able to experience it deeply, like it's the most important thing you could possibly experience today. The highest highs, the lowest lows, this is it. My life will be over. I cannot possibly go on. That feeling, and then after a nap, you're feeling amazing. That might be something that emerges. So why would a nap make an AI being feel better? First of all, we don't know that for a human either, right? But we do know that that's actually true for many people much of the time. Maybe you're utterly depressed and you have a nap and you do, in fact, feel better. Oh, you are actually asking the technical question there. So there's a biological answer to that. And so the question is whether AI needs to have the same kind of attachments to its body, bodily function and preservation of the brain's successful function, self-preservation, essentially, in some deep biological sense. I mean, to my mind, it comes back around to the problem we were talking about before, about simulations and sensory input and learning what all of this stuff means. And life and death, that biology, unlike society, has a death penalty over everything. And natural selection works on that death penalty, that if you make this decision wrongly, you die. And the next generation is represented by beings that made a slightly different decision on balance. And that is something that's intrinsically difficult to simulate in all this richness, I would say. So what is... Death in all its richness, our relationship with death, or the whole of it. So when you say richness, of course, there's a lot in that, which is hard to simulate. What's part of the richness that's hard to simulate? I suppose the complexity of the environment and your position in that, or the position of an organism in that environment, in the full richness of that environment over its entire life, over multiple generations, with changes in gene sequence over those generations, so slight changes in the makeup of those individuals over generations. But if you take it back to the level of single cells, which I do in the book, and ask, how does a single cell, in effect, know it exists as a unit, as an entity? I mean, no, in inverted commas, obviously, it doesn't know anything. But it acts as a unit, and it acts with astonishing precision as a unit. And I had suggested that that's linked to the electrical fields on the membranes themselves, and that they give some indication of how am I doing in relation to my environment as a kind of real-time feedback on the world. And this is something physical, which can be selected over generations, that if you get this wrong, it's linked with this set of circumstances that I've just... As an individual, I have a moment of blind panic and run. As a bacterium or something, you have some electrical discharge that says blind panic, and it runs, whatever it may be. And you associate over generations, multiple generations, that... But this electrical phase that I'm in now is associated with a response like that. And it's easy to see how feelings come in through the back door almost with that kind of giving real-time feedback on your position in the world in relation to how am I doing. And then you complexify the system and, yes, I have no problem with phase transition and I, you know, can all of this be done purely by the language, by the issues with how the system understands itself? Maybe it can. I honestly don't know. But I, you know, the philosophers for a long time have talked about the possibility that you can have a zombie intelligence and that there are no feelings there, but everything else is the same. I mean, I have to throw this back to you, really. How do you deal with a zombie intelligence? So first of all, I can see that from a biologist's perspective, you think of all the complexities that led up to the human being. The entirety of the history of four billion years that in some deep sense integrated the human being into this environment and that dance of the organism and the environment, you could see how emotions arise from that and their emotions are deeply connected, creating a human experience. And from that, you mix in consciousness and the full mess of it, yeah. But from a perspective of an intelligent organism that's already here, like a baby that learns, it doesn't need to learn how to be a collection of cells or how to do all the things it needs to do. The basic function of a baby, as it learns, is to interact with its environment, to learn from its environment, to learn how to fit in to this social society, to like... And the basic response of the baby is to cry a lot of the time. To cry, to, well, to convince the humans to protect it or to discipline it, to teach it. I mean, we've developed a bunch of different tricks, how to get our parents to take care of us, to educate us, to teach us about the world. Also, we've constructed the world in such a way that it's safe enough for us to survive in and yet dangerous enough for learning the valuable lessons, like the tables are still hard with corners, so it can still run into them, it hurts like hell. So AI needs to solve that problem, not the problem of constructing this super complex organism that leads up, so to run the whole, to make an apple pie to build the whole universe, you need to build the whole universe. I think the zombie question, it's something I would leave to the philosophers, and I will also leave to them the definition of love and what is, what happens between two human beings when there's a magic that just grabs them, like nothing else matters in the world and somehow you've been searching for this feeling, this moment, this person your whole life. That feeling, the philosophers can have a lot of fun with that one, and also say that that's just, you can have a biological explanation, you can have all kinds of, it's all fake, it's actually, Ayn Rand will say it's all selfish, there's a lot of different interpretations, I'll leave it to the philosophers. The point is the feeling, sure as hell feels very real, and if my toaster makes me feel like it's the only toaster in the world, and when I leave and I miss the toaster, and when I come back, I'm excited to see the toaster, and my life is meaningful and joyful, and the friends I have around me get a better version of me because that toaster exists, that sure as hell feels like a conscious toaster. Is that psychologically different to having a dog? No. Because, I mean, most people would dispute whether we can say a dog, I would say a dog is undoubtedly conscious, but some people would say it doesn't.