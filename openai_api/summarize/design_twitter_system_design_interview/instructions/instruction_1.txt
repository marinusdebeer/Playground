system:
Always only provide a detailed summary of the input. Don't answer questions or complete the text. The following is the context to keep in mind: 
Designing the high-level architecture of Twitter may seem like a daunting task, but it's a fascinating exercise that can provide valuable insights into the underlying infrastructure and design of this popular social media platform. This article is based on a video from the ongoing course, System Design Interview, available on neatcode.io. 

Twitter is a social network where users can follow each other, creating a mutual or one-sided relationship. Some users may have more followers than others, and the frequency of tweets varies among users. This suggests that Twitter is a read-heavy system, with users primarily consuming content rather than creating it. 

The functionality of Twitter includes creating tweets, liking, retweeting, following or unfollowing other users, and even editing tweets. Each tweet consists of a user profile picture, username, and the content of the tweet, which can include text, images, or videos. 

When designing the architecture of Twitter, it's important to consider the scale of the platform. With millions of users and billions of tweets, it's not feasible to design every piece of functionality in a single interview. Instead, the focus should be on the key features such as following other users, creating tweets, and viewing a news feed. 

The design should also take into account the size of each tweet and the amount of data being read and written each day. For example, if each tweet is approximately one kilobyte and there are 20 billion tweets read per day, this equates to 20 petabytes of data being read daily. 

The architecture should also consider the number of active users and the frequency of their interactions. For instance, if there are 200 million daily active users, each reading about 100 tweets per day, this results in 20 billion tweet reads per day. 

The design should also account for the storage of media such as images and videos. These could be stored separately in an object storage solution like Google Cloud Storage or Amazon S3, with a reference to the media included in the tweet data. 

The high-level design of Twitter starts with the client, which could be a computer or mobile device. The client interacts with application servers to perform actions like creating a tweet or viewing a news feed. To handle the large volume of reads, a caching layer is included between the application servers and the database. 

The database could be a relational database, which allows for the creation of relationships between followers and followees. However, a NoSQL database could also be used for its scalability. 

The design also includes a content delivery network (CDN) for distributing static assets like images and videos. The CDN is geographically located close to the user, ensuring that the most relevant and popular content is delivered quickly. 

In conclusion, designing the high-level architecture of Twitter involves considering the scale of the platform, the nature of user interactions, and the amount of data being read and written each day. While the design does not have to exactly match the actual product, it should demonstrate an understanding of the trade-offs and the ability to weigh the pros and cons of different approaches.
user:
Please turn the following into an article including all the key ideas: 
As users view their feed, we may have to query multiple shards to find all the relevant tweets and then order them and then send them back to the user. That could definitely be very slow. That's clearly the bottleneck here. Now we do have a caching layer and in theory, the most popular tweets will be stored already on this caching layer. And if they're not, then we can have some type of LRU algorithm working here because we care about the most recent tweets. Most likely people aren't going to be viewing tweets from a year ago, even if they had like a billion views. After a while, people get tired of them. LRU might be better here. And in theory, caching should definitely help us lower the latency. But remember, different people will have different tweets. We definitely can't guarantee that all 20 tweets that this person wants to see are already going to be cached. What if 19 of them are already cached and then that last one tweet, we still have to go and read disk to get that tweet. And while you're scrolling, maybe all your tweets are loaded, but one tweet in the middle is taking a few extra seconds. That's not a great user experience. So the problem we're running into is not really scalability, it's latency. And caching helps with that. And sharding also helps with that. That's kind of the point of sharding. With read-only replicas, we're able to handle scale, but as we break up the data into smaller chunks, then we can also lower latency because we're going to be querying a smaller amount of data, but we still may have to query multiple shards to get this latency even lower. We can get pretty creative and we can actually generate the newsfeed of users asynchronously. And we would do that for every single user in theory, because we know a large amount of the users, 200 million out of 500 million are actually active. It makes sense to generate the newsfeed for all of them, even if 60% of them aren't going to actually view it. It's not a ton of wasted work. And we could also tune it such that we only pre-generate the newsfeed for people that are actually active within the last 30 days. Now at a very high level, what we can do is have some kind of message queue or PubSub system, which will take every new tweet that is created, will also not just be written to the database, but it will be sent from the app servers to the PubSub queue. And this queue will feed into a cluster, something like a Spark cluster. But the point is that these workers will in parallel process all the messages that we're getting, which will include every time there's a new tweet that is generated, there could be a lot. So we need to do this asynchronously. And the point of this is that these will basically feed into a new cache and this cache will be responsible for actually storing the user feed. So now when a user loads their homepage and gets their list of 20 tweets, the application server will actually be hitting this feed cache. Maybe this cache is used for individual tweets that are maybe embedded on websites or sometimes you just open up an individual tweet, not in the context of a user feed. And you want to maybe look at the replies of that tweet. But this is starting to make a little bit less sense even having this cache. So I'm just kind of giving some possible use cases. But this is the cache that will actually have a feed for every single user. So if this is in memory, it may have a large amount of data. Now we have 200 million users. And if we want to have 100 tweets for every single user, it could be a very large amount. So we probably want to shard this similar to how we did with our relational database. But the point is that this will definitely lower the latency because the feeds are not generated as a user actually requests to actually get the feed. We don't have to actually run a complex query on our relational database, having to query multiple shards and then joining the results together based on the time that they were created and then ordering them like that. The feed will actually already be created. Now the complicated part is actually updating the tweet. We kind of went through the flow of when a tweet is created, it will be added to the message queue. And then for that individual tweet, these workers will add that tweet to all the feeds of people that are following the author of that tweet. But now the problem is if somebody has 100 followers, then these workers will have to update 100 feeds. But what about somebody like Kim Kardashian, who has maybe 100 million followers, updating 100 million feeds every single time somebody popular like that makes a tweet is very, very expensive, maybe in that case, it's not the end of the world for us to actually have to update the feed of that user at the time that they actually request it, because somebody could have 100 million followers and us having to update 100 million feeds every time they make a tweet is pretty expensive. But not all 100 million of those followers are even loading their feed every single day. So it would probably be easier to do that work as it's needed. So when a user makes a request, they get their feed, but maybe in parallel to that, our app server could look for other tweets, probably a popular tweet by somebody like that is already cached here. So then at that point, our application server would also update the feed of that user. Now, what gets even more complicated is what happens when a user follows somebody new, their feed has to be updated in that case as well. Now, it's OK, we can tolerate a few seconds, maybe 5 or 10 seconds before their feed is actually updated, but it does have to get updated nonetheless, and we would have sort of a similar mechanism where a user follows somebody new, we add a message to a message queue, we have a bunch of workers that then have to update the feed cache of the person who just followed somebody else. And by the way, this cluster, of course, is going to actually have to read our database as well, which actually has the tweets themselves. Remember, the whole point of doing this is to lower the latency when somebody opens up their homepage and they want to see 20 tweets. We want it to be as quick as possible and all this complexity arises from that. If we can tolerate a few more seconds, we can simplify our design. But as long as this discussion has already been, I want to say that all of this has actually been pretty high level. Things can get much more complicated and this is actually definitely not exactly how Twitter is designed. There are actually problems with this design as well. There could be concurrent updates to our cache. There's a lot of details that we haven't discussed. How would the ordering of the feed even be in this case when somebody loads 20 tweets, they want to see 20 more. We obviously have to paginate that and there's a ton more details that we could have gone into. The main takeaway though here is that Twitter definitely cannot be designed or built in a weekend. Designing these large scale systems is a very complicated task. Even engineers who worked at Twitter for years still ran into issues where they had to modify designs. And if you read some of the white papers from 2010 up until now, they actually did. They changed parts of their database, how they were storing relations, how they were storing who follows who. So with that, I encourage you to read some of the official papers that the engineering teams at Twitter have written if you'd like to learn more, because while things can get complicated, they are still pretty interesting.Let's design the high-level architecture of Twitter. I mean, how hard could it be? By the way, this video is taken from my ongoing course, System Design Interview, which will be complete by the end of this month. You can check it out on neatcode.io. Before we get started, I do want to mention Twitter has been quite a popular topic recently, especially the underlying infrastructure and design. But keep in mind that in a real interview, your design does not have to exactly match the product. That's not what it's about at all. It's about discussing the trade-offs and kind of demonstrating your knowledge of being able to weigh the pros and cons of an approach. And of course, there's many similar products to Twitter. There really isn't any one correct approach. So we don't have to actually replicate the real Twitter design, unless, of course, you're interviewing at Twitter. In that case, you might have to because they recently fired everyone, so they need people to know how it works. So let's start with the background. We know that Twitter is a social network, first and foremost, where some people can follow other people, and that relationship can be mutual. This person can also follow the other person, but some people might end up with more followers than others, right? So assume that one person is really popular, and everybody wants to read all of their tweets. But, you know, most people on Twitter probably aren't actually tweeting very often. Most people don't actually have many followers, including myself. I'm mentioning this because it kind of hints that this is going to be a very read-heavy system. And, of course, on Twitter, the whole point is that people can create tweets. So on a particular tweet, you have a person, like this is their profile picture, and their username, and then the actual content of the tweet. It can have some text, it can have some images, and it can have a video. There's a lot of things you can do to interact with a tweet. Of course, you can like the tweet, you can do a retweet, you can follow the person who actually made the tweet, or unfollow them. Recently, you can even edit tweets now, but this is just to give you a general idea of what kind of functionality you might want to clarify with your interviewer. So now actually digging into the functional requirements. Twitter is very, very large. Of course, we can't design every little piece of functionality in a 45-minute interview. So what exactly do we want to spend most of our time on, and what parts can we just kind of hand wave, and dismiss, and just briefly discuss? Let's say the first feature, the priority feature, is that we want to be able to follow other users. So users can follow each other. Now there's no point of following other people if you can't actually create tweets. So that's also going to be just as important. And then third is actually viewing a news feed. Now at a basic level, these two features are pretty simple, but viewing a feed can definitely be more complicated, especially when we get into how we want to rank that feed. What kind of algorithm are we going to use? Probably there's machine learning going on in there. And in many cases, you end up seeing tweets in your feed by people that you're not even following, but we're going to assume that that's not the case. For viewing a feed, we just want to see tweets of people that we actually follow. I think that would be something worth clarifying with your interviewer. That's something that can kind of scope down this interview, because you might assume that we're doing something really complicated, but your interviewer is actually looking for something more simple. That's a trap that you don't want to fall into. Now what exactly is going to go in the tweet itself? We know that Twitter actually has a limit on tweet size. I think it's 140 characters. Let's assume that your interviewer gives you that number. But at the same time, with social networks, of course, we end up with images and videos. And let's say that, yes, we are going to include these in our design. So now let's transition into the non-functional requirements, which is not going to be completely separate from these actual features we're implementing. The first thing you probably want to know is how many users we're talking about here. Let's say the number is 500 million total users. But in terms of daily active users, we have about 200 million of them that are daily active. I think that's pretty close to the real number. But remember, the real number isn't so important. I think the main observation here is that almost half of the users are active. So when we actually create feeds for users, we'll be doing it for most users. Most people are going to be logging on. Most people are going to be viewing their feed. Well, not most, but nearly half, which is a pretty large percentage, about 40%. Now, while most people will be viewing their feed, they probably won't be creating tweets. But again, this is something we have to clarify. Let's say of those 200 million daily active users, each of them will read about 100 tweets per day. So 200 million times 100, that's going to be 20 billion tweet reads per day. Now, what is the size of each tweet? If we have 140 characters, that's about 140 bytes. But let's assume that there's additional information with a tweet. We have the username of that tweet, and possibly there's a lot more metadata. To be safe for just a basic tweet that just includes text, we can assume that for each tweet, we have to do a kilobyte of reading from our storage. Now, we also know that some tweets can contain images and videos. So on average, this is going to be higher. How much higher? We could spend a lot of time digging into the math. You could ask your interviewer a bunch of questions. But most likely, this is not what they want you to spend time on. Let's just average this up to a megabyte because maybe videos on average are 10 megabytes if they're a bit longer, which I don't know what the limit is for a video length on Twitter, but it could be reasonably high. But we also know that few tweets are going to actually have this. So we average it down to this because most tweets are going to be about a kilobyte, let's say. So let's say a megabyte per each tweet. So how much data are we going to be reading? If it's 20 billion tweets, a megabyte for each tweet, that is quite a lot of data. So if each tweet was just one byte, 20 billion, that's going to be 20 gigabytes. But now we're actually multiplying this by a megabyte, which is a million. So multiply this by a thousand, we get 20 terabytes, but we have to multiply it by a thousand again because that's, you know, what a million is. And then we get to 20 petabytes. So overall, we're going to be reading 20 petabytes of data per day. Now, it's no surprise to us that this is going to be a read-heavy system. This is kind of hinting to us what type of storage solution should we use. We probably don't need to be strongly consistent. Eventual consistency is enough. And that brings us to how much are we going to be writing per day? How many tweets are we going to be creating per day? Well, we have 200 million daily active users. So let's say a reasonable number is 50 million tweets created per day. Most people aren't going to be creating tweets, but maybe some people create 10 tweets per day. So this is a decent number, but you're not going to be guessing this. Your interviewer should be giving you something reasonable. Now, we could go through the rest of the math with this number, and I can show you that we're going to be writing much less than 20 petabytes of data per day, especially if we don't include the images and videos, which we're probably not going to be directly storing in a database. But the main thing here to realize is that, yes, we're going to be writing much less than we're reading. So that's how we want to optimize our design. And let's say that the average user follows about 100 people. So 100 follows per person. But of course, there can be power users who have 1,000 or 10,000 people that they follow. But the more important consideration here is, for a user, how many followers can they have? Someone like Kim Kardashian, I don't know if she's the most popular on Twitter, but I think it's at least over 100 million followers that they have. So this is the more important consideration, people who have a massive amount of followers. So the question is going to be, for all the people that follow Kim Kardashian, how are they going to get the tweets? This is kind of hinting to us that wherever we're storing her tweets, it's going to get overloaded pretty quickly. So now that we kind of know what we're looking for, let's get into the high level design. We know, of course, that everything is going to start with our client, whether that's a computer or a mobile device, it doesn't really matter for us. We're focusing on the back end, which is agnostic to the front end. We know the first thing our user is going to be hitting is the application servers to perform actions like creating a tweet or getting their news feed or following someone. Now, because of the scale that we're dealing with, we're probably going to be bottlenecked by getting the news feed. That's what's going to be happening most frequently. And if we want to scale this up, assuming that these application servers are stateless, it should be easy to scale them up. And of course, we will have the load balancer in between this. That's pretty hand wavy. I mean, that's something you can just memorize and say, well, if you want to scale horizontally, scale this and put a load balancer in there. It's pretty trivial, so I'm not going to spend a lot of time on that. Now, of course, our application server is going to be reading from some storage. Let's say we do have a database. And let's say that it is a relational database. And you might be thinking, if we're going to be doing read heavy, why use a relational database? Why not just have a NoSQL database? Well, it depends on what type of data we're going to be storing. Do we need joins in this case? And we could, because we do have a very relational model when it comes to following. That's a pretty clear relationship between followers and followees. So that's a reason to go with a relational database. Now, in theory, it would be easier to scale a NoSQL database, but we can implement sharding with a relational database. So that does give us some flexibility, though. After finishing our high level design, we might want to revise this because we could just store tweets and user information in a NoSQL database and then have a GraphDB, which would be very easy to find that follower relationship because a GraphDB is essentially like an adjacency list graph where every person is like a node in a graph. And to find all the people that they follow, you just have to look at every outgoing edge. And to find all the followers of a person, you just have to look at every incoming edge. So depending on your expertise and your background, and of course, what your interviewer is looking for and what they might be familiar with, you can kind of have some discussion about these differences. Now, with the massive amount of reads that we're going to be doing, we basically have to have a caching layer in between. So as we're reading tweets, we will be hitting our cache before we hit our database. But also remember that we are going to be storing media, so we need a separate storage solution for that media. Relational databases aren't the best for that. So we'll have some type of object storage for that, something like Google Cloud Storage or Amazon S3. So when we actually read a tweet, we'll be getting the information about that tweet, like the tweet ID, who is the creator of that tweet, what time was it created, whether it included an image or not, what was the image that it included, or the profile picture of the person who made it. The application server can then fetch the image, like the profile picture or the video that showed up in that tweet, and it can do that separately. But at the same time, because these assets are static in nature, it may be better to actually distribute them over a CDN network. So then actually our application server does not have to interact with the object storage. The application server will respond to the user with all the information that they need, including the URL of that image or video that they need. And then the client, whether they're using a mobile device or a desktop browser, will make a separate request, which will actually hit our CDN network, which is tied to our object storage. What type of algorithm would we use in this case? Well, even though we're looking at the high level right now, we probably want to use a pull-based CDN. We don't want to necessarily push every image or video to the CDN immediately. Also, remember the benefit of a CDN is that it's geographically located close to the user. We know that people in India might be looking at different types of tweets and images and videos than people in the United States. So it doesn't make sense to put every single new tweet, push it directly to the CDN network. And with a pull-based CDN, we kind of guarantee that the stuff that's loaded onto our CDN is the relevant stuff that people want to see anyway, the popular things. So now let's spend most of our time actually digging into the details, which some people like to start out with the interface that we'll be using. We will have a couple. So remember, we have a create tweet. There could be a lot of metadata sent with that request. Of course, the user ID of the person creating the tweet, but mainly the user is actually responsible for sending the content of the tweet. So one is the actual text, and then second is going to be the actual media. Of course, every tweet has a created timestamp, but we assume that that'll be handled server-side. And every tweet has to be identified, but we'll assume that the tweet ID is also created server-side.
