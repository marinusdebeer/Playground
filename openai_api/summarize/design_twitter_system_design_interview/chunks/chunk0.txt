Let's design the high-level architecture of Twitter. I mean, how hard could it be? By the way, this video is taken from my ongoing course, System Design Interview, which will be complete by the end of this month. You can check it out on neatcode.io. Before we get started, I do want to mention Twitter has been quite a popular topic recently, especially the underlying infrastructure and design. But keep in mind that in a real interview, your design does not have to exactly match the product. That's not what it's about at all. It's about discussing the trade-offs and kind of demonstrating your knowledge of being able to weigh the pros and cons of an approach. And of course, there's many similar products to Twitter. There really isn't any one correct approach. So we don't have to actually replicate the real Twitter design, unless, of course, you're interviewing at Twitter. In that case, you might have to because they recently fired everyone, so they need people to know how it works. So let's start with the background. We know that Twitter is a social network, first and foremost, where some people can follow other people, and that relationship can be mutual. This person can also follow the other person, but some people might end up with more followers than others, right? So assume that one person is really popular, and everybody wants to read all of their tweets. But, you know, most people on Twitter probably aren't actually tweeting very often. Most people don't actually have many followers, including myself. I'm mentioning this because it kind of hints that this is going to be a very read-heavy system. And, of course, on Twitter, the whole point is that people can create tweets. So on a particular tweet, you have a person, like this is their profile picture, and their username, and then the actual content of the tweet. It can have some text, it can have some images, and it can have a video. There's a lot of things you can do to interact with a tweet. Of course, you can like the tweet, you can do a retweet, you can follow the person who actually made the tweet, or unfollow them. Recently, you can even edit tweets now, but this is just to give you a general idea of what kind of functionality you might want to clarify with your interviewer. So now actually digging into the functional requirements. Twitter is very, very large. Of course, we can't design every little piece of functionality in a 45-minute interview. So what exactly do we want to spend most of our time on, and what parts can we just kind of hand wave, and dismiss, and just briefly discuss? Let's say the first feature, the priority feature, is that we want to be able to follow other users. So users can follow each other. Now there's no point of following other people if you can't actually create tweets. So that's also going to be just as important. And then third is actually viewing a news feed. Now at a basic level, these two features are pretty simple, but viewing a feed can definitely be more complicated, especially when we get into how we want to rank that feed. What kind of algorithm are we going to use? Probably there's machine learning going on in there. And in many cases, you end up seeing tweets in your feed by people that you're not even following, but we're going to assume that that's not the case. For viewing a feed, we just want to see tweets of people that we actually follow. I think that would be something worth clarifying with your interviewer. That's something that can kind of scope down this interview, because you might assume that we're doing something really complicated, but your interviewer is actually looking for something more simple. That's a trap that you don't want to fall into. Now what exactly is going to go in the tweet itself? We know that Twitter actually has a limit on tweet size. I think it's 140 characters. Let's assume that your interviewer gives you that number. But at the same time, with social networks, of course, we end up with images and videos. And let's say that, yes, we are going to include these in our design. So now let's transition into the non-functional requirements, which is not going to be completely separate from these actual features we're implementing. The first thing you probably want to know is how many users we're talking about here. Let's say the number is 500 million total users. But in terms of daily active users, we have about 200 million of them that are daily active. I think that's pretty close to the real number. But remember, the real number isn't so important. I think the main observation here is that almost half of the users are active. So when we actually create feeds for users, we'll be doing it for most users. Most people are going to be logging on. Most people are going to be viewing their feed. Well, not most, but nearly half, which is a pretty large percentage, about 40%. Now, while most people will be viewing their feed, they probably won't be creating tweets. But again, this is something we have to clarify. Let's say of those 200 million daily active users, each of them will read about 100 tweets per day. So 200 million times 100, that's going to be 20 billion tweet reads per day. Now, what is the size of each tweet? If we have 140 characters, that's about 140 bytes. But let's assume that there's additional information with a tweet. We have the username of that tweet, and possibly there's a lot more metadata. To be safe for just a basic tweet that just includes text, we can assume that for each tweet, we have to do a kilobyte of reading from our storage. Now, we also know that some tweets can contain images and videos. So on average, this is going to be higher. How much higher? We could spend a lot of time digging into the math. You could ask your interviewer a bunch of questions. But most likely, this is not what they want you to spend time on. Let's just average this up to a megabyte because maybe videos on average are 10 megabytes if they're a bit longer, which I don't know what the limit is for a video length on Twitter, but it could be reasonably high. But we also know that few tweets are going to actually have this. So we average it down to this because most tweets are going to be about a kilobyte, let's say. So let's say a megabyte per each tweet. So how much data are we going to be reading? If it's 20 billion tweets, a megabyte for each tweet, that is quite a lot of data. So if each tweet was just one byte, 20 billion, that's going to be 20 gigabytes. But now we're actually multiplying this by a megabyte, which is a million. So multiply this by a thousand, we get 20 terabytes, but we have to multiply it by a thousand again because that's, you know, what a million is. And then we get to 20 petabytes. So overall, we're going to be reading 20 petabytes of data per day. Now, it's no surprise to us that this is going to be a read-heavy system. This is kind of hinting to us what type of storage solution should we use. We probably don't need to be strongly consistent. Eventual consistency is enough. And that brings us to how much are we going to be writing per day? How many tweets are we going to be creating per day? Well, we have 200 million daily active users. So let's say a reasonable number is 50 million tweets created per day. Most people aren't going to be creating tweets, but maybe some people create 10 tweets per day. So this is a decent number, but you're not going to be guessing this. Your interviewer should be giving you something reasonable. Now, we could go through the rest of the math with this number, and I can show you that we're going to be writing much less than 20 petabytes of data per day, especially if we don't include the images and videos, which we're probably not going to be directly storing in a database. But the main thing here to realize is that, yes, we're going to be writing much less than we're reading. So that's how we want to optimize our design. And let's say that the average user follows about 100 people. So 100 follows per person. But of course, there can be power users who have 1,000 or 10,000 people that they follow. But the more important consideration here is, for a user, how many followers can they have? Someone like Kim Kardashian, I don't know if she's the most popular on Twitter, but I think it's at least over 100 million followers that they have. So this is the more important consideration, people who have a massive amount of followers. So the question is going to be, for all the people that follow Kim Kardashian, how are they going to get the tweets? This is kind of hinting to us that wherever we're storing her tweets, it's going to get overloaded pretty quickly. So now that we kind of know what we're looking for, let's get into the high level design. We know, of course, that everything is going to start with our client, whether that's a computer or a mobile device, it doesn't really matter for us. We're focusing on the back end, which is agnostic to the front end. We know the first thing our user is going to be hitting is the application servers to perform actions like creating a tweet or getting their news feed or following someone. Now, because of the scale that we're dealing with, we're probably going to be bottlenecked by getting the news feed. That's what's going to be happening most frequently. And if we want to scale this up, assuming that these application servers are stateless, it should be easy to scale them up. And of course, we will have the load balancer in between this. That's pretty hand wavy. I mean, that's something you can just memorize and say, well, if you want to scale horizontally, scale this and put a load balancer in there. It's pretty trivial, so I'm not going to spend a lot of time on that. Now, of course, our application server is going to be reading from some storage. Let's say we do have a database. And let's say that it is a relational database. And you might be thinking, if we're going to be doing read heavy, why use a relational database? Why not just have a NoSQL database? Well, it depends on what type of data we're going to be storing. Do we need joins in this case? And we could, because we do have a very relational model when it comes to following. That's a pretty clear relationship between followers and followees. So that's a reason to go with a relational database. Now, in theory, it would be easier to scale a NoSQL database, but we can implement sharding with a relational database. So that does give us some flexibility, though. After finishing our high level design, we might want to revise this because we could just store tweets and user information in a NoSQL database and then have a GraphDB, which would be very easy to find that follower relationship because a GraphDB is essentially like an adjacency list graph where every person is like a node in a graph. And to find all the people that they follow, you just have to look at every outgoing edge. And to find all the followers of a person, you just have to look at every incoming edge. So depending on your expertise and your background, and of course, what your interviewer is looking for and what they might be familiar with, you can kind of have some discussion about these differences. Now, with the massive amount of reads that we're going to be doing, we basically have to have a caching layer in between. So as we're reading tweets, we will be hitting our cache before we hit our database. But also remember that we are going to be storing media, so we need a separate storage solution for that media. Relational databases aren't the best for that. So we'll have some type of object storage for that, something like Google Cloud Storage or Amazon S3. So when we actually read a tweet, we'll be getting the information about that tweet, like the tweet ID, who is the creator of that tweet, what time was it created, whether it included an image or not, what was the image that it included, or the profile picture of the person who made it. The application server can then fetch the image, like the profile picture or the video that showed up in that tweet, and it can do that separately. But at the same time, because these assets are static in nature, it may be better to actually distribute them over a CDN network. So then actually our application server does not have to interact with the object storage. The application server will respond to the user with all the information that they need, including the URL of that image or video that they need. And then the client, whether they're using a mobile device or a desktop browser, will make a separate request, which will actually hit our CDN network, which is tied to our object storage. What type of algorithm would we use in this case? Well, even though we're looking at the high level right now, we probably want to use a pull-based CDN. We don't want to necessarily push every image or video to the CDN immediately. Also, remember the benefit of a CDN is that it's geographically located close to the user. We know that people in India might be looking at different types of tweets and images and videos than people in the United States. So it doesn't make sense to put every single new tweet, push it directly to the CDN network. And with a pull-based CDN, we kind of guarantee that the stuff that's loaded onto our CDN is the relevant stuff that people want to see anyway, the popular things. So now let's spend most of our time actually digging into the details, which some people like to start out with the interface that we'll be using. We will have a couple. So remember, we have a create tweet. There could be a lot of metadata sent with that request. Of course, the user ID of the person creating the tweet, but mainly the user is actually responsible for sending the content of the tweet. So one is the actual text, and then second is going to be the actual media. Of course, every tweet has a created timestamp, but we assume that that'll be handled server-side. And every tweet has to be identified, but we'll assume that the tweet ID is also created server-side. And of course, the user ID of the person that's actually creating it, well, we'll assume that in the header of the HTTP request, that there's some authorization token for us to know that the correct person is making the tweet. But we could also have the user ID passed into this request or the username of the person. And next, of course, we have getting the actual feed, and that really doesn't need any information at all. That's a very basic read request. We don't need to send additional data that we're going to be actually storing. Get feed should just pass in the user ID so that we know which person's feed are we getting. But at the same time, I should not be able to pass in your user ID, even if I know it, to get your user feed. There's nothing that can go super wrong with that, but it shouldn't be allowed, and that would be handled by the HTTP header. We know that there's actual authentication going on in this system. It's just that we're not focusing on that. We're focusing on the actual Twitter design. Authentication happens with pretty much every application. And we also have the follow interaction. So a user can follow another person. They'll pass in their user ID and maybe the username of the person that they're trying to follow. So these are the three main interactions. Now, how are we actually going to be storing this data? In particular, we're going to be storing two things, the actual tweets. So assuming we have a relational database, we'll have a table of tweets, and we'll also have a table of follows. So the follow relationship is pretty simple. You have the followee, the person who's following the other person. That's going to be a string. And we'll have another, the actual follower. I think I misspoke when I was describing followee. So the follower is the person that's following the followee. The followee is the one that's being followed. Now, assuming that this is a table, and remember, for a user, we want to get all the tweets of people that this person follows. So they're the follower, and they want all the tweets of their followees. How would we index this table? We'd probably want to index based on the follower, because then all records in that table will be grouped together based on the follower. So all the people that this guy follows will be grouped together in the table. It'll just be a range query. So assuming this is our table, all records for this person will be grouped together in a particular range over here. I think it's worth mentioning that we would favor indexing based on the follower if we have a read-heavy system. Now, for the tweet itself, we kind of briefly talked about it. Of course, we're going to have the tweet ID. We're going to have the timestamp. We're going to have the user ID of the person who created it. And we're going to have the content of the tweet, whether it's the text, which technically could be empty if we have some media attached to it. But we actually won't be storing the media itself in the database. We'll have a reference to that media that references the object store. Now, the bigger problem with our storage is we're going to be storing a massive amount of data. If you went through the calculation that I mentioned earlier, you would have gotten to, I think, roughly 50 gigabytes of data are going to be written per day to our relational database if we're not including the media. So that's a lot of data. In a month, we'll have, I think, 1.5 terabytes. In the course of a year, we'll have roughly 18 terabytes. So it is a large amount of data, but it's actually reasonable. And the good thing about Twitter is that tweets are relatively small. But the problem is we're going to be having so many reads hitting this database. A person is going to be reading 100 tweets per day, and we have 200 million of them. So the first approach and the obvious thing to do is to have read-only replicas of this database. If reads are the bottleneck, it's not hard to just add additional database instances. Now, the problem will be when a user actually creates a tweet, if we have single leader replication, all those writes are going to be hitting a single database. And remember, if we have 50 million writes per day, you divide that by 100,000, which is roughly the amount of seconds in a day, we get 500 writes per second. Well, probably even more than that. But at the same time, there's going to be moments where we have traffic. This is the average, but peak could be much higher. I mean, peak could be even 10 times this amount, depending on what's going on. Maybe Elon Musk does something crazy. So ideally, we should be able to scale our rights as well. And the obvious way to do this is by using sharding. But the question is, how are we going to be implementing this sharding? Before we get into that, I forgot to mention, if we do have read-only replicas, it's okay if a single instance gets the rights and then asynchronously populates the read-only replicas with the data, because in the case that a user ends up hitting one of the replicas and gets some stale data, it's okay if it takes five seconds after a tweet is created before a user actually gets that tweet, or it might even take 20 seconds. That wouldn't be ideal, but it's not the end of the world with something like Twitter. Now, the question is, how are we actually going to be sharding this? What type of shard key are we going to be using? I think the most obvious and easy way would be to do it based on user ID, because that's kind of the whole point of our design, the way we scoped it out. A user only cares about a subset of users. They don't care about every user. It doesn't make a lot of sense to do it based on tweet ID, because what we want is to break up our database into pieces, and we want a particular user to ideally only have to hit one of these pieces. With the logic of our system, especially the sharding logic, a user should know which people they follow, and then we can route the request to the appropriate shards that contain the tweets of the people that they follow. And we'll know that because we can use our shard key to determine that. But if we break up the tweets based on tweet ID, we actually don't know which shard contains the tweets of the people that they follow. So we'd have to query all the shards. That kind of defeats the purpose. So we'll be choosing to do this based on user ID. And since we don't actually have complex queries and joins that we're doing to actually retrieve the tweets, how will we get the people that they follow? Well, if we shard that based on user ID as well, all the people that this guy follows should be on a single shard. Now, a potential problem is that we will actually have to order the tweets. Just fetching the tweets is not enough. We will actually have to query them and then order them based on the time that they were created. And of course, how many tweets are we looking for? If we're just looking for a small number like 20, what if the user wants to scroll down in their feed and we want 20 more tweets? We don't necessarily need to get all tweets immediately. We need to wait for the user to actually scroll, or maybe we can actually get all the tweets immediately. But before I even get into that, let's quickly look at how our current system is going to work. When a user creates a tweet, it will go through the server. Based on the user ID, we will find the appropriate shard and then store the tweet on that shard and then any images and media on object storage.