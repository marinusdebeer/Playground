And of course, the user ID of the person that's actually creating it, well, we'll assume that in the header of the HTTP request, that there's some authorization token for us to know that the correct person is making the tweet. But we could also have the user ID passed into this request or the username of the person. And next, of course, we have getting the actual feed, and that really doesn't need any information at all. That's a very basic read request. We don't need to send additional data that we're going to be actually storing. Get feed should just pass in the user ID so that we know which person's feed are we getting. But at the same time, I should not be able to pass in your user ID, even if I know it, to get your user feed. There's nothing that can go super wrong with that, but it shouldn't be allowed, and that would be handled by the HTTP header. We know that there's actual authentication going on in this system. It's just that we're not focusing on that. We're focusing on the actual Twitter design. Authentication happens with pretty much every application. And we also have the follow interaction. So a user can follow another person. They'll pass in their user ID and maybe the username of the person that they're trying to follow. So these are the three main interactions. Now, how are we actually going to be storing this data? In particular, we're going to be storing two things, the actual tweets. So assuming we have a relational database, we'll have a table of tweets, and we'll also have a table of follows. So the follow relationship is pretty simple. You have the followee, the person who's following the other person. That's going to be a string. And we'll have another, the actual follower. I think I misspoke when I was describing followee. So the follower is the person that's following the followee. The followee is the one that's being followed. Now, assuming that this is a table, and remember, for a user, we want to get all the tweets of people that this person follows. So they're the follower, and they want all the tweets of their followees. How would we index this table? We'd probably want to index based on the follower, because then all records in that table will be grouped together based on the follower. So all the people that this guy follows will be grouped together in the table. It'll just be a range query. So assuming this is our table, all records for this person will be grouped together in a particular range over here. I think it's worth mentioning that we would favor indexing based on the follower if we have a read-heavy system. Now, for the tweet itself, we kind of briefly talked about it. Of course, we're going to have the tweet ID. We're going to have the timestamp. We're going to have the user ID of the person who created it. And we're going to have the content of the tweet, whether it's the text, which technically could be empty if we have some media attached to it. But we actually won't be storing the media itself in the database. We'll have a reference to that media that references the object store. Now, the bigger problem with our storage is we're going to be storing a massive amount of data. If you went through the calculation that I mentioned earlier, you would have gotten to, I think, roughly 50 gigabytes of data are going to be written per day to our relational database if we're not including the media. So that's a lot of data. In a month, we'll have, I think, 1.5 terabytes. In the course of a year, we'll have roughly 18 terabytes. So it is a large amount of data, but it's actually reasonable. And the good thing about Twitter is that tweets are relatively small. But the problem is we're going to be having so many reads hitting this database. A person is going to be reading 100 tweets per day, and we have 200 million of them. So the first approach and the obvious thing to do is to have read-only replicas of this database. If reads are the bottleneck, it's not hard to just add additional database instances. Now, the problem will be when a user actually creates a tweet, if we have single leader replication, all those writes are going to be hitting a single database. And remember, if we have 50 million writes per day, you divide that by 100,000, which is roughly the amount of seconds in a day, we get 500 writes per second. Well, probably even more than that. But at the same time, there's going to be moments where we have traffic. This is the average, but peak could be much higher. I mean, peak could be even 10 times this amount, depending on what's going on. Maybe Elon Musk does something crazy. So ideally, we should be able to scale our rights as well. And the obvious way to do this is by using sharding. But the question is, how are we going to be implementing this sharding? Before we get into that, I forgot to mention, if we do have read-only replicas, it's okay if a single instance gets the rights and then asynchronously populates the read-only replicas with the data, because in the case that a user ends up hitting one of the replicas and gets some stale data, it's okay if it takes five seconds after a tweet is created before a user actually gets that tweet, or it might even take 20 seconds. That wouldn't be ideal, but it's not the end of the world with something like Twitter. Now, the question is, how are we actually going to be sharding this? What type of shard key are we going to be using? I think the most obvious and easy way would be to do it based on user ID, because that's kind of the whole point of our design, the way we scoped it out. A user only cares about a subset of users. They don't care about every user. It doesn't make a lot of sense to do it based on tweet ID, because what we want is to break up our database into pieces, and we want a particular user to ideally only have to hit one of these pieces. With the logic of our system, especially the sharding logic, a user should know which people they follow, and then we can route the request to the appropriate shards that contain the tweets of the people that they follow. And we'll know that because we can use our shard key to determine that. But if we break up the tweets based on tweet ID, we actually don't know which shard contains the tweets of the people that they follow. So we'd have to query all the shards. That kind of defeats the purpose. So we'll be choosing to do this based on user ID. And since we don't actually have complex queries and joins that we're doing to actually retrieve the tweets, how will we get the people that they follow? Well, if we shard that based on user ID as well, all the people that this guy follows should be on a single shard. Now, a potential problem is that we will actually have to order the tweets. Just fetching the tweets is not enough. We will actually have to query them and then order them based on the time that they were created. And of course, how many tweets are we looking for? If we're just looking for a small number like 20, what if the user wants to scroll down in their feed and we want 20 more tweets? We don't necessarily need to get all tweets immediately. We need to wait for the user to actually scroll, or maybe we can actually get all the tweets immediately. But before I even get into that, let's quickly look at how our current system is going to work. When a user creates a tweet, it will go through the server. Based on the user ID, we will find the appropriate shard and then store the tweet on that shard and then any images and media on object storage. As users view their feed, we may have to query multiple shards to find all the relevant tweets and then order them and then send them back to the user. That could definitely be very slow. That's clearly the bottleneck here. Now we do have a caching layer and in theory, the most popular tweets will be stored already on this caching layer. And if they're not, then we can have some type of LRU algorithm working here because we care about the most recent tweets. Most likely people aren't going to be viewing tweets from a year ago, even if they had like a billion views. After a while, people get tired of them. LRU might be better here. And in theory, caching should definitely help us lower the latency. But remember, different people will have different tweets. We definitely can't guarantee that all 20 tweets that this person wants to see are already going to be cached. What if 19 of them are already cached and then that last one tweet, we still have to go and read disk to get that tweet. And while you're scrolling, maybe all your tweets are loaded, but one tweet in the middle is taking a few extra seconds. That's not a great user experience. So the problem we're running into is not really scalability, it's latency. And caching helps with that. And sharding also helps with that. That's kind of the point of sharding. With read-only replicas, we're able to handle scale, but as we break up the data into smaller chunks, then we can also lower latency because we're going to be querying a smaller amount of data, but we still may have to query multiple shards to get this latency even lower. We can get pretty creative and we can actually generate the newsfeed of users asynchronously. And we would do that for every single user in theory, because we know a large amount of the users, 200 million out of 500 million are actually active. It makes sense to generate the newsfeed for all of them, even if 60% of them aren't going to actually view it. It's not a ton of wasted work. And we could also tune it such that we only pre-generate the newsfeed for people that are actually active within the last 30 days. Now at a very high level, what we can do is have some kind of message queue or PubSub system, which will take every new tweet that is created, will also not just be written to the database, but it will be sent from the app servers to the PubSub queue. And this queue will feed into a cluster, something like a Spark cluster. But the point is that these workers will in parallel process all the messages that we're getting, which will include every time there's a new tweet that is generated, there could be a lot. So we need to do this asynchronously. And the point of this is that these will basically feed into a new cache and this cache will be responsible for actually storing the user feed. So now when a user loads their homepage and gets their list of 20 tweets, the application server will actually be hitting this feed cache. Maybe this cache is used for individual tweets that are maybe embedded on websites or sometimes you just open up an individual tweet, not in the context of a user feed. And you want to maybe look at the replies of that tweet. But this is starting to make a little bit less sense even having this cache. So I'm just kind of giving some possible use cases. But this is the cache that will actually have a feed for every single user. So if this is in memory, it may have a large amount of data. Now we have 200 million users. And if we want to have 100 tweets for every single user, it could be a very large amount. So we probably want to shard this similar to how we did with our relational database. But the point is that this will definitely lower the latency because the feeds are not generated as a user actually requests to actually get the feed. We don't have to actually run a complex query on our relational database, having to query multiple shards and then joining the results together based on the time that they were created and then ordering them like that. The feed will actually already be created. Now the complicated part is actually updating the tweet. We kind of went through the flow of when a tweet is created, it will be added to the message queue. And then for that individual tweet, these workers will add that tweet to all the feeds of people that are following the author of that tweet. But now the problem is if somebody has 100 followers, then these workers will have to update 100 feeds. But what about somebody like Kim Kardashian, who has maybe 100 million followers, updating 100 million feeds every single time somebody popular like that makes a tweet is very, very expensive, maybe in that case, it's not the end of the world for us to actually have to update the feed of that user at the time that they actually request it, because somebody could have 100 million followers and us having to update 100 million feeds every time they make a tweet is pretty expensive. But not all 100 million of those followers are even loading their feed every single day. So it would probably be easier to do that work as it's needed. So when a user makes a request, they get their feed, but maybe in parallel to that, our app server could look for other tweets, probably a popular tweet by somebody like that is already cached here. So then at that point, our application server would also update the feed of that user. Now, what gets even more complicated is what happens when a user follows somebody new, their feed has to be updated in that case as well. Now, it's OK, we can tolerate a few seconds, maybe 5 or 10 seconds before their feed is actually updated, but it does have to get updated nonetheless, and we would have sort of a similar mechanism where a user follows somebody new, we add a message to a message queue, we have a bunch of workers that then have to update the feed cache of the person who just followed somebody else. And by the way, this cluster, of course, is going to actually have to read our database as well, which actually has the tweets themselves. Remember, the whole point of doing this is to lower the latency when somebody opens up their homepage and they want to see 20 tweets. We want it to be as quick as possible and all this complexity arises from that. If we can tolerate a few more seconds, we can simplify our design. But as long as this discussion has already been, I want to say that all of this has actually been pretty high level. Things can get much more complicated and this is actually definitely not exactly how Twitter is designed. There are actually problems with this design as well. There could be concurrent updates to our cache. There's a lot of details that we haven't discussed. How would the ordering of the feed even be in this case when somebody loads 20 tweets, they want to see 20 more. We obviously have to paginate that and there's a ton more details that we could have gone into. The main takeaway though here is that Twitter definitely cannot be designed or built in a weekend. Designing these large scale systems is a very complicated task. Even engineers who worked at Twitter for years still ran into issues where they had to modify designs. And if you read some of the white papers from 2010 up until now, they actually did. They changed parts of their database, how they were storing relations, how they were storing who follows who. So with that, I encourage you to read some of the official papers that the engineering teams at Twitter have written if you'd like to learn more, because while things can get complicated, they are still pretty interesting.