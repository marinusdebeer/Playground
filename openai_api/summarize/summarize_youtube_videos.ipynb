{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a youtube URL the following will download transcript then summarize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m load_dotenv()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[0;32m---> 16\u001b[0m client \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m openai\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# video_url = 'https://www.youtube.com/watch?v=SNgoul4vyDM'\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# summary_title = \"ufo_hearing_whisper_transcribe_audio_gpt_4\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/venv/lib/python3.8/site-packages/openai/_client.py:105\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    103\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    107\u001b[0m     )\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m api_key\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m organization \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "from __future__ import unicode_literals\n",
    "import openai\n",
    "import os\n",
    "import whisper\n",
    "from dotenv import load_dotenv\n",
    "import youtube_dl\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import shutil\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# video_url = 'https://www.youtube.com/watch?v=SNgoul4vyDM'\n",
    "# summary_title = \"ufo_hearing_whisper_transcribe_audio_gpt_4\"\n",
    "\n",
    "video_url = 'https://youtu.be/RF72hGIdrG8?si=B3sowi0RGHEgcUU5'\n",
    "summary_title = \"apple_watch_ultra\"\n",
    "\n",
    "video_id = video_url.split('=')[1]\n",
    "task = \"Please provide a long detailed summary of the following transcript from a Youtube video \\n\"\n",
    "\n",
    "models = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-1106-preview']\n",
    "USE = 2\n",
    "TRANSCRIBE = True\n",
    "LOCAL_WHISPER = False\n",
    "tokens_cost = 0\n",
    "if USE == 0:\n",
    "    chunk_size = 2300\n",
    "    input_cost = 0.0015/1000\n",
    "    output_cost = 0.002/1000\n",
    "elif USE == 2:\n",
    "    chunk_size = 40_000\n",
    "    input_cost = 0.01/1000\n",
    "    output_cost = 0.03/1000\n",
    "else:\n",
    "    chunk_size = 4000\n",
    "    input_cost = 0.03/1000\n",
    "    output_cost = 0.06/1000\n",
    "\n",
    "audio_cost_per_second = 0.006/60\n",
    "audio_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(\"b1m_solved_urban_sprawl/transcript.txt\", \"rb\"),\n",
    "  purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Summarizer\",\n",
    "  description=\"Your task is to provide detailed and comprehensive summaries of YouTube video transcripts. These summaries should capture all key ideas, main points, and important details presented in the transcript. Focus on maintaining the essence and flow of the original content while ensuring clarity and coherence in the summary. Your summaries should be long enough to encompass all critical information and insights from the transcript, aiming to give a complete understanding of the video's content.\",\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  tools=[{\"type\": \"retrieval\"}],\n",
    "  file_ids=[file.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_xE5hTluqqBdVn8UZz54ZcT7b', assistant_id='asst_MgG8KonDU56w7rUL9O1bWdd4', content=[MessageContentText(text=Text(annotations=[], value=\"The transcript appears to be a detailed account of architect Moshe Safdie's project, Habitat 67, which originally aimed to create a high-density housing solution in Montreal for Expo 67. Safdie's vision was to have modules stacked high like a hillside, providing each housing unit with a roof terrace. He faced budget constraints which forced a scale-back of the project from a community of 1,200 families to just 158 residences across three smaller pyramids.\\n\\nDespite the scale-down, Habitat 67 was a success and became a highly desirable place to live, with its long waitlist and long-term occupancy. However, the revolutionary impact on architecture it promised never fully materialized. Later, Safdie's architects and Epic Games used Unreal Engine to digitally complete Habitat 67 to its original design as a way to preserve and share it. They worked closely with Safdie, who was amazed by the results, noting that it would have been very convincing if he had this technology back in the '60s.\\n\\nThe project evidently captured a renewed interest in Safdie's ideas among a new generation of architects, who are considering how to leverage his concepts in modern urban developments. The transcript concludes with an invitation to explore the hillside model and mentions a podcast for further discussion on this and other topics related to the construction industry.\"), type='text')], created_at=1699924080, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_2PeI3tRPjUotICCvkhbM0vhv', thread_id='thread_qDzRBcaaN6weWIl2CD3sak7V'), ThreadMessage(id='msg_hC5rqh1fgRUZQo7IUmQNYjQX', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='summarize the transcript'), type='text')], created_at=1699924028, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_qDzRBcaaN6weWIl2CD3sak7V')], object='list', first_id='msg_xE5hTluqqBdVn8UZz54ZcT7b', last_id='msg_hC5rqh1fgRUZQo7IUmQNYjQX', has_more=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"summarize the transcript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_from_youtube(video_url, output_path, file_name=\"audio.mp4\"):\n",
    "    \n",
    "    yt = YouTube(video_url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_stream.download(output_path, filename=file_name)\n",
    "    print(\"Audio download completed!\")\n",
    "    \n",
    "    \n",
    "    return output_path + \"/\" + file_name\n",
    "\n",
    "def split_audio_into_chunks(audio, output_folder, chunk_duration=900):\n",
    "    # audio = AudioSegment.from_file(audio_path, format=\"mp4\")\n",
    "    audio_duration_ms = len(audio)\n",
    "    \n",
    "    for start_time_ms in range(0, audio_duration_ms, chunk_duration * 1000):\n",
    "        end_time_ms = start_time_ms + (chunk_duration * 1000)\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        location = f\"{output_folder}/audio_chunks\"\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        output_file = f\"{output_folder}/audio_chunks/chunk_{start_time_ms//1000}.mp4\"\n",
    "        chunk.export(output_file, format=\"mp4\")\n",
    "\n",
    "\n",
    "def get_video_title(url):\n",
    "    with youtube_dl.YoutubeDL({}) as ydl:\n",
    "        info_dict = ydl.extract_info(url, download=False)\n",
    "        return info_dict.get('title', None)\n",
    "\n",
    "\n",
    "def summarize(input, model=models[USE]):\n",
    "    global input_tokens, output_tokens, input_cost, output_cost, tokens_cost\n",
    "  \n",
    "    completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=input,\n",
    "      temperature=0\n",
    "    )\n",
    "    # completion = openai.ChatCompletion.create(\n",
    "    #     model=model,\n",
    "    #     temperature=0,\n",
    "    #     messages=input)\n",
    "\n",
    "    input_tokens += completion.usage.prompt_tokens\n",
    "    output_tokens += completion.usage.completion_tokens\n",
    "\n",
    "    tokens_cost = input_tokens*input_cost + output_tokens*output_cost\n",
    "    print(f\"Tokens thus far: {input_tokens + output_tokens} with a cost of ${round(tokens_cost, 4)}\")\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    return reply_content\n",
    "\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "def download_transcript(video_id):\n",
    "    video_id = video_id.split('=')[-1]\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "    with open(f\"{summary_title}/transcript.txt\", \"w\") as file:\n",
    "        for line in transcript:\n",
    "            file.write(line['text'] + '\\n')\n",
    "\n",
    "\n",
    "def chunk_transcript_sentences(file_name, chunk_size, summary_title):\n",
    "    # Open and read the file\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split('(?<=[.!?]) +', text)\n",
    "\n",
    "    chunks, chunk = [], []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split(' ')\n",
    "        sentence_length = len(sentence_words)\n",
    "\n",
    "        # If adding the next sentence doesn't exceed the chunk size, add it to the current chunk\n",
    "        if current_chunk_size + sentence_length <= chunk_size:\n",
    "            chunk.append(sentence)\n",
    "            current_chunk_size += sentence_length\n",
    "        else:\n",
    "            # Otherwise, finish the current chunk and start a new one\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = [sentence]\n",
    "            current_chunk_size = sentence_length\n",
    "\n",
    "    # Add the last chunk if it's non-empty\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    create_directory(f\"{summary_title}/chunks\")\n",
    "\n",
    "    # Write each chunk to a file\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(f'{summary_title}/chunks/chunk{i}.txt', 'w') as file:\n",
    "            file.write(chunk)\n",
    "\n",
    "\n",
    "def chunk_transcript(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "    words = text.split(' ')\n",
    "    chunks = [words[i:i + chunk_size]\n",
    "              for i in range(0, len(words), chunk_size)]\n",
    "    create_directory(f\"{summary_title}/chunks\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(f'{summary_title}/chunks/chunk{i}.txt', 'w') as file:\n",
    "            if chunk[-1] != '.':\n",
    "                chunk[-1] += '.'\n",
    "            file.write(' '.join(chunk))\n",
    "\n",
    "\n",
    "def get_sorted_files_by_date(directory):\n",
    "    file_list = os.listdir(directory)\n",
    "    files_with_mtime = [(file, os.path.getmtime(\n",
    "        os.path.join(directory, file))) for file in file_list]\n",
    "    sorted_files = sorted(files_with_mtime, key=lambda x: x[1])\n",
    "    sorted_filenames = [file[0] for file in sorted_files]\n",
    "    return sorted_filenames\n",
    "\n",
    "\n",
    "def read_text_from_files(directory_path):\n",
    "    text_array = []\n",
    "    files = get_sorted_files_by_date(directory_path)\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                text_array.append(file.read())\n",
    "    return text_array\n",
    "\n",
    "\n",
    "def read_and_join_text_from_files(directory_path):\n",
    "    joined_text = \"\"\n",
    "    files = get_sorted_files_by_date(directory_path)\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                joined_text += file.read() + \"\\n\"\n",
    "    return joined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(summary_title):\n",
    "  shutil.rmtree(summary_title)\n",
    "create_directory(summary_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download audio then transcribe using openai whisper api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio download completed!\n"
     ]
    }
   ],
   "source": [
    "if TRANSCRIBE:\n",
    "    audio_path = download_audio_from_youtube(video_url, summary_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio_duration: 7638947ms\n",
      "audio file size: 46581451\n",
      "Chunks to transcribe: ['chunk_0.mp4', 'chunk_1200.mp4', 'chunk_2400.mp4', 'chunk_3600.mp4', 'chunk_4800.mp4', 'chunk_6000.mp4', 'chunk_7200.mp4']\n",
      "Transcribing chunk_0.mp4\n",
      "Transcribing chunk_1200.mp4\n",
      "Transcribing chunk_2400.mp4\n",
      "Transcribing chunk_3600.mp4\n",
      "Transcribing chunk_4800.mp4\n",
      "Transcribing chunk_6000.mp4\n",
      "Transcribing chunk_7200.mp4\n",
      "Total audio cost: $ 0.7638\n"
     ]
    }
   ],
   "source": [
    "if TRANSCRIBE:\n",
    "    if os.path.exists(f\"{summary_title}/transcript.txt\"):\n",
    "        os.remove(f\"{summary_title}/transcript.txt\")\n",
    "    if LOCAL_WHISPER:\n",
    "        model = whisper.load_model(\"small\")\n",
    "        transcript = model.transcribe(f\"{summary_title}/audio.mp4\")[\"text\"]\n",
    "        with open(f\"{summary_title}/transcript.txt\", \"w\") as f:\n",
    "            f.write(transcript)\n",
    "    else:\n",
    "        global audio_cost\n",
    "        audio = AudioSegment.from_file(f\"{summary_title}/audio.mp4\", format=\"mp4\")\n",
    "        audio_duration_ms = len(audio)\n",
    "        audio_cost += audio_duration_ms // 1000 * audio_cost_per_second\n",
    "        print(f\"Audio_duration: {audio_duration_ms}ms\")\n",
    "        audio_size = os.path.getsize(f\"{summary_title}/audio.mp4\");\n",
    "        print(f\"audio file size: {audio_size}\")\n",
    "        if audio_size > 25_000_000:\n",
    "            split_audio_into_chunks(audio, summary_title, chunk_duration=1200)\n",
    "            files = get_sorted_files_by_date(summary_title+\"/audio_chunks\")\n",
    "            print(f\"Chunks to transcribe: {files}\")\n",
    "            for file in files:\n",
    "                print(f\"Transcribing {file}\")\n",
    "                file = open(summary_title + \"/audio_chunks/\" + file, \"rb\")\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                  model=\"whisper-1\", \n",
    "                  file=file\n",
    "                ).text\n",
    "                # transcript = openai.Audio.transcribe(\"whisper-1\", file).text\n",
    "                with open(f\"{summary_title}/transcript.txt\", \"a\") as f:\n",
    "                    f.write(transcript)\n",
    "                    f.close()\n",
    "                file.close()\n",
    "        else:\n",
    "            file = open(summary_title + \"/\" + \"audio.mp4\", \"rb\")\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                  model=\"whisper-1\", \n",
    "                  file=file\n",
    "                ).text\n",
    "            with open(f\"{summary_title}/transcript.txt\", \"a\") as f:\n",
    "                f.write(transcript)\n",
    "                f.close()\n",
    "            file.close()\n",
    "        print(\"Total audio cost: $\", round(audio_cost, 4))\n",
    "else:\n",
    "    download_transcript(video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkify the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(summary_title+\"/chunks\"):\n",
    "    shutil.rmtree(summary_title+\"/chunks\")\n",
    "create_directory(summary_title+\"/chunks\")\n",
    "chunk_transcript_sentences(f'{summary_title}/transcript.txt', chunk_size, summary_title)\n",
    "# chunk_transcript(f'{summary_title}/transcript.txt')\n",
    "data = read_text_from_files(f\"{summary_title}/chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing 1 articles\n",
      "Tokens thus far: 20740 with a cost of $0.2166\n",
      "Total cost so far is: $0.217 tokens and $0.7638 audio\n"
     ]
    }
   ],
   "source": [
    "input_tokens = 0\n",
    "output_tokens = 0\n",
    "tokens_cost = 0\n",
    "\n",
    "data = read_text_from_files(f\"{summary_title}/chunks\")\n",
    "if os.path.exists(summary_title+\"/summaries\"):\n",
    "    shutil.rmtree(summary_title+\"/summaries\")\n",
    "create_directory(f\"{summary_title}/summaries\")\n",
    "\n",
    "if os.path.exists(summary_title+\"/instructions\"):\n",
    "    shutil.rmtree(summary_title+\"/instructions\")\n",
    "create_directory(f\"{summary_title}/instructions\")\n",
    "\n",
    "if os.path.exists(summary_title+\"/system\"):\n",
    "    shutil.rmtree(summary_title+\"/system\")\n",
    "create_directory(f\"{summary_title}/system\")\n",
    "\n",
    "print(f\"Summarizing {len(data)} articles\")\n",
    "multiple_summaries = len(data) > 1\n",
    "\n",
    "for i in range(len(data)):\n",
    "    messages = []\n",
    "    if i > 1:\n",
    "        with open(f\"{summary_title}/summaries/summary{i-2}.txt\", \"r\") as f:\n",
    "            sum1 = f.read()\n",
    "        with open(f\"{summary_title}/summaries/summary{i-1}.txt\", \"r\") as f:\n",
    "            sum2 = f.read()\n",
    "\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": \"Summarize the following as detailed as possible:\\n\" + sum1 + '\\n' + sum2})\n",
    "        system_message = summarize(messages)\n",
    "\n",
    "        system_message = \"Always only provide a detailed summary of the input. Don't answer questions or complete the text. The following is the context to keep in mind: \\n\" + system_message\n",
    "        with open(f\"{summary_title}/system/system{i}.txt\", \"w\") as f:\n",
    "            f.write(system_message)\n",
    "        messages = []\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    elif i > 0:\n",
    "        with open(f\"{summary_title}/summaries/summary{i-1}.txt\", \"r\") as f:\n",
    "            system_message = \"Always only provide a detailed summary of the input. Don't answer questions or complete the text. The following is the context to keep in mind: \\n\" + f.read()\n",
    "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": task + data[i]})\n",
    "\n",
    "    with open(f\"{summary_title}/instructions/instruction_{i}.txt\", \"a\") as file:\n",
    "        for msg in messages:\n",
    "            file.write(msg[\"role\"] + \":\\n\")\n",
    "            file.write(msg[\"content\"] + \"\\n\")\n",
    "    # print(f\"Summarizing article {i} with {len(instruction.split(' '))} words\")\n",
    "\n",
    "    response = summarize(messages)\n",
    "    with open(f\"{summary_title}/summaries/summary{i}.txt\", \"w\") as f:\n",
    "        f.write(response)\n",
    "print(f\"Total cost so far is: ${round(tokens_cost, 3)} tokens and ${audio_cost} audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join summaries together then summarize them altogether using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = f'{summary_title}/summaries'\n",
    "joined_summaries = read_and_join_text_from_files(directory_path)\n",
    "files = get_sorted_files_by_date(directory_path)\n",
    "multiple_summaries = len(files) > 1\n",
    "with open(f'{summary_title}/summaries/joined_summaries.txt', 'w') as file:\n",
    "    file.write(joined_summaries)\n",
    "if multiple_summaries:\n",
    "    print(f\"Summarizing joined summaries with {len(joined_summaries.split(' '))} words\")\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Make the following summaries flow as one long and detailed article: \\n\" + joined_summaries.rstrip()}]\n",
    "    with open(f\"{summary_title}/instructions/instruction.txt\", \"a\") as f:\n",
    "      for message in messages:\n",
    "        f.write(message[\"content\"] + \"\\n\")\n",
    "\n",
    "    sum = summarize(input=messages, model=\"gpt-4\")\n",
    "else:\n",
    "    sum = joined_summaries\n",
    "with open(f\"{summary_title}/summary.txt\", \"w\") as f:\n",
    "    f.write(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import pygame\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with open(f\"{summary_title}/summary.txt\", \"r\") as f:\n",
    "    story = f.read()\n",
    "\n",
    "def save_audio(response, count):\n",
    "    with open(f\"{summary_title}/openai_audio/story_{count}.mp3\", \"wb\") as f:\n",
    "        f.write(response[\"AudioStream\"].read())\n",
    "\n",
    "def stream_and_play(text, count):\n",
    "\n",
    "  response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=text,\n",
    "  )\n",
    "\n",
    "  # Convert the binary response content to a byte stream\n",
    "  # byte_stream = io.BytesIO(response.content)\n",
    "\n",
    "  # Read the audio data from the byte stream\n",
    "  # audio = AudioSegment.from_file(byte_stream, format=\"mp3\")\n",
    "  response.stream_to_file(f\"{summary_title}/openai_audio/story_{count}.mp3\")\n",
    "  # save_audio(audio, count)\n",
    "  # Play the audio\n",
    "  # play(audio)\n",
    "\n",
    "for count, section in enumerate(story.split('\\n\\n')):\n",
    "  audio = stream_and_play(section, count)\n",
    "# stream_and_play()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(file):\n",
    "  pygame.mixer.init()\n",
    "  pygame.mixer.music.load(file)\n",
    "  pygame.mixer.music.play()\n",
    "\n",
    "  while pygame.mixer.music.get_busy():\n",
    "    # Optional: add a delay to reduce CPU usage\n",
    "    time.sleep(0.1)  \n",
    "\n",
    "  return pygame.mixer.music\n",
    "\n",
    "for i in range(14):\n",
    "   read_audio(f\"{summary_title}/openai_audio/story_{i}.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# voice_id = \"Matthew\"\n",
    "# voice_id = \"Ruth\"\n",
    "voice_id = \"Stephen\"\n",
    "output_format = \"mp3\"\n",
    "import pygame\n",
    "\n",
    "if os.path.exists(summary_title+\"/audio\"):\n",
    "    shutil.rmtree(summary_title+\"/audio\")\n",
    "create_directory(f\"{summary_title}/audio\")\n",
    "\n",
    "# Create an Amazon Polly client\n",
    "polly_client = boto3.Session(\n",
    "    aws_access_key_id=os.getenv(\"polly_access_key_id\"),\n",
    "    aws_secret_access_key=os.getenv(\"polly_secret_key\"),\n",
    "    region_name='ca-central-1').client('polly')\n",
    "\n",
    "def generate_audio(input, count):\n",
    "  start_time = time.time()\n",
    "  audio = polly_client.synthesize_speech(\n",
    "      Text=input,\n",
    "      VoiceId=voice_id,\n",
    "      OutputFormat=output_format,\n",
    "      Engine=\"neural\"\n",
    "      )\n",
    "  save_audio(audio, count)\n",
    "  return audio\n",
    "\n",
    "def save_audio(response, count):\n",
    "  with open(f\"{summary_title}/audio/story_{count}.mp3\", \"wb\") as f:\n",
    "      f.write(response[\"AudioStream\"].read())\n",
    "\n",
    "def read_audio(file):\n",
    "  pygame.mixer.init()\n",
    "  pygame.mixer.music.load(file)\n",
    "  pygame.mixer.music.play()\n",
    "\n",
    "  while pygame.mixer.music.get_busy():\n",
    "    # Optional: add a delay to reduce CPU usage\n",
    "    time.sleep(0.1)  \n",
    "\n",
    "  return pygame.mixer.music\n",
    "\n",
    "for i in range(count):\n",
    "  media_player = read_audio(f\"{summary_title}/audio/story_{i}.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{summary_title}/summary.txt\", \"r\") as f:\n",
    "  story = f.read()\n",
    "count = 0\n",
    "for section in story.split('\\n\\n'):\n",
    "  audio = generate_audio(section, count)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(count):\n",
    "  section = story.split('\\n\\n')[i]\n",
    "  print(i, section)\n",
    "  media_player = read_audio(f\"{summary_title}/audio/story_{i}.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
