{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Given a youtube URL the following will download transcript then summarize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "import openai\n",
    "import os\n",
    "import whisper\n",
    "from dotenv import load_dotenv\n",
    "import youtube_dl\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import shutil\n",
    "from pytube import YouTube\n",
    "from pydub import AudioSegment\n",
    "import time\n",
    "import re\n",
    "import json\n",
    "load_dotenv()\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# video_url = 'https://www.youtube.com/watch?v=SNgoul4vyDM'\n",
    "# summary_title = \"ufo_hearing_whisper_transcribe_audio_gpt_4\"\n",
    "\n",
    "video_url = 'https://www.youtube.com/watch?v=co_MeKSnyAo'\n",
    "summary_title = \"lex_fridman_jared_kushner_399\"\n",
    "\n",
    "video_id = video_url.split('=')[1]\n",
    "task = \"Please provide a long detailed summary of the following transcript from a youtube podcast with Lex Fridman: \\n\"\n",
    "\n",
    "models = ['gpt-3.5-turbo', 'gpt-4', 'gpt-4-1106-preview']\n",
    "USE = 1\n",
    "TRANSCRIBE = True\n",
    "LOCAL_WHISPER = False\n",
    "tokens_cost = 0\n",
    "if USE == 0:\n",
    "    chunk_size = 2300\n",
    "    input_cost = 0.0015/1000\n",
    "    output_cost = 0.002/1000\n",
    "elif USE == 2:\n",
    "    chunk_size = 4000\n",
    "    input_cost = 0.01/1000\n",
    "    output_cost = 0.03/1000\n",
    "else:\n",
    "    chunk_size = 4000\n",
    "    input_cost = 0.03/1000\n",
    "    output_cost = 0.06/1000\n",
    "\n",
    "audio_cost_per_second = 0.006/60\n",
    "audio_cost = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_NZkk9WfoC6GstVDdO2jxrBig', assistant_id='asst_CThGowNyagEiGpe2Nit5nyWh', content=[MessageContentText(text=Text(annotations=[], value='The transcript begins with a conversation between Lex Fridman and Elon Musk, marking Musk\\'s fourth appearance on the Lex Fridman podcast. The discussion opens with a light-hearted reference to film music, leading to a more serious dialogue about the Roman Empire. Musk reflects on the prevalence of war and military conquest throughout history, noting that many empires and societies—including the Roman Empire—prioritized military capability and celebrated exceptional generals.\\n\\nThe conversation turns philosophical as Fridman questions Musk on whether war is inherent to human nature or a byproduct of societal structure. Musk positions himself as a proponent of peace, addressing the problematic nature of war. He argues that, while all creatures, including humans, engage in conflict as seen in the natural world, people possess a greater capacity to curb violent instincts compared to other species such as chimpanzees.\\n\\nMusk describes chimpanzee society as violent and without the concept of a \"just war,\" juxtaposing it with human societies that have the potential to choose peace. The dialogue then shifts to the specific context of Elon Musk\\'s advocacy for peace in current global conflicts, like those in Israel and Gaza.\\n\\nOn the topic of the Israeli-Hamas conflict, Musk shares his opinion on the goals of Hamas and suggests an unconventional approach for Israel. He advocates for conspicuous acts of kindness from Israel towards Palestinians, coupled with efforts to neutralize Hamas members, as a strategy to counteract hate and prevent the recruitment of new militants.\\n\\nMusk further discusses the effectiveness of violent retribution by referencing the phrase \"an eye for an eye makes everyone blind,\" pointing out its counterproductive nature in achieving long-term peace. The conversation broadens to consider the persistent existence of war, with Musk expressing pessimism about the complete eradication of conflict but pondering the balance between the scope of war and the potential for flourishing societies.\\n\\nFridman and Musk then contemplate the implications of expanding human civilization throughout the galaxy and the inevitability of galactic-scale war. Musk expresses concern over the growing scale of war and the lack of visceral understanding of its horrors among the Western population. He recalls the traumatic experiences of his grandfather in World War II as an example of the kind of impact war can have on individuals.\\n\\nThe summary provided above encompasses the approximately first 8% of the conversation, as visible in the initial section of the transcript. Please note that additional content from the rest of the transcript encompasses further topics not covered in this summary. If you wish to know more about the conversation, please let me know, and I can continue summarizing the remaining parts of the transcript.'), type='text')], created_at=1699923598, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_xLUCL8obp8VOysLy8vdaHxb1', thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_OokIsAsGbyroq0xd2MxAkYKC', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='summarize the transcript'), type='text')], created_at=1699923594, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_7wPXBqYEPmKdFgWrawJeyXvV', assistant_id='asst_CThGowNyagEiGpe2Nit5nyWh', content=[MessageContentText(text=Text(annotations=[], value=\"Certainly! But it seems I may have missed some context for what needs to be summarized. Could you please specify the content you want me to summarize? If you're referring to the file you've uploaded, I will open it and provide you with a detailed summary. Please let me know how you'd like to proceed.\"), type='text')], created_at=1699923565, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_JVGeMxhHWjdDpBhR2rWHt2KL', thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_yHHRzGZuAjXsYc3ld6g4HDoJ', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='summarize'), type='text')], created_at=1699923562, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_K3VORIjImSBVqIyNzB9LV0e6', assistant_id='asst_CThGowNyagEiGpe2Nit5nyWh', content=[MessageContentText(text=Text(annotations=[], value='The solution to the equation \\\\(3x + 11 = 14\\\\) is \\\\(x = 1\\\\).'), type='text')], created_at=1699923523, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_gLNghsoAMWGhtRPTpQj0GLuL', thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_C9kQa2sG3HDFbXG5MT4prAi4', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='I need to solve the equation `3x + 11 = 14`. Can you help me?'), type='text')], created_at=1699923475, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_cA5j369jlIjirWpO6Cv565Ux', assistant_id='asst_CThGowNyagEiGpe2Nit5nyWh', content=[MessageContentText(text=Text(annotations=[], value=\"Hello! How can I assist you today? If you have a specific request related to the file you've uploaded, please let me know, and I'll be happy to help.\"), type='text')], created_at=1699923128, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_5wvrHWRK4uS5eRdiRG6EbbkU', thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG'), ThreadMessage(id='msg_20tFeuxUskF6qkrBk0wVMKCQ', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='hi'), type='text')], created_at=1699923127, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_lh9BQEzGjfOWhKKYjn6jSkhG')], object='list', first_id='msg_NZkk9WfoC6GstVDdO2jxrBig', last_id='msg_20tFeuxUskF6qkrBk0wVMKCQ', has_more=False)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = client.files.create(\n",
    "  file=open(\"b1m_solved_urban_sprawl/transcript.txt\", \"rb\"),\n",
    "  purpose='assistants'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "  name=\"Summarizer\",\n",
    "  description=\"Your task is to provide detailed and comprehensive summaries of YouTube video transcripts. These summaries should capture all key ideas, main points, and important details presented in the transcript. Focus on maintaining the essence and flow of the original content while ensuring clarity and coherence in the summary. Your summaries should be long enough to encompass all critical information and insights from the transcript, aiming to give a complete understanding of the video's content.\",\n",
    "  model=\"gpt-4-1106-preview\",\n",
    "  tools=[{\"type\": \"retrieval\"}],\n",
    "  file_ids=[file.id]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "thread = client.beta.threads.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = client.beta.threads.runs.create(\n",
    "  thread_id=thread.id,\n",
    "  assistant_id=assistant.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[ThreadMessage](data=[ThreadMessage(id='msg_xE5hTluqqBdVn8UZz54ZcT7b', assistant_id='asst_MgG8KonDU56w7rUL9O1bWdd4', content=[MessageContentText(text=Text(annotations=[], value=\"The transcript appears to be a detailed account of architect Moshe Safdie's project, Habitat 67, which originally aimed to create a high-density housing solution in Montreal for Expo 67. Safdie's vision was to have modules stacked high like a hillside, providing each housing unit with a roof terrace. He faced budget constraints which forced a scale-back of the project from a community of 1,200 families to just 158 residences across three smaller pyramids.\\n\\nDespite the scale-down, Habitat 67 was a success and became a highly desirable place to live, with its long waitlist and long-term occupancy. However, the revolutionary impact on architecture it promised never fully materialized. Later, Safdie's architects and Epic Games used Unreal Engine to digitally complete Habitat 67 to its original design as a way to preserve and share it. They worked closely with Safdie, who was amazed by the results, noting that it would have been very convincing if he had this technology back in the '60s.\\n\\nThe project evidently captured a renewed interest in Safdie's ideas among a new generation of architects, who are considering how to leverage his concepts in modern urban developments. The transcript concludes with an invitation to explore the hillside model and mentions a podcast for further discussion on this and other topics related to the construction industry.\"), type='text')], created_at=1699924080, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_2PeI3tRPjUotICCvkhbM0vhv', thread_id='thread_qDzRBcaaN6weWIl2CD3sak7V'), ThreadMessage(id='msg_hC5rqh1fgRUZQo7IUmQNYjQX', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='summarize the transcript'), type='text')], created_at=1699924028, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_qDzRBcaaN6weWIl2CD3sak7V')], object='list', first_id='msg_xE5hTluqqBdVn8UZz54ZcT7b', last_id='msg_hC5rqh1fgRUZQo7IUmQNYjQX', has_more=False)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = client.beta.threads.messages.list(\n",
    "  thread_id=thread.id\n",
    ")\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"summarize the transcript\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio_from_youtube(video_url, output_path, file_name=\"audio.mp4\"):\n",
    "    \n",
    "    yt = YouTube(video_url)\n",
    "    audio_stream = yt.streams.filter(only_audio=True).first()\n",
    "    audio_stream.download(output_path, filename=file_name)\n",
    "    print(\"Audio download completed!\")\n",
    "    \n",
    "    \n",
    "    return output_path + \"/\" + file_name\n",
    "\n",
    "def split_audio_into_chunks(audio, output_folder, chunk_duration=900):\n",
    "    # audio = AudioSegment.from_file(audio_path, format=\"mp4\")\n",
    "    audio_duration_ms = len(audio)\n",
    "    \n",
    "    for start_time_ms in range(0, audio_duration_ms, chunk_duration * 1000):\n",
    "        end_time_ms = start_time_ms + (chunk_duration * 1000)\n",
    "        chunk = audio[start_time_ms:end_time_ms]\n",
    "        location = f\"{output_folder}/audio_chunks\"\n",
    "        if not os.path.exists(location):\n",
    "            os.makedirs(location)\n",
    "        output_file = f\"{output_folder}/audio_chunks/chunk_{start_time_ms//1000}.mp4\"\n",
    "        chunk.export(output_file, format=\"mp4\")\n",
    "\n",
    "\n",
    "def get_video_title(url):\n",
    "    with youtube_dl.YoutubeDL({}) as ydl:\n",
    "        info_dict = ydl.extract_info(url, download=False)\n",
    "        return info_dict.get('title', None)\n",
    "\n",
    "\n",
    "def summarize(input, model=models[USE]):\n",
    "    global input_tokens, output_tokens, input_cost, output_cost, tokens_cost\n",
    "  \n",
    "    completion = client.chat.completions.create(\n",
    "      model=model,\n",
    "      messages=input,\n",
    "      temperature=0\n",
    "    )\n",
    "    # completion = openai.ChatCompletion.create(\n",
    "    #     model=model,\n",
    "    #     temperature=0,\n",
    "    #     messages=input)\n",
    "\n",
    "    input_tokens += completion.usage.prompt_tokens\n",
    "    output_tokens += completion.usage.completion_tokens\n",
    "\n",
    "    tokens_cost = input_tokens*input_cost + output_tokens*output_cost\n",
    "    print(f\"Tokens thus far: {input_tokens + output_tokens} with a cost of ${round(tokens_cost, 4)}\")\n",
    "    reply_content = completion.choices[0].message.content\n",
    "    return reply_content\n",
    "\n",
    "\n",
    "def create_directory(dir_name):\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "\n",
    "\n",
    "def download_transcript(video_id):\n",
    "    video_id = video_id.split('=')[-1]\n",
    "    transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "    with open(f\"{summary_title}/transcript.txt\", \"w\") as file:\n",
    "        for line in transcript:\n",
    "            file.write(line['text'] + '\\n')\n",
    "\n",
    "\n",
    "def chunk_transcript_sentences(file_name, chunk_size, summary_title):\n",
    "    # Open and read the file\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split('(?<=[.!?]) +', text)\n",
    "\n",
    "    chunks, chunk = [], []\n",
    "    current_chunk_size = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sentence_words = sentence.split(' ')\n",
    "        sentence_length = len(sentence_words)\n",
    "\n",
    "        # If adding the next sentence doesn't exceed the chunk size, add it to the current chunk\n",
    "        if current_chunk_size + sentence_length <= chunk_size:\n",
    "            chunk.append(sentence)\n",
    "            current_chunk_size += sentence_length\n",
    "        else:\n",
    "            # Otherwise, finish the current chunk and start a new one\n",
    "            chunks.append(' '.join(chunk))\n",
    "            chunk = [sentence]\n",
    "            current_chunk_size = sentence_length\n",
    "\n",
    "    # Add the last chunk if it's non-empty\n",
    "    if chunk:\n",
    "        chunks.append(' '.join(chunk))\n",
    "\n",
    "    create_directory(f\"{summary_title}/chunks\")\n",
    "\n",
    "    # Write each chunk to a file\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(f'{summary_title}/chunks/chunk{i}.txt', 'w') as file:\n",
    "            file.write(chunk)\n",
    "\n",
    "\n",
    "def chunk_transcript(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        text = file.read().replace('\\n', ' ')\n",
    "    words = text.split(' ')\n",
    "    chunks = [words[i:i + chunk_size]\n",
    "              for i in range(0, len(words), chunk_size)]\n",
    "    create_directory(f\"{summary_title}/chunks\")\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        with open(f'{summary_title}/chunks/chunk{i}.txt', 'w') as file:\n",
    "            if chunk[-1] != '.':\n",
    "                chunk[-1] += '.'\n",
    "            file.write(' '.join(chunk))\n",
    "\n",
    "\n",
    "def get_sorted_files_by_date(directory):\n",
    "    file_list = os.listdir(directory)\n",
    "    files_with_mtime = [(file, os.path.getmtime(\n",
    "        os.path.join(directory, file))) for file in file_list]\n",
    "    sorted_files = sorted(files_with_mtime, key=lambda x: x[1])\n",
    "    sorted_filenames = [file[0] for file in sorted_files]\n",
    "    return sorted_filenames\n",
    "\n",
    "\n",
    "def read_text_from_files(directory_path):\n",
    "    text_array = []\n",
    "    files = get_sorted_files_by_date(directory_path)\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                text_array.append(file.read())\n",
    "    return text_array\n",
    "\n",
    "\n",
    "def read_and_join_text_from_files(directory_path):\n",
    "    joined_text = \"\"\n",
    "    files = get_sorted_files_by_date(directory_path)\n",
    "    for file_name in files:\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r') as file:\n",
    "                joined_text += file.read() + \"\\n\"\n",
    "    return joined_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(summary_title):\n",
    "  shutil.rmtree(summary_title)\n",
    "create_directory(summary_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download audio then transcribe using openai whisper api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio download completed!\n"
     ]
    }
   ],
   "source": [
    "if TRANSCRIBE:\n",
    "    audio_path = download_audio_from_youtube(video_url, summary_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio_duration: 13718814ms\n",
      "audio file size: 83654578\n",
      "Chunks to transcribe: ['chunk_0.mp4', 'chunk_1200.mp4', 'chunk_2400.mp4', 'chunk_3600.mp4', 'chunk_4800.mp4', 'chunk_6000.mp4', 'chunk_7200.mp4', 'chunk_8400.mp4', 'chunk_9600.mp4', 'chunk_10800.mp4', 'chunk_12000.mp4', 'chunk_13200.mp4']\n",
      "Transcribing chunk_0.mp4\n",
      "Transcribing chunk_1200.mp4\n",
      "Transcribing chunk_2400.mp4\n",
      "Transcribing chunk_3600.mp4\n",
      "Transcribing chunk_4800.mp4\n",
      "Transcribing chunk_6000.mp4\n",
      "Transcribing chunk_7200.mp4\n",
      "Transcribing chunk_8400.mp4\n",
      "Transcribing chunk_9600.mp4\n",
      "Transcribing chunk_10800.mp4\n",
      "Transcribing chunk_12000.mp4\n",
      "Transcribing chunk_13200.mp4\n",
      "Total audio cost: $ 1.3718\n"
     ]
    }
   ],
   "source": [
    "if TRANSCRIBE:\n",
    "    if os.path.exists(f\"{summary_title}/transcript.txt\"):\n",
    "        os.remove(f\"{summary_title}/transcript.txt\")\n",
    "    if LOCAL_WHISPER:\n",
    "        model = whisper.load_model(\"small\")\n",
    "        transcript = model.transcribe(f\"{summary_title}/audio.mp4\")[\"text\"]\n",
    "        with open(f\"{summary_title}/transcript.txt\", \"w\") as f:\n",
    "            f.write(transcript)\n",
    "    else:\n",
    "        global audio_cost\n",
    "        audio = AudioSegment.from_file(f\"{summary_title}/audio.mp4\", format=\"mp4\")\n",
    "        audio_duration_ms = len(audio)\n",
    "        audio_cost += audio_duration_ms // 1000 * audio_cost_per_second\n",
    "        print(f\"Audio_duration: {audio_duration_ms}ms\")\n",
    "        audio_size = os.path.getsize(f\"{summary_title}/audio.mp4\");\n",
    "        print(f\"audio file size: {audio_size}\")\n",
    "        if audio_size > 25_000_000:\n",
    "            split_audio_into_chunks(audio, summary_title, chunk_duration=1200)\n",
    "            files = get_sorted_files_by_date(summary_title+\"/audio_chunks\")\n",
    "            print(f\"Chunks to transcribe: {files}\")\n",
    "            for file in files:\n",
    "                print(f\"Transcribing {file}\")\n",
    "                file = open(summary_title + \"/audio_chunks/\" + file, \"rb\")\n",
    "                transcript = client.audio.transcriptions.create(\n",
    "                  model=\"whisper-1\", \n",
    "                  file=file\n",
    "                ).text\n",
    "                # transcript = openai.Audio.transcribe(\"whisper-1\", file).text\n",
    "                with open(f\"{summary_title}/transcript.txt\", \"a\") as f:\n",
    "                    f.write(transcript)\n",
    "                    f.close()\n",
    "                file.close()\n",
    "        else:\n",
    "            file = open(summary_title + \"/\" + \"audio.mp4\", \"rb\")\n",
    "            transcript = client.audio.transcriptions.create(\n",
    "                  model=\"whisper-1\", \n",
    "                  file=file\n",
    "                ).text\n",
    "            with open(f\"{summary_title}/transcript.txt\", \"a\") as f:\n",
    "                f.write(transcript)\n",
    "                f.close()\n",
    "            file.close()\n",
    "        print(\"Total audio cost: $\", round(audio_cost, 4))\n",
    "else:\n",
    "    download_transcript(video_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunkify the transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(summary_title+\"/chunks\"):\n",
    "    shutil.rmtree(summary_title+\"/chunks\")\n",
    "create_directory(summary_title+\"/chunks\")\n",
    "chunk_transcript_sentences(f'{summary_title}/transcript.txt', chunk_size, summary_title)\n",
    "# chunk_transcript(f'{summary_title}/transcript.txt')\n",
    "data = read_text_from_files(f\"{summary_title}/chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize each chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing 11 articles\n",
      "Tokens thus far: 4831 with a cost of $0.1468\n",
      "Tokens thus far: 9763 with a cost of $0.2965\n",
      "Tokens thus far: 10035 with a cost of $0.3087\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m         file\u001b[39m.\u001b[39mwrite(msg[\u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# print(f\"Summarizing article {i} with {len(instruction.split(' '))} words\")\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m response \u001b[39m=\u001b[39m summarize(messages)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msummary_title\u001b[39m}\u001b[39;00m\u001b[39m/summaries/summary\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mw\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m     f\u001b[39m.\u001b[39mwrite(response)\n",
      "\u001b[1;32m/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msummarize\u001b[39m(\u001b[39minput\u001b[39m, model\u001b[39m=\u001b[39mmodels[USE]):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mglobal\u001b[39;00m input_tokens, output_tokens, input_cost, output_cost, tokens_cost\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     completion \u001b[39m=\u001b[39m client\u001b[39m.\u001b[39;49mchat\u001b[39m.\u001b[39;49mcompletions\u001b[39m.\u001b[39;49mcreate(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m       model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m       messages\u001b[39m=\u001b[39;49m\u001b[39minput\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m       temperature\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39m# completion = openai.ChatCompletion.create(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     \u001b[39m#     model=model,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m     \u001b[39m#     temperature=0,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m#     messages=input)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/marinus/Documents/Development/Playground/openai_api/summarize/summarize_youtube_videos.ipynb#X13sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     input_tokens \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m completion\u001b[39m.\u001b[39musage\u001b[39m.\u001b[39mprompt_tokens\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/openai/_utils/_utils.py:299\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m             msg \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mMissing required argument: \u001b[39m\u001b[39m{\u001b[39;00mquote(missing[\u001b[39m0\u001b[39m])\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    298\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 299\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/openai/resources/chat/completions.py:556\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39m@required_args\u001b[39m([\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], [\u001b[39m\"\u001b[39m\u001b[39mmessages\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    513\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[1;32m    514\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m     timeout: \u001b[39mfloat\u001b[39m \u001b[39m|\u001b[39m httpx\u001b[39m.\u001b[39mTimeout \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m|\u001b[39m NotGiven \u001b[39m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    555\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ChatCompletion \u001b[39m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 556\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_post(\n\u001b[1;32m    557\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39m/chat/completions\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    558\u001b[0m         body\u001b[39m=\u001b[39;49mmaybe_transform(\n\u001b[1;32m    559\u001b[0m             {\n\u001b[1;32m    560\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmessages\u001b[39;49m\u001b[39m\"\u001b[39;49m: messages,\n\u001b[1;32m    561\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m\"\u001b[39;49m: model,\n\u001b[1;32m    562\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfrequency_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: frequency_penalty,\n\u001b[1;32m    563\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunction_call\u001b[39;49m\u001b[39m\"\u001b[39;49m: function_call,\n\u001b[1;32m    564\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mfunctions\u001b[39;49m\u001b[39m\"\u001b[39;49m: functions,\n\u001b[1;32m    565\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mlogit_bias\u001b[39;49m\u001b[39m\"\u001b[39;49m: logit_bias,\n\u001b[1;32m    566\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mmax_tokens\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_tokens,\n\u001b[1;32m    567\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m: n,\n\u001b[1;32m    568\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mpresence_penalty\u001b[39;49m\u001b[39m\"\u001b[39;49m: presence_penalty,\n\u001b[1;32m    569\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mresponse_format\u001b[39;49m\u001b[39m\"\u001b[39;49m: response_format,\n\u001b[1;32m    570\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mseed\u001b[39;49m\u001b[39m\"\u001b[39;49m: seed,\n\u001b[1;32m    571\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstop\u001b[39;49m\u001b[39m\"\u001b[39;49m: stop,\n\u001b[1;32m    572\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mstream\u001b[39;49m\u001b[39m\"\u001b[39;49m: stream,\n\u001b[1;32m    573\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtemperature\u001b[39;49m\u001b[39m\"\u001b[39;49m: temperature,\n\u001b[1;32m    574\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtool_choice\u001b[39;49m\u001b[39m\"\u001b[39;49m: tool_choice,\n\u001b[1;32m    575\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtools\u001b[39;49m\u001b[39m\"\u001b[39;49m: tools,\n\u001b[1;32m    576\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39mtop_p\u001b[39;49m\u001b[39m\"\u001b[39;49m: top_p,\n\u001b[1;32m    577\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m\"\u001b[39;49m: user,\n\u001b[1;32m    578\u001b[0m             },\n\u001b[1;32m    579\u001b[0m             completion_create_params\u001b[39m.\u001b[39;49mCompletionCreateParams,\n\u001b[1;32m    580\u001b[0m         ),\n\u001b[1;32m    581\u001b[0m         options\u001b[39m=\u001b[39;49mmake_request_options(\n\u001b[1;32m    582\u001b[0m             extra_headers\u001b[39m=\u001b[39;49mextra_headers, extra_query\u001b[39m=\u001b[39;49mextra_query, extra_body\u001b[39m=\u001b[39;49mextra_body, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    583\u001b[0m         ),\n\u001b[1;32m    584\u001b[0m         cast_to\u001b[39m=\u001b[39;49mChatCompletion,\n\u001b[1;32m    585\u001b[0m         stream\u001b[39m=\u001b[39;49mstream \u001b[39mor\u001b[39;49;00m \u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    586\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mStream[ChatCompletionChunk],\n\u001b[1;32m    587\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/openai/_base_client.py:1055\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpost\u001b[39m(\n\u001b[1;32m   1042\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1043\u001b[0m     path: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1050\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1051\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[1;32m   1052\u001b[0m     opts \u001b[39m=\u001b[39m FinalRequestOptions\u001b[39m.\u001b[39mconstruct(\n\u001b[1;32m   1053\u001b[0m         method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m, url\u001b[39m=\u001b[39mpath, json_data\u001b[39m=\u001b[39mbody, files\u001b[39m=\u001b[39mto_httpx_files(files), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions\n\u001b[1;32m   1054\u001b[0m     )\n\u001b[0;32m-> 1055\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(ResponseT, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest(cast_to, opts, stream\u001b[39m=\u001b[39;49mstream, stream_cls\u001b[39m=\u001b[39;49mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/openai/_base_client.py:834\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrequest\u001b[39m(\n\u001b[1;32m    826\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    827\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    832\u001b[0m     stream_cls: \u001b[39mtype\u001b[39m[_StreamT] \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    833\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ResponseT \u001b[39m|\u001b[39m _StreamT:\n\u001b[0;32m--> 834\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(\n\u001b[1;32m    835\u001b[0m         cast_to\u001b[39m=\u001b[39;49mcast_to,\n\u001b[1;32m    836\u001b[0m         options\u001b[39m=\u001b[39;49moptions,\n\u001b[1;32m    837\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m    838\u001b[0m         stream_cls\u001b[39m=\u001b[39;49mstream_cls,\n\u001b[1;32m    839\u001b[0m         remaining_retries\u001b[39m=\u001b[39;49mremaining_retries,\n\u001b[1;32m    840\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/openai/_base_client.py:858\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_request(request)\n\u001b[1;32m    857\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 858\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_client\u001b[39m.\u001b[39;49msend(request, auth\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcustom_auth, stream\u001b[39m=\u001b[39;49mstream)\n\u001b[1;32m    859\u001b[0m     log\u001b[39m.\u001b[39mdebug(\n\u001b[1;32m    860\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mHTTP Request: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m, request\u001b[39m.\u001b[39mmethod, request\u001b[39m.\u001b[39murl, response\u001b[39m.\u001b[39mstatus_code, response\u001b[39m.\u001b[39mreason_phrase\n\u001b[1;32m    861\u001b[0m     )\n\u001b[1;32m    862\u001b[0m     response\u001b[39m.\u001b[39mraise_for_status()\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpx/_client.py:908\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    900\u001b[0m follow_redirects \u001b[39m=\u001b[39m (\n\u001b[1;32m    901\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfollow_redirects\n\u001b[1;32m    902\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    903\u001b[0m     \u001b[39melse\u001b[39;00m follow_redirects\n\u001b[1;32m    904\u001b[0m )\n\u001b[1;32m    906\u001b[0m auth \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 908\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_auth(\n\u001b[1;32m    909\u001b[0m     request,\n\u001b[1;32m    910\u001b[0m     auth\u001b[39m=\u001b[39;49mauth,\n\u001b[1;32m    911\u001b[0m     follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    912\u001b[0m     history\u001b[39m=\u001b[39;49m[],\n\u001b[1;32m    913\u001b[0m )\n\u001b[1;32m    914\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    915\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpx/_client.py:936\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    933\u001b[0m request \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(auth_flow)\n\u001b[1;32m    935\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_handling_redirects(\n\u001b[1;32m    937\u001b[0m         request,\n\u001b[1;32m    938\u001b[0m         follow_redirects\u001b[39m=\u001b[39;49mfollow_redirects,\n\u001b[1;32m    939\u001b[0m         history\u001b[39m=\u001b[39;49mhistory,\n\u001b[1;32m    940\u001b[0m     )\n\u001b[1;32m    941\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    942\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpx/_client.py:973\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    970\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mrequest\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    971\u001b[0m     hook(request)\n\u001b[0;32m--> 973\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_send_single_request(request)\n\u001b[1;32m    974\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    975\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_hooks[\u001b[39m\"\u001b[39m\u001b[39mresponse\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpx/_client.py:1009\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1004\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1005\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m     )\n\u001b[1;32m   1008\u001b[0m \u001b[39mwith\u001b[39;00m request_context(request\u001b[39m=\u001b[39mrequest):\n\u001b[0;32m-> 1009\u001b[0m     response \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m   1011\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(response\u001b[39m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1013\u001b[0m response\u001b[39m.\u001b[39mrequest \u001b[39m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpx/_transports/default.py:218\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    205\u001b[0m req \u001b[39m=\u001b[39m httpcore\u001b[39m.\u001b[39mRequest(\n\u001b[1;32m    206\u001b[0m     method\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mmethod,\n\u001b[1;32m    207\u001b[0m     url\u001b[39m=\u001b[39mhttpcore\u001b[39m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    215\u001b[0m     extensions\u001b[39m=\u001b[39mrequest\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    216\u001b[0m )\n\u001b[1;32m    217\u001b[0m \u001b[39mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 218\u001b[0m     resp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_pool\u001b[39m.\u001b[39;49mhandle_request(req)\n\u001b[1;32m    220\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(resp\u001b[39m.\u001b[39mstream, typing\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m    222\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    223\u001b[0m     status_code\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mstatus,\n\u001b[1;32m    224\u001b[0m     headers\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mheaders,\n\u001b[1;32m    225\u001b[0m     stream\u001b[39m=\u001b[39mResponseStream(resp\u001b[39m.\u001b[39mstream),\n\u001b[1;32m    226\u001b[0m     extensions\u001b[39m=\u001b[39mresp\u001b[39m.\u001b[39mextensions,\n\u001b[1;32m    227\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:253\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    252\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresponse_closed(status)\n\u001b[0;32m--> 253\u001b[0m     \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    254\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    255\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/connection_pool.py:237\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[39mraise\u001b[39;00m exc\n\u001b[1;32m    236\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49mhandle_request(request)\n\u001b[1;32m    238\u001b[0m \u001b[39mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    239\u001b[0m     \u001b[39m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[39m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[39m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     \u001b[39m# up as HTTP/1.1.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pool_lock:\n\u001b[1;32m    247\u001b[0m         \u001b[39m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[1;32m    248\u001b[0m         \u001b[39m# status so that the request becomes queued again.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/connection.py:90\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m     88\u001b[0m         \u001b[39mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[0;32m---> 90\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection\u001b[39m.\u001b[39;49mhandle_request(request)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/http11.py:112\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\u001b[39m\"\u001b[39m\u001b[39mhttp11.response_closed\u001b[39m\u001b[39m\"\u001b[39m, request) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m    111\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_closed()\n\u001b[0;32m--> 112\u001b[0m \u001b[39mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/http11.py:91\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_send_request_body(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     83\u001b[0m \u001b[39mwith\u001b[39;00m Trace(\n\u001b[1;32m     84\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mhttp11.receive_response_headers\u001b[39m\u001b[39m\"\u001b[39m, request, kwargs\n\u001b[1;32m     85\u001b[0m ) \u001b[39mas\u001b[39;00m trace:\n\u001b[1;32m     86\u001b[0m     (\n\u001b[1;32m     87\u001b[0m         http_version,\n\u001b[1;32m     88\u001b[0m         status,\n\u001b[1;32m     89\u001b[0m         reason_phrase,\n\u001b[1;32m     90\u001b[0m         headers,\n\u001b[0;32m---> 91\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_response_headers(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     92\u001b[0m     trace\u001b[39m.\u001b[39mreturn_value \u001b[39m=\u001b[39m (\n\u001b[1;32m     93\u001b[0m         http_version,\n\u001b[1;32m     94\u001b[0m         status,\n\u001b[1;32m     95\u001b[0m         reason_phrase,\n\u001b[1;32m     96\u001b[0m         headers,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     99\u001b[0m \u001b[39mreturn\u001b[39;00m Response(\n\u001b[1;32m    100\u001b[0m     status\u001b[39m=\u001b[39mstatus,\n\u001b[1;32m    101\u001b[0m     headers\u001b[39m=\u001b[39mheaders,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    107\u001b[0m     },\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/http11.py:155\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    152\u001b[0m timeout \u001b[39m=\u001b[39m timeouts\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mread\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    154\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 155\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_receive_event(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[1;32m    156\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(event, h11\u001b[39m.\u001b[39mResponse):\n\u001b[1;32m    157\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/_sync/http11.py:191\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    188\u001b[0m     event \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mnext_event()\n\u001b[1;32m    190\u001b[0m \u001b[39mif\u001b[39;00m event \u001b[39mis\u001b[39;00m h11\u001b[39m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 191\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_network_stream\u001b[39m.\u001b[39;49mread(\n\u001b[1;32m    192\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mREAD_NUM_BYTES, timeout\u001b[39m=\u001b[39;49mtimeout\n\u001b[1;32m    193\u001b[0m     )\n\u001b[1;32m    195\u001b[0m     \u001b[39m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[39m#\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[39m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[39m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[39m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[39mif\u001b[39;00m data \u001b[39m==\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_h11_state\u001b[39m.\u001b[39mtheir_state \u001b[39m==\u001b[39m h11\u001b[39m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/site-packages/httpcore/backends/sync.py:28\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[39mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m     27\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sock\u001b[39m.\u001b[39msettimeout(timeout)\n\u001b[0;32m---> 28\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv(max_bytes)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/ssl.py:1226\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1223\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1224\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1225\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1226\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(buflen)\n\u001b[1;32m   1227\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1228\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/gpu_venv/lib/python3.8/ssl.py:1101\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m)\n\u001b[1;32m   1102\u001b[0m \u001b[39mexcept\u001b[39;00m SSLError \u001b[39mas\u001b[39;00m x:\n\u001b[1;32m   1103\u001b[0m     \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m SSL_ERROR_EOF \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_tokens = 0\n",
    "output_tokens = 0\n",
    "tokens_cost = 0\n",
    "\n",
    "data = read_text_from_files(f\"{summary_title}/chunks\")\n",
    "if os.path.exists(summary_title+\"/summaries\"):\n",
    "    shutil.rmtree(summary_title+\"/summaries\")\n",
    "create_directory(f\"{summary_title}/summaries\")\n",
    "\n",
    "if os.path.exists(summary_title+\"/instructions\"):\n",
    "    shutil.rmtree(summary_title+\"/instructions\")\n",
    "create_directory(f\"{summary_title}/instructions\")\n",
    "\n",
    "if os.path.exists(summary_title+\"/system\"):\n",
    "    shutil.rmtree(summary_title+\"/system\")\n",
    "create_directory(f\"{summary_title}/system\")\n",
    "\n",
    "print(f\"Summarizing {len(data)} articles\")\n",
    "multiple_summaries = len(data) > 1\n",
    "\n",
    "for i in range(len(data)):\n",
    "    messages = []\n",
    "    if i > 1:\n",
    "        with open(f\"{summary_title}/summaries/summary{i-2}.txt\", \"r\") as f:\n",
    "            sum1 = f.read()\n",
    "        with open(f\"{summary_title}/summaries/summary{i-1}.txt\", \"r\") as f:\n",
    "            sum2 = f.read()\n",
    "\n",
    "        messages.append(\n",
    "            {\"role\": \"user\", \"content\": \"Summarize the following as detailed as possible:\\n\" + sum1 + '\\n' + sum2})\n",
    "        system_message = summarize(messages)\n",
    "\n",
    "        system_message = \"Always only provide a detailed summary of the input. Don't answer questions or complete the text. The following is the context to keep in mind: \\n\" + system_message\n",
    "        with open(f\"{summary_title}/system/system{i}.txt\", \"w\") as f:\n",
    "            f.write(system_message)\n",
    "        messages = []\n",
    "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    elif i > 0:\n",
    "        with open(f\"{summary_title}/summaries/summary{i-1}.txt\", \"r\") as f:\n",
    "            system_message = \"Always only provide a detailed summary of the input. Don't answer questions or complete the text. The following is the context to keep in mind: \\n\" + f.read()\n",
    "            messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": task + data[i]})\n",
    "\n",
    "    with open(f\"{summary_title}/instructions/instruction_{i}.txt\", \"a\") as file:\n",
    "        for msg in messages:\n",
    "            file.write(msg[\"role\"] + \":\\n\")\n",
    "            file.write(msg[\"content\"] + \"\\n\")\n",
    "    # print(f\"Summarizing article {i} with {len(instruction.split(' '))} words\")\n",
    "\n",
    "    response = summarize(messages)\n",
    "    with open(f\"{summary_title}/summaries/summary{i}.txt\", \"w\") as f:\n",
    "        f.write(response)\n",
    "print(f\"Total cost so far is: ${round(tokens_cost, 3)} tokens and ${audio_cost} audio\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join summaries together then summarize them altogether using GPT-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing joined summaries with 2966 words\n",
      "Tokens thus far: 71831 with a cost of $2.3865\n"
     ]
    }
   ],
   "source": [
    "directory_path = f'{summary_title}/summaries'\n",
    "joined_summaries = read_and_join_text_from_files(directory_path)\n",
    "files = get_sorted_files_by_date(directory_path)\n",
    "multiple_summaries = len(files) > 1\n",
    "with open(f'{summary_title}/summaries/joined_summaries.txt', 'w') as file:\n",
    "    file.write(joined_summaries)\n",
    "if multiple_summaries:\n",
    "    print(f\"Summarizing joined summaries with {len(joined_summaries.split(' '))} words\")\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Make the following summaries flow as one long and detailed article: \\n\" + joined_summaries.rstrip()}]\n",
    "    with open(f\"{summary_title}/instructions/instruction.txt\", \"a\") as f:\n",
    "      for message in messages:\n",
    "        f.write(message[\"content\"] + \"\\n\")\n",
    "\n",
    "    sum = summarize(input=messages, model=\"gpt-4\")\n",
    "else:\n",
    "    sum = joined_summaries\n",
    "with open(f\"{summary_title}/summary.txt\", \"w\") as f:\n",
    "    f.write(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from openai import OpenAI\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import pygame\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "with open(f\"{summary_title}/summary.txt\", \"r\") as f:\n",
    "    story = f.read()\n",
    "\n",
    "def save_audio(response, count):\n",
    "    with open(f\"{summary_title}/openai_audio/story_{count}.mp3\", \"wb\") as f:\n",
    "        f.write(response[\"AudioStream\"].read())\n",
    "\n",
    "def stream_and_play(text, count):\n",
    "\n",
    "  response = client.audio.speech.create(\n",
    "    model=\"tts-1\",\n",
    "    voice=\"alloy\",\n",
    "    input=text,\n",
    "  )\n",
    "\n",
    "  # Convert the binary response content to a byte stream\n",
    "  # byte_stream = io.BytesIO(response.content)\n",
    "\n",
    "  # Read the audio data from the byte stream\n",
    "  # audio = AudioSegment.from_file(byte_stream, format=\"mp3\")\n",
    "  response.stream_to_file(f\"{summary_title}/openai_audio/story_{count}.mp3\")\n",
    "  # save_audio(audio, count)\n",
    "  # Play the audio\n",
    "  # play(audio)\n",
    "\n",
    "for count, section in enumerate(story.split('\\n\\n')):\n",
    "  audio = stream_and_play(section, count)\n",
    "# stream_and_play()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_audio(file):\n",
    "  pygame.mixer.init()\n",
    "  pygame.mixer.music.load(file)\n",
    "  pygame.mixer.music.play()\n",
    "\n",
    "  while pygame.mixer.music.get_busy():\n",
    "    # Optional: add a delay to reduce CPU usage\n",
    "    time.sleep(0.1)  \n",
    "\n",
    "  return pygame.mixer.music\n",
    "\n",
    "for i in range(14):\n",
    "   read_audio(f\"{summary_title}/openai_audio/story_{i}.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "# voice_id = \"Matthew\"\n",
    "# voice_id = \"Ruth\"\n",
    "voice_id = \"Stephen\"\n",
    "output_format = \"mp3\"\n",
    "import pygame\n",
    "\n",
    "if os.path.exists(summary_title+\"/audio\"):\n",
    "    shutil.rmtree(summary_title+\"/audio\")\n",
    "create_directory(f\"{summary_title}/audio\")\n",
    "\n",
    "# Create an Amazon Polly client\n",
    "polly_client = boto3.Session(\n",
    "    aws_access_key_id=os.getenv(\"polly_access_key_id\"),\n",
    "    aws_secret_access_key=os.getenv(\"polly_secret_key\"),\n",
    "    region_name='ca-central-1').client('polly')\n",
    "\n",
    "def generate_audio(input, count):\n",
    "  start_time = time.time()\n",
    "  audio = polly_client.synthesize_speech(\n",
    "      Text=input,\n",
    "      VoiceId=voice_id,\n",
    "      OutputFormat=output_format,\n",
    "      Engine=\"neural\"\n",
    "      )\n",
    "  save_audio(audio, count)\n",
    "  return audio\n",
    "\n",
    "def save_audio(response, count):\n",
    "  with open(f\"{summary_title}/audio/story_{count}.mp3\", \"wb\") as f:\n",
    "      f.write(response[\"AudioStream\"].read())\n",
    "\n",
    "def read_audio(file):\n",
    "  pygame.mixer.init()\n",
    "  pygame.mixer.music.load(file)\n",
    "  pygame.mixer.music.play()\n",
    "\n",
    "  while pygame.mixer.music.get_busy():\n",
    "    # Optional: add a delay to reduce CPU usage\n",
    "    time.sleep(0.1)  \n",
    "\n",
    "  return pygame.mixer.music\n",
    "\n",
    "for i in range(count):\n",
    "  media_player = read_audio(f\"{summary_title}/audio/story_{i}.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{summary_title}/summary.txt\", \"r\") as f:\n",
    "  story = f.read()\n",
    "count = 0\n",
    "for section in story.split('\\n\\n'):\n",
    "  audio = generate_audio(section, count)\n",
    "  count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(count):\n",
    "  section = story.split('\\n\\n')[i]\n",
    "  print(i, section)\n",
    "  media_player = read_audio(f\"{summary_title}/audio/story_{i}.mp3\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
