welcome everybody to a video on the Falcon large language model falcon 40b instruct is the Top Model on the hugging face large language model leaderboards but how good is it in practice and what might we actually use it for today I hope to answer those questions for you first off there are two size variants 40b for 40 billion parameters and 7B for 7 billion as well as other fine-tuned variants the main ones being instruction or instruct fine-tuned for Pure text generation or if you wish to actually fine-tune the model to some specific task or a context subject that sort of thing you'd probably want to use the pre-trained bass variants but for chat Bots q a and other sorts of back and forth correspondence then you're probably going to want to use the instruct variant and then if you wanted to fine tune a conversational model then you might actually further fine tune the instruct variant rather than the bass these models are all available under the Apache 2.0 license which is very open and permissive of for distribution and commercial use and so on so this makes this a very business friendly model to use finally the AI team behind the Falcon models the Technology Innovation institutes currently have an open call for proposals for them to provide essentially you with compute grant money for projects that utilize the Falcon model but more on that in a bit to start these models are available on hugging phase via the Transformers Library while the 7 billion parameter model is a little more comfortable to run locally needing a mere 10 ish gigabytes of memory at 8-bit the 40 billion parameter variant can be a little more challenging wanting more like 45 to 55 gigabytes at 8 Bits and 100 plus at 16 depending on context length you can feel free to run these locally even on CPU and RAM if you want but I would personally suggest Lambda for the two dollar an hour h100 80 gigabyte card instances they don't pay me to say that it's just a fact they are the best right now in terms of price to performance on cloud hosting getting set up to actually run the Falcon models is most likely going to be just upgrading to torch 2.0 since I imagine most people are still on 1.x in some form in the description of this video I will have a link to my GitHub with all relevant code and Snippets including the install shell script that I personally used with Lambda to get things rolling but this would work on AWS or other platforms as well even locally from here we can check out a basic example of Falcon 40b's performance and quality and in this case it's just continuing a thought of course though this is an instruct model which can be more geared towards things like instructions or a conversation so let's try that and as you can see already the Falcon model has a pretty good grasp of the you know just natural language right and it's really quite interesting to me how fast this like bar to be impressed by large language models has moved since chat GPT hit the scene late last last year like a year ago just a year ago this alone would have been just huge like this just just Falcon 40b's performance right now like would have been uh just massive huge breaking news and when chat gbt came out it was and suddenly we just we just happened we just live in a world now where we have these models that can speak and generate text as if we're speaking to a human or another human is writing to us and it's an it's good enough to convince other people that it is another human indeed writing to us and I think what makes Falcon and Falcon 40 be especially huge right now is rather than needing to run all of your queries through open AI you can just download and run Falcon on your own or you can fine tune it further it's yours to do with as you please and it's just crazy to me the the the progress that we've made in just one year in Ai and what's actually available to people to use like right now in software develop element and just the just an AI that is available to you and now ai that is available literally that you could just download it's yours you do whatever you want with it from here that's just insane it's just insane agile students see in some of these examples I actually think Falcon 40b is very comparable to Chad gbt's base model the GP or GPT 3.5 uh it's a little inferior to gpt4 but we'll talk a little bit more about some reasons why I think that is but also it's just a vastly smaller model the than gbt4 but I think we actually could probably eke out a lot more performance than just what the model itself right now is outputting so how intelligent is Falcon for all these examples I'm going to just use the Falcon 40b instruct model from my very brief testing I would likely classify Falcon 7B as best suited really for either like few shot learning examples or even likely as a model that you'd further fine-tune to a very something more specific whereas Falcon 40b especially that instruct variant is much more suitable for just general use and working right out of the box to start some fairly random General Knowledge Questions here I'm showing what the initial prompt input was and then the results to the best of my knowledge these are all accurate and good answers from Falcon 40b here as you can see I use this sort of format in my prompt to suggest a sort of conversation between a user and an assistant these names do not need to be this way or the same or even used at all this model is extremely open and general purpose you don't even need there to be like one to one like user then assistant then user then assistant you can have something like user assistant assistant like I'll show example of that later too but there's the possibilities here are very very open so here though I do particularly like the question about practicing Law Without a law school degree in the United States because most models get this wrong and and the question and answer itself can also speak towards potential uh moderation and after the fact sort of censorship or just a bias towards safe answers if there are concerns about models saying incorrect or unsafe things in general highly professional Fields like law and Medicine carry a lot of risk if people who are uneducated attempt to practice it so a model is likely to be biased away from answering this question correctly even though that is the truth that you can practice law in certain states without a law degree if attempts are made to encourage or bias that model towards safe answers so for example chat gbt with GPT 3.5 gets this wrong and says no you cannot practice law in any state without a law degree GPT 4.0 does actually get this right and I can't remember if it's always gotten this question right I want to say if memory serves me it didn't used to get this question right but anyway here it does but now without multiple warnings and what I would call cyas so it does seem that at the very least the potential risks in answering this question were identified by GPT 4.0 and then it's very careful in its response to you now Beyond this question all the answers indicate a wide range of accurate knowledge that you can tap into from the safety of drinking dehumidifier water to the iPhone's release date to the atomic mass of thallium and so on obviously this is a terribly small sample size and I'm confident that we could find wrong answers generated by this model but as a general purpose model this is really surprisingly good for a mere 40 billion parameters at least in my opinion next up is the topic of math an area that GPT models tend to struggle significantly with due to the auto aggressive nature of how they actually generate responses going always linearly algebraic expressions are often calculated in like chunks and not necessarily in a linear order of the characters that are seen right so large language models often struggle here for simple math problems Falcon 40b gets the correct answer but as you complicate things with algebraic problems you can often find GPD models including even gpt4 and GPT 3.5 they begin to struggle chat GPT especially for gpt4 uses something in the background that will essentially convert your math prompts to show your work prompts so where the the machine was a at least asked I just want this answer it is clearly detecting that it's a math problem and then being fed and an additional I think prompt that is suggesting that hey please show your work because this is a common trick to getting GPD models to correctly solve problems like this that are maybe not necessarily solved by thinking linearly like if you need to kind of be able to bounce around the way to do that is essentially asking it to show its work and the theory here is just the more tokens that you give the model to kind of like think through a problem that's like token is like brain power or something and then it also allows it to think non-linearly now where as if you tell it just straight up give me that answer and nothing more it's probably going to get it wrong I also exemplified this example in my analyzing gpt4 video I'll put a link to that video in the description but here's a couple friends from that where both GPD 3.5 and gpt4 get this right nowadays due to some GPT post-processing or you know just tricks that are being applied and actually GPT 3.5 got these questions used to get these questions wrong and I actually think using rbrms and other kind of heuristics and techniques and stuff that that openai learned from gpt4 I think they just went back and applied them to GPD 3.5 I'm just taking guesses here but now gbd 3.5 just responds in a very similar way to gpd4 such that I'm pretty confident they're probably running the exact same kind of forms and like like pre-pre-prompts I guess or maybe a post prompt but I don't know how I don't know what the right word is so I'll just call it heuristics but essentially a trick to get the model to Think Through the answers but you can still show that they will both of these models will fail if they try to just generate just the answer and nothing more but coming back to Falcon we can see that if we just ask Falcon without telling it to show its work it does get the question wrong but if we tell Falcon hey please show your work then it shows its work and it actually gets the question correct another area that some GPT models are surprisingly impressive is this concept of theory of Mind essentially understanding underlying thoughts and especially like human emotions and behavior for situations and scenarios so here's an example from the Sparks of AGI paper from Microsoft that I've run through Falcon 40b essentially the white is the prompt and the green is the generated answers I went one answer at a time each time passing the entire history up to that point in that new question so kind of think of this as a continued conversation between me and Falcon 40b asking about this sort of conversation scenario Falcon 40b here correctly identifies that Mark isn't necessarily unhappy with Judy's disciplining of Jack but instead how she went about it but also correctly identifies how Judy is perceiving things herself and feeling about Jack's sort of stepping in and Falcon understands that they're both essentially talking past each other and even has suggestions about how they could improve this situation these theory of Mind examples just always impress me as GPD models are strangely really good at this sort of thing often performing much better than you might have predicted if you weren't aware that these models are just good at this stuff so here's another example of theory of mind and an answer that I think is quite good again there's nothing in this text that suggests what Luke's reasoning might be this is purely an understanding of just like human sort of psychology and emotion to attempt to explain some incongruence between requests statements and behavior so again you know generally we you would expect AI to think in this like deterministic way and not really take into account human emotion and like you know odd behavior like it's really just emotion like emotions are strange in humans it's like as opposed to like something like programming where everything is deterministic and it just it's logic right um there's this whole other side to like humans that is sometimes very difficult to understand whereas uh the GPT models and Falcon 40 be here in particular um show a pretty good understanding of human emotion and behavior next we have some programming examples which is usually my personal interest and focus with GPT models five percent of the.