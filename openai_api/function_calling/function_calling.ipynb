{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=0lOSvOoF2to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API Function Calling for GPT-4 and GPT-3.5\n",
    "\n",
    "Video walkthrough of this notebook: https://www.youtube.com/watch?v=0lOSvOoF2to\n",
    "\n",
    "OpenAI has released a new capability for their models through the API, called \"Function Calling.\" The intent is to help make it far easier to extract structured, deterministic, information from an otherwise unstructured and non-deterministic language model like GPT-4. \n",
    "\n",
    "This task of structuring and getting deterministic outputs from a language model has so far been a very difficult task, and has been the subject of much research. Usually the approach is to keep trying various pre-prompts and few shot learning examples until you find one that at least works. While doable, the end result was clunky and not very reliable. Now though, you can use the function calling capability to build out quite powerful programs. Essentially, adding intelligence to your programs.\n",
    "\n",
    "Imagine you want to be able to intelligently handle for a bunch of different types of input, but also something like: \"What's the weather like in Boston? "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task now, given this natural language input to GPT-4 would be:\n",
    "\n",
    "1. To identify if the user is seeking weather information\n",
    "2. If they are, extract the location from their input\n",
    "\n",
    "So if the user said \"Hello, how are you today?\", we wouldn't need to run the function or try to extract a location. \n",
    "\n",
    "But, if the user said something like: \"What's the weather like in Boston?\" then we want to identify the desire to get the weather and extract the location \"Boston\" from the input.\n",
    "\n",
    "Previously, you might pass this input to the OpenAI API for GPT 4 like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then you'd access the response via:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As an AI, I cannot provide real-time information. Please check a reliable weather source like a weather website or app for the current conditions in Boston.\n"
     ]
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message.content\n",
    "print(reply_content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this isn't quite what we would want to happen in this scenario! While GPT-4 may not currently be able to access the internet for us, we could conceivably do this ourselves, but we still would need to identify the intent, as well as the particular desired location. Imagine we have a function like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location, unit=\"fahrenheit\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"72\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a placeholder type of function to show how this all ties together, but it could be anything here. Extracting the intent and location could be done with a preprompt, and this is sort of how OpenAI is doing it through the API, but the model has been trained for us with a particular structure, so we can use this to save a lot of R&D time to find the \"best prompt\" to get it done. \n",
    "\n",
    "To do this, we want to make sure that we're using version `gpt-4-0613` or later. Then, we can pass a new `functions` parameter to the `ChatCompletion` call like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"What's the weather like in Boston?\"}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                },\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "        },\n",
    "    }\n",
    "],\n",
    "function_call=\"auto\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to note is the `function_call=\"auto\",` part. This will let GPT-4 determine if it should seek to fulfill the function parameters. You can also set it to `none` to force no function to be detected, and finally you can set it to seek parameters for a specific function by doing something like `function_call={\"name\": \"get_current_weather\"}`. There are many instances where it could be hard for GPT-4 to determine if a function should be run, so being able to force it to run if you know it should be is actually very powerful, which I'll show soon. \n",
    "\n",
    "Beyond this, we name and describe the function, then describe the parameters that we'd hope to pass to this function. GPT-4 is relying on this description to help identify what it is you want, so try to be as clear as possible here. The API is going to return to you a json structured object, and this is how you structure your function description, which affords you quite a bit of flexibility in how you describe/structure this functionality. \n",
    "\n",
    "Okay let's see how GPT-4 responds to our new prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x1327c0180> JSON: {\n",
       "  \"finish_reason\": \"function_call\",\n",
       "  \"index\": 0,\n",
       "  \"message\": {\n",
       "    \"content\": null,\n",
       "    \"function_call\": {\n",
       "      \"arguments\": \"{\\n  \\\"location\\\": \\\"Boston, MA\\\"\\n}\",\n",
       "      \"name\": \"get_current_weather\"\n",
       "    },\n",
       "    \"role\": \"assistant\"\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_content = completion.choices[0]\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we don't actually have any `message` `content`. We instead have an identified `function_call` for the function named `get_current_weather` and we have the `parameters` that were extracted from the input, in this case the location, which is accurately detected as `Boston` by GPT-4.\n",
    "\n",
    "We can convert this OpenAI object to a more familiar Python dict by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current weather in Boston is sunny and windy with a temperature of 72 degrees.\n"
     ]
    }
   ],
   "source": [
    "# Step 2, check if the model wants to call a function\n",
    "message = completion.choices[0].message\n",
    "if message.get(\"function_call\"):\n",
    "    function_name = message[\"function_call\"][\"name\"]\n",
    "\n",
    "    # Step 3, call the function\n",
    "    # Note: the JSON response from the model may not be valid JSON\n",
    "    function_response = get_current_weather(\n",
    "        location=message.get(\"location\"),\n",
    "        unit=message.get(\"unit\"),\n",
    "    )\n",
    "\n",
    "    # Step 4, send model the info on the function call and function response\n",
    "    second_response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-0613\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is the weather like in boston?\"},\n",
    "            message,\n",
    "            {\n",
    "                \"role\": \"function\",\n",
    "                \"name\": function_name,\n",
    "                \"content\": function_response,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "    print(second_response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_current_weather\n"
     ]
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message\n",
    "\n",
    "funcs = reply_content.to_dict()['function_call']['name']\n",
    "print(funcs)\n",
    "# funcs = json.loads(funcs)\n",
    "# print(funcs)\n",
    "# print(funcs['location'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not only can we extract information or intent from a user's input, we can also extract structured data from GPT-4 in a response. \n",
    "\n",
    "For example here, I've been working on a project, called TermGPT, to create terminal commands to satisfy a user's query for doing engineering/programming.\n",
    "\n",
    "Imagine in this scenario, you have user input like: `\"How do I install Tensorflow for my GPU?\"`\n",
    "\n",
    "In this case, we'd get a useful natural language response from GPT-4, but it wouldn't be structured as JUST terminal commands that could be run. We have an intent that could be extracted from here, but the commands themselves need to be determined by GPT-4. \n",
    "\n",
    "With the function calling capability, we can do this by passing a function description like:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "functions=[\n",
    "{\n",
    "    \"name\": \"get_commands\",\n",
    "    \"description\": \"Get a list of bash commands on an Ubuntu machine to run\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"commands\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A terminal command string\"\n",
    "                },\n",
    "                \"description\": \"List of terminal command strings to be executed\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"commands\"]\n",
    "    }\n",
    "}\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my first attempt at a description and structure, it's likely there are even better ones, for reasons I'll explain shortly. In this case, the name for the function will be \"get_commands\" and then we describe it as `Get a list of bash commands on an Ubuntu machine to run`. Then, we specify the parameter as an \"`array`\" (`list` in python), and this array contains items, where each \"item\" is a terminal command string, and the description of this list is `List of terminal command strings to be executed`.\n",
    "\n",
    "Now, let's see how GPT-4 responds to this prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_user_input = \"How do I install Tensorflow for my GPU?\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": example_user_input}],\n",
    "        functions=[\n",
    "        {\n",
    "            \"name\": \"get_commands\",\n",
    "            \"description\": \"Get a list of bash commands on an Ubuntu machine\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"commands\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"A terminal command string\"\n",
    "                        },\n",
    "                        \"description\": \"List of terminal command strings to be executed\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"commands\"]\n",
    "            }\n",
    "        }\n",
    "        ],\n",
    "        function_call=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x13277bd60> JSON: {\n",
       "  \"content\": \"Here's a general guide on how you can install TensorFlow for your GPU in an Ubuntu environment.\\n\\nPlease, ensure that you have already installed the GPU driver for your system and have CUDA compatible with TensorFlow version, then follow the steps below:\\n\\n1. Create a new environment (Recommended)\\n    - By using Conda: `conda create -n tf_gpu`\\n    - By using virtualenv: `virtualenv --system-site-packages -p python3 ./venv`\\n\\n2. Activate the environment.\\n    - If you used Conda: `conda activate tf_gpu`\\n    - If you used virtualenv: `source ./venv/bin/activate`\\n\\n3. Now, install TensorFlow by using pip:\\n    - By using pip: `pip install tensorflow-gpu`\\n\\nHere are these commands in a structured format:\\n```python\\n{\\n  \\\"commands\\\": [\\n    \\\"conda create -n tf_gpu\\\",\\n    \\\"conda activate tf_gpu\\\",\\n    \\\"pip install tensorflow-gpu\\\"\\n  ]\\n}\\n```\\n\\nNote: If you find that you do not have Python or pip installed, you can install them using the following commands:\\n```python\\n{\\n  \\\"commands\\\": [\\n    \\\"sudo apt-get update\\\",\\n    \\\"sudo apt-get install python3.6\\\",\\n    \\\"sudo apt-get install python3-pip\\\"\\n  ]\\n}\\n```\\nAlways remember to double-check TensorFlow's official GPU guide and your GPU compute capacity to ensure compatibility. \\n\\nAlso, remember that, when you install TensorFlow for GPU, it will need to be in a system that CUDA and cuDNN are installed in, you will need to get those installed if they are not already, their installation is a process on it's own.\",\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply_content = completion.choices[0].message\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, I set `function_call` to be \"auto.\" In this case, it's fairly hard to GPT-4 to determine that the intent was to run this function. I suspect this is caused by the difference between extracting info from a user's input vs structuring a response. That said, I am quite sure that we could adjust the names/descriptions for our function call to be far more successful here. \n",
    "\n",
    "But, even when this auto version fails, we do have the ability to \"nudge\" GPT-4 to do it anyway by setting `function_call` to be `{\"name\": \"your_function\"}`. This will force GPT-4 to run the function, even if it doesn't think it should or doesn't realize it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x132755a90> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n  \\\"commands\\\": [\\n    \\\"sudo apt update\\\",\\n    \\\"sudo apt install python3-dev python3-pip\\\",\\n    \\\"sudo pip3 install -U virtualenv\\\",\\n    \\\"virtualenv --system-site-packages -p python3 ./venv\\\",\\n    \\\"source ./venv/bin/activate\\\",\\n    \\\"pip install --upgrade pip\\\",\\n    \\\"pip install tensorflow-gpu\\\"\\n  ]\\n}\",\n",
       "    \"name\": \"get_commands\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_user_input = \"How do I install Tensorflow for my GPU?\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": example_user_input}],\n",
    "        functions=[\n",
    "        {\n",
    "            \"name\": \"get_commands\",\n",
    "            \"description\": \"Get a list of bash commands on an Ubuntu machine\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"commands\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"A terminal command string\"\n",
    "                        },\n",
    "                        \"description\": \"List of terminal command strings to be executed\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"commands\"]\n",
    "            }\n",
    "        }\n",
    "        ],\n",
    "        function_call={\"name\": \"get_commands\"},\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message\n",
    "reply_content"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do get the function call, and we can grab those commands with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sudo apt update',\n",
       " 'sudo apt install python3-dev python3-pip',\n",
       " 'sudo pip3 install -U virtualenv',\n",
       " 'virtualenv --system-site-packages -p python3 ./venv',\n",
       " 'source ./venv/bin/activate',\n",
       " 'pip install --upgrade pip',\n",
       " 'pip install tensorflow-gpu']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcs = reply_content.to_dict()['function_call']['arguments']\n",
    "funcs = json.loads(funcs)\n",
    "funcs['commands']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just can't express how powerful this is. We can now extract structured data from GPT-4, and we can also pass structured data to GPT-4 to have it generate a response. Being able to do this reliably is basically never before seen, and this will make this sort of interaction between deterministic programming logic and non-deterministic language models far more common and just plain possible. The ability to do this is going to be a game changer for the field of AI and programming, and I'm very excited to see what people do with this capability.\n",
    "\n",
    "Here's another example of how powerful this could be. We could generate responses for a given query in a variety of \"personalities\" so to speak:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject at 0x132776b30> JSON: {\n",
       "  \"content\": null,\n",
       "  \"function_call\": {\n",
       "    \"arguments\": \"{\\n  \\\"sassy_and_sarcastic\\\": \\\"Oh, absolutely. If you're a fan of bacteria, dust and possibly some nasty metal particles, then by all means, enjoy that dehumidifier water!\\\",\\n  \\\"happy_and_helpful\\\": \\\"Actually, it's not recommended to drink water from a dehumidifier. It could contain harmful bacteria and dust. Stick with tap water or bottled water to stay safe and healthy.\\\"\\n}\",\n",
       "    \"name\": \"get_varied_personality_responses\"\n",
       "  },\n",
       "  \"role\": \"assistant\"\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_user_input = \"Is it safe to drink water from a dehumidifer?\"\n",
    "\n",
    "completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4-0613\",\n",
    "    messages=[{\"role\": \"user\", \"content\": example_user_input}],\n",
    "    functions=[\n",
    "    {\n",
    "        \"name\": \"get_varied_personality_responses\",\n",
    "        \"description\": \"ingest the various personality responses\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sassy_and_sarcastic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A sassy and sarcastic version of the response to a user's query\",\n",
    "                },\n",
    "                \"happy_and_helpful\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"A happy and helpful version of the response to a user's query\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sassy_and_sarcastic\", \"happy_and_helpful\"],\n",
    "        },\n",
    "    }\n",
    "        ],\n",
    "        function_call={\"name\": \"get_varied_personality_responses\"},\n",
    ")\n",
    "\n",
    "reply_content = completion.choices[0].message\n",
    "reply_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_options = reply_content.to_dict()['function_call']['arguments']\n",
    "options = json.loads(response_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh, absolutely. If you're a fan of bacteria, dust and possibly some nasty metal particles, then by all means, enjoy that dehumidifier water!\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options[\"sassy_and_sarcastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Actually, it's not recommended to drink water from a dehumidifier. It could contain harmful bacteria and dust. Stick with tap water or bottled water to stay safe and healthy.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "options[\"happy_and_helpful\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully that gives you some ideas about what's possible here, but this is truly 0.000001% of what's actually possible here. There's going to be some incredible things built with this capability. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
