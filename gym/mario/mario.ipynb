{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym-super-mario-bros==7.4.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.23.5)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.0.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (7.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym-super-mario-bros==7.4.0\n",
    "# !pip install tensordict==0.3.0\n",
    "# !pip install torchrl==0.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (3.7.5)\n",
      "Requirement already satisfied: tensorflow-macos in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (2.12.0)\n",
      "Requirement already satisfied: tensorflow-metal in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (1.0.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.20 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (3.10.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (0.4.13)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (69.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (4.9.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (1.60.1)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-macos) (2.12.0)\n",
      "Requirement already satisfied: wheel~=0.35 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorflow-metal) (0.42.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: ml-dtypes>=0.1.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow-macos) (0.2.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow-macos) (1.10.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.6 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from jax>=0.3.15->tensorflow-macos) (7.0.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.28.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (3.5.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (2.31.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensorboard<2.13,>=2.12->tensorflow-macos) (3.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (5.3.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow-macos) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow-macos) (2.1.5)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow-macos) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow-macos) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "# !pip install torchvision\n",
    "!pip install matplotlib tensorflow-macos tensorflow-metal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torch import nn\n",
    "# from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, os\n",
    "\n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "# NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros\n",
    "\n",
    "# from tensordict import TensorDict\n",
    "# from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import layers, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0.0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
    "# if gym.__version__ < '0.26':\n",
    "#     env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
    "# else:\n",
    "env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right\n",
    "#   1. jump right\n",
    "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, trunc, info = env.step(action=0)\n",
    "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, trunk, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, trunk, info\n",
    "\n",
    "\n",
    "# class GrayScaleObservation(gym.ObservationWrapper):\n",
    "#     def __init__(self, env):\n",
    "#         super().__init__(env)\n",
    "#         obs_shape = self.observation_space.shape[:2]\n",
    "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "#     def permute_orientation(self, observation):\n",
    "#         # permute [H, W, C] array to [C, H, W] tensor\n",
    "#         observation = np.transpose(observation, (2, 0, 1))\n",
    "#         observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "#         return observation\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         observation = self.permute_orientation(observation)\n",
    "#         transform = T.Grayscale()\n",
    "#         observation = transform(observation)\n",
    "#         return observation\n",
    "\n",
    "\n",
    "# class ResizeObservation(gym.ObservationWrapper):\n",
    "#     def __init__(self, env, shape):\n",
    "#         super().__init__(env)\n",
    "#         if isinstance(shape, int):\n",
    "#             self.shape = (shape, shape)\n",
    "#         else:\n",
    "#             self.shape = tuple(shape)\n",
    "\n",
    "#         obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "#         self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "#     def observation(self, observation):\n",
    "#         transforms = T.Compose(\n",
    "#             [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
    "#         )\n",
    "#         observation = transforms(observation).squeeze(0)\n",
    "#         return observation\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]  # Assuming grayscale reduces the channel to 1\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def permute_orientation(self, observation):\n",
    "        # TensorFlow processes images in [H, W, C] format, so no need to permute\n",
    "        return observation\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.permute_orientation(observation)\n",
    "        observation = tf.image.rgb_to_grayscale(observation)\n",
    "        observation = tf.cast(observation, tf.uint8)  # Ensure the data type matches the observation space\n",
    "        return observation.numpy().squeeze(-1)  # Squeeze to remove the last dimension\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        # Adjust observation space to match the new shape, keeping the channel dimension as is\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Ensure observation has at least 3 dimensions [height, width, channels]\n",
    "        if len(observation.shape) == 2:  # Grayscale image, missing channels dimension\n",
    "            observation = np.expand_dims(observation, axis=-1)  # Add channels dimension\n",
    "\n",
    "        observation = np.expand_dims(observation, axis=0)  # Add batch dimension for single observation\n",
    "\n",
    "        observation = tf.convert_to_tensor(observation, dtype=tf.float32)\n",
    "        observation = tf.image.resize(observation, self.shape, method='area')\n",
    "        observation = tf.cast(observation, tf.uint8)  # Ensure the data type matches the observation space\n",
    "\n",
    "        # Remove batch dimension before returning\n",
    "        observation = tf.squeeze(observation, axis=0)\n",
    "        return observation.numpy()\n",
    "\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "from packaging import version\n",
    "\n",
    "if version.parse(gym.__version__) < version.parse('0.26'):\n",
    "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
    "else:\n",
    "    env = FrameStack(env, num_stack=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MarioNet(nn.Module):\n",
    "#     \"\"\"mini CNN structure\n",
    "#   input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "#   \"\"\"\n",
    "\n",
    "#     def __init__(self, input_dim, output_dim):\n",
    "#         super().__init__()\n",
    "#         c, h, w = input_dim\n",
    "\n",
    "#         if h != 84:\n",
    "#             raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "#         if w != 84:\n",
    "#             raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "#         self.online = self.__build_cnn(c, output_dim)\n",
    "\n",
    "#         self.target = self.__build_cnn(c, output_dim)\n",
    "#         self.target.load_state_dict(self.online.state_dict())\n",
    "\n",
    "#         # Q_target parameters are frozen.\n",
    "#         for p in self.target.parameters():\n",
    "#             p.requires_grad = False\n",
    "\n",
    "#     def forward(self, input, model):\n",
    "#         if model == \"online\":\n",
    "#             return self.online(input)\n",
    "#         elif model == \"target\":\n",
    "#             return self.target(input)\n",
    "\n",
    "#     def __build_cnn(self, c, output_dim):\n",
    "#         return nn.Sequential(\n",
    "#             nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Flatten(),\n",
    "#             nn.Linear(3136, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, output_dim),\n",
    "#         )\n",
    "\n",
    "class MarioNet(tf.keras.Model):\n",
    "    \"\"\"mini CNN structure\n",
    "    input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(MarioNet, self).__init__()\n",
    "        c, h, w = input_dim\n",
    "\n",
    "        if h != 84:\n",
    "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
    "        if w != 84:\n",
    "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
    "\n",
    "        self.online = self.__build_cnn(c, output_dim)\n",
    "        self.target = self.__build_cnn(c, output_dim)\n",
    "        self.target.set_weights(self.online.get_weights())\n",
    "\n",
    "        # Freeze Q_target parameters.\n",
    "        self.target.trainable = False\n",
    "\n",
    "    def call(self, inputs, training=None, model=\"online\"):\n",
    "        if model == \"online\":\n",
    "            return self.online(inputs)\n",
    "        elif model == \"target\":\n",
    "            return self.target(inputs)\n",
    "\n",
    "    def __build_cnn(self, c, output_dim):\n",
    "        model = models.Sequential([\n",
    "            layers.Conv2D(filters=32, kernel_size=8, strides=4, activation='relu', input_shape=(c, 84, 84, 1)),\n",
    "            layers.Conv2D(filters=64, kernel_size=4, strides=2, activation='relu'),\n",
    "            layers.Conv2D(filters=64, kernel_size=3, strides=1, activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(512, activation='relu'),\n",
    "            layers.Dense(output_dim)\n",
    "        ])\n",
    "        return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        # self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "        # self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "        # self.net = self.net.to(device=self.device)\n",
    "        self.net = MarioNet(self.state_dim, self.action_dim)\n",
    "        self.net.compile(optimizer=tf.keras.optimizers.legacy.Adam(learning_rate=0.00025), loss='mean_squared_error')\n",
    "\n",
    "        self.exploration_rate = 0.8\n",
    "        self.exploration_rate_decay = 0.99999975\n",
    "        self.exploration_rate_min = 0.1\n",
    "        self.curr_step = 0\n",
    "\n",
    "        self.save_every = 50_000  # no. of experiences between saving Mario Net\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "        # EXPLORE\n",
    "        if np.random.rand() < self.exploration_rate:\n",
    "            action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "        # EXPLOIT\n",
    "        # else:\n",
    "        #     state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
    "        #     state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
    "        #     action_values = self.net(state, model=\"online\")\n",
    "        #     action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "        # EXPLOIT\n",
    "        else:\n",
    "            state = np.array(state[0]) if isinstance(state, tuple) else np.array(state)\n",
    "            state = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            state = tf.expand_dims(state, axis=0)  # Equivalent to unsqueeze in PyTorch\n",
    "            action_values = self.net(state, training=False)  # Predict with the online model\n",
    "            action_idx = tf.argmax(action_values, axis=1).numpy().item()\n",
    "        # EXPLOIT\n",
    "        # else:\n",
    "        #     # Assuming 'state' is a LazyFrames object or similar, first convert to numpy array\n",
    "        #     state = np.array(state[0]) if isinstance(state, tuple) else np.array(state)\n",
    "            \n",
    "        #     # Convert state to float32 and normalize\n",
    "        #     state = state.astype(np.float32) / 255.0\n",
    "            \n",
    "        #     # Ensure state has the shape (84, 84, 4) which is required by the model\n",
    "        #     # If the last dimension is 1 (grayscale images), we assume the state needs to be reshaped\n",
    "        #     if state.shape[-1] == 1:\n",
    "        #         # Remove the last dimension since it's unnecessary\n",
    "        #         state = np.squeeze(state, axis=-1)\n",
    "            \n",
    "        #     # Reshape the state to have the shape (1, 84, 84, 4) for the model\n",
    "        #     # This includes adding a batch dimension and ensuring the channel dimension is correct\n",
    "        #     state = np.transpose(state, (1, 2, 0))  # Transpose to (84, 84, 4)\n",
    "        #     state = np.expand_dims(state, axis=0)  # Add batch dimension to make it (1, 84, 84, 4)\n",
    "            \n",
    "        #     # Convert to a TensorFlow tensor\n",
    "        #     state_tensor = tf.convert_to_tensor(state, dtype=tf.float32)\n",
    "            \n",
    "        #     # Predict with the online model\n",
    "        #     action_values = self.net(state_tensor, training=False)\n",
    "            \n",
    "        #     # Determine the action\n",
    "        #     action_idx = tf.argmax(action_values, axis=1).numpy().item()\n",
    "\n",
    "\n",
    "\n",
    "        # decrease exploration_rate\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "        # increment step\n",
    "        self.curr_step += 1\n",
    "        return action_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mario(Mario):  # subclassing for continuity\n",
    "#     def __init__(self, state_dim, action_dim, save_dir):\n",
    "#         super().__init__(state_dim, action_dim, save_dir)\n",
    "#         self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
    "#         self.batch_size = 32\n",
    "\n",
    "#     def cache(self, state, next_state, action, reward, done):\n",
    "#         \"\"\"\n",
    "#         Store the experience to self.memory (replay buffer)\n",
    "\n",
    "#         Inputs:\n",
    "#         state (``LazyFrame``),\n",
    "#         next_state (``LazyFrame``),\n",
    "#         action (``int``),\n",
    "#         reward (``float``),\n",
    "#         done(``bool``))\n",
    "#         \"\"\"\n",
    "#         def first_if_tuple(x):\n",
    "#             return x[0] if isinstance(x, tuple) else x\n",
    "#         state = first_if_tuple(state).__array__()\n",
    "#         next_state = first_if_tuple(next_state).__array__()\n",
    "\n",
    "#         state = torch.tensor(state)\n",
    "#         next_state = torch.tensor(next_state)\n",
    "#         action = torch.tensor([action])\n",
    "#         reward = torch.tensor([reward])\n",
    "#         done = torch.tensor([done])\n",
    "\n",
    "#         # self.memory.append((state, next_state, action, reward, done,))\n",
    "#         self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
    "\n",
    "#     def recall(self):\n",
    "#         \"\"\"\n",
    "#         Retrieve a batch of experiences from memory\n",
    "#         \"\"\"\n",
    "#         batch = self.memory.sample(self.batch_size).to(self.device)\n",
    "#         state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
    "#         return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "class Mario(Mario):  # Assuming 'Mario' is properly defined elsewhere for TensorFlow\n",
    "    def __init__(self, state_dim, action_dim, save_dir, buffer_size=100000):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.memory = deque(maxlen=buffer_size)  # Replay buffer using deque\n",
    "        self.batch_size = 32\n",
    "\n",
    "    def cache(self, state, next_state, action, reward, done):\n",
    "        \"\"\"\n",
    "        Store the experience to self.memory (replay buffer)\n",
    "\n",
    "        Inputs:\n",
    "        state, next_state (could be `LazyFrame` or any supported format),\n",
    "        action (int),\n",
    "        reward (float),\n",
    "        done (bool)\n",
    "        \"\"\"\n",
    "        def first_if_tuple(x):\n",
    "            return x[0] if isinstance(x, tuple) else x\n",
    "        \n",
    "        state = np.array(first_if_tuple(state))\n",
    "        next_state = np.array(first_if_tuple(next_state))\n",
    "\n",
    "        # No need to convert to TensorFlow tensors here, as we'll do it in batch during recall\n",
    "        self.memory.append((state, next_state, action, reward, done))\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"\n",
    "        Retrieve a batch of experiences from memory\n",
    "        \"\"\"\n",
    "        batch = np.random.choice(len(self.memory), size=self.batch_size, replace=False)\n",
    "        states, next_states, actions, rewards, dones = zip(*[self.memory[idx] for idx in batch])\n",
    "\n",
    "        # Convert batched data to TensorFlow tensors\n",
    "        states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "        next_states = tf.convert_to_tensor(next_states, dtype=tf.float32)\n",
    "        actions = tf.convert_to_tensor(actions, dtype=tf.int32)\n",
    "        rewards = tf.convert_to_tensor(rewards, dtype=tf.float32)\n",
    "        dones = tf.convert_to_tensor(dones, dtype=tf.float32)\n",
    "\n",
    "        return states, next_states, actions, rewards, dones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mario(Mario):\n",
    "#     def __init__(self, state_dim, action_dim, save_dir):\n",
    "#         super().__init__(state_dim, action_dim, save_dir)\n",
    "#         self.gamma = 0.9\n",
    "\n",
    "#     def td_estimate(self, state, action):\n",
    "#         current_Q = self.net(state, model=\"online\")[\n",
    "#             np.arange(0, self.batch_size), action\n",
    "#         ]  # Q_online(s,a)\n",
    "#         return current_Q\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def td_target(self, reward, next_state, done):\n",
    "#         next_state_Q = self.net(next_state, model=\"online\")\n",
    "#         best_action = torch.argmax(next_state_Q, axis=1)\n",
    "#         next_Q = self.net(next_state, model=\"target\")[\n",
    "#             np.arange(0, self.batch_size), best_action\n",
    "#         ]\n",
    "#         return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
    "import tensorflow as tf\n",
    "\n",
    "class Mario(Mario):  # Assuming 'Mario' is the base class properly defined for TensorFlow\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.gamma = 0.9\n",
    "\n",
    "    def td_estimate(self, state, action):\n",
    "        current_Q = self.net(state, training=False)  # Get Q values for all actions\n",
    "        batch_indices = tf.range(0, self.batch_size, dtype=tf.int32)  # Ensure dtype matches action's dtype\n",
    "        action_indices = tf.stack([batch_indices, action], axis=1)\n",
    "        current_Q = tf.gather_nd(current_Q, action_indices)  # Extract Q value for the taken action\n",
    "        return current_Q\n",
    "\n",
    "    def td_target(self, reward, next_state, done):\n",
    "        next_state_Q_online = self.net(next_state, training=False)  # Predicted Q values from the online network\n",
    "        best_action = tf.argmax(next_state_Q_online, axis=1, output_type=tf.int32)  # Explicitly set output type\n",
    "        \n",
    "        next_state_Q_target = self.net(next_state, training=False, model=\"target\")  # Predicted Q values from the target network\n",
    "        batch_indices = tf.range(0, self.batch_size, dtype=tf.int32)  # Ensure dtype matches best_action's dtype\n",
    "        best_action_indices = tf.stack([batch_indices, best_action], axis=1)\n",
    "        next_Q = tf.gather_nd(next_state_Q_target, best_action_indices)\n",
    "        \n",
    "        not_done = 1 - tf.cast(done, tf.float32)\n",
    "        td_target = reward + not_done * self.gamma * next_Q\n",
    "        return td_target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mario(Mario):\n",
    "#     def __init__(self, state_dim, action_dim, save_dir):\n",
    "#         super().__init__(state_dim, action_dim, save_dir)\n",
    "#         self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "#         self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "#     def update_Q_online(self, td_estimate, td_target):\n",
    "#         loss = self.loss_fn(td_estimate, td_target)\n",
    "#         self.optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         self.optimizer.step()\n",
    "#         return loss.item()\n",
    "\n",
    "#     def sync_Q_target(self):\n",
    "#         self.net.target.load_state_dict(self.net.online.state_dict())\n",
    "import tensorflow as tf\n",
    "\n",
    "class Mario(Mario):  # Assuming 'Mario' is the base class properly defined for TensorFlow\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        # Initialize the optimizer\n",
    "        self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=0.00025)\n",
    "        # Initialize the loss function\n",
    "        self.loss_fn = tf.keras.losses.Huber()\n",
    "\n",
    "    def update_Q_online(self, state_batch, action_batch, td_target):\n",
    "        with tf.GradientTape() as tape:\n",
    "            state_action_values = tf.reduce_sum(\n",
    "                self.net(state_batch, training=True) * tf.one_hot(action_batch, self.action_dim), axis=1)\n",
    "            loss = self.loss_fn(td_target, state_action_values)\n",
    "        gradients = tape.gradient(loss, self.net.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.net.trainable_variables))\n",
    "        return loss.numpy()\n",
    "\n",
    "    def sync_Q_target(self):\n",
    "        self.net.target.set_weights(self.net.online.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mario(Mario):\n",
    "#     def save(self):\n",
    "#         save_path = (self.save_dir / f\"mario_net_{self.curr_step}.chkpt\")\n",
    "#         torch.save(dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate), save_path)\n",
    "#         print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "    \n",
    "#     def load(self, file_name=None):\n",
    "#         checkpoint = torch.load(file_name)\n",
    "#         self.net.load_state_dict(checkpoint['model'])\n",
    "#         # self.exploration_rate = checkpoint['exploration_rate']\n",
    "#         print(f\"Loaded MarioNet from {file_name}\")\n",
    "import tensorflow as tf\n",
    "import json\n",
    "\n",
    "class Mario(Mario):  # Assuming 'Mario' is the base class properly defined for TensorFlow\n",
    "    def save(self):\n",
    "        save_path = str(self.save_dir / f\"mario_net_{self.curr_step}\")\n",
    "        self.net.save(save_path + \".tf\")  # Save the TensorFlow model\n",
    "        # Save exploration rate in a separate JSON file for simplicity\n",
    "        with open(save_path + \"_exploration_rate.json\", 'w') as f:\n",
    "            json.dump({\"exploration_rate\": self.exploration_rate}, f)\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "    \n",
    "    def load(self, file_name=None):\n",
    "        # Load the TensorFlow model\n",
    "        self.net = tf.keras.models.load_model(file_name + \".tf\")\n",
    "        # Load exploration rate from a separate JSON file\n",
    "        with open(file_name + \"_exploration_rate.json\", 'r') as f:\n",
    "            exploration_rate_data = json.load(f)\n",
    "            self.exploration_rate = exploration_rate_data['exploration_rate']\n",
    "        print(f\"Loaded MarioNet from {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 1e4  # min. experiences before training\n",
    "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 10_000  # no. of experiences between Q_target & Q_online sync\n",
    "\n",
    "    def learn(self):\n",
    "        if self.curr_step % self.sync_every == 0:\n",
    "            self.sync_Q_target()\n",
    "\n",
    "        if self.curr_step % self.save_every == 0:\n",
    "            self.save()\n",
    "\n",
    "        if self.curr_step < self.burnin:\n",
    "            return None, None\n",
    "\n",
    "        if self.curr_step % self.learn_every != 0:\n",
    "            return None, None\n",
    "\n",
    "        state, next_state, action, reward, done = self.recall()\n",
    "\n",
    "        td_target = self.td_target(reward, next_state, done)\n",
    "        loss = self.update_Q_online(state, action, td_target)\n",
    "\n",
    "        return td_target.numpy().mean(), loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class MetricLogger:\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {round(epsilon,4)} - \"\n",
    "            f\"Mean Reward {round(mean_ep_reward,0)} - \"\n",
    "            f\"Mean Length {round(mean_ep_length, 0)} - \"\n",
    "            f\"Mean Loss {round(mean_ep_loss, 0)} - \"\n",
    "            f\"Mean Q Value {round(mean_ep_q, 0)} - \"\n",
    "            f\"Time Delta {round(time_since_last_record, 2)} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
    "            plt.clf()\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
    "            plt.legend()\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actions:  2\n",
      "Episode 0 - Step 2005 - Epsilon 0.4979 - Mean Reward 267.0 - Mean Length 2005.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 15.51 - Time 2024-02-21T16:43:51\n",
      "Episode 10 - Step 7608 - Epsilon 0.4922 - Mean Reward 409.0 - Mean Length 692.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 43.34 - Time 2024-02-21T16:44:34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 6). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/2024-02-21T16-43-35/mario_net_10000.tf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: checkpoints/2024-02-21T16-43-35/mario_net_10000.tf/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MarioNet saved to checkpoints/2024-02-21T16-43-35/mario_net_10000 at step 10000\n",
      "Episode 20 - Step 10781 - Epsilon 0.4889 - Mean Reward 477.0 - Mean Length 513.0 - Mean Loss 1.0 - Mean Q Value 0.0 - Time Delta 242.75 - Time 2024-02-21T16:48:37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m mario\u001b[38;5;241m.\u001b[39mcache(state, next_state, action, reward, done)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m     48\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog_step(reward, loss, q)\n",
      "Cell \u001b[0;32mIn[26], line 21\u001b[0m, in \u001b[0;36mMario.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurr_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlearn_every \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m state, next_state, action, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m td_target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtd_target(reward, next_state, done)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_Q_online(state, action, td_target)\n",
      "Cell \u001b[0;32mIn[22], line 75\u001b[0m, in \u001b[0;36mMario.recall\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m states, next_states, actions, rewards, dones \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Convert batched data to TensorFlow tensors\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m states \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m next_states \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(next_states, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m     77\u001b[0m actions \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(actions, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32)\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:1176\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1176\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1178\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1179\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1180\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1496\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m   1433\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m   1435\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1436\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m   1437\u001b[0m \n\u001b[1;32m   1438\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1496\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1497\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1502\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1502\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m      \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[43m      \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/profiler/trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1642\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1633\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1634\u001b[0m           _add_error_prefix(\n\u001b[1;32m   1635\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1638\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1639\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m   1641\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1642\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m   1645\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:344\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_tensor_conversion_function\u001b[39m(v, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    342\u001b[0m                                          as_ref\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    343\u001b[0m   _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m--> 344\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:268\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 268\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:280\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    279\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 280\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[1;32m    283\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:305\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[1;32m    304\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 305\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/miniconda3/envs/mario/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:103\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    102\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use_cuda = torch.cuda.is_available()\n",
    "# print(f\"Using CUDA: {use_cuda}\")\n",
    "# print()\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "print(\"actions: \", env.action_space.n)\n",
    "# mario.load(\"checkpoints/trained_mario.chkpt\")\n",
    "# mario.load(\"checkpoints/2024-02-20T23-02-38/mario_net_60.chkpt\")\n",
    "\n",
    "mario.exploration_rate = 0.5\n",
    "mario.exploration_rate_decay = 0.9999979205606203 #0.99999975\n",
    "mario.exploration_rate_min = 0.1\n",
    "mario.save_every = 10_000  # no. of experiences between saving Mario Net\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 4000\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "        # print(state)\n",
    "        # action = 0\n",
    "        # if action == 0:\n",
    "        #     action = 1\n",
    "        # else:\n",
    "        #     action = 0\n",
    "        # print(action)\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, trunc, info = env.step(action)\n",
    "        \n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = next_state\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info[\"flag_get\"]:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if (e % 10 == 0) or (e == episodes - 1):\n",
    "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mario",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
