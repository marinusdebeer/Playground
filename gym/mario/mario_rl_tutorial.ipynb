{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# Train a Mario-playing RL Agent\n",
        "\n",
        "**Authors:** [Yuansong Feng](https://github.com/YuansongFeng)_, [Suraj Subramanian](https://github.com/suraj813)_, [Howard Wang](https://github.com/hw26)_, [Steven Guo](https://github.com/GuoYuzhang)_.\n",
        "\n",
        "\n",
        "This tutorial walks you through the fundamentals of Deep Reinforcement\n",
        "Learning. At the end, you will implement an AI-powered Mario (using\n",
        "[Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf)_) that\n",
        "can play the game by itself.\n",
        "\n",
        "Although no prior knowledge of RL is necessary for this tutorial, you\n",
        "can familiarize yourself with these RL\n",
        "[concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html)_,\n",
        "and have this handy\n",
        "[cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N)_\n",
        "as your companion. The full code is available\n",
        "[here](https://github.com/yuansongFeng/MadMario/)_.\n",
        "\n",
        ".. figure:: /_static/img/mario.gif\n",
        "   :alt: mario\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym-super-mario-bros==7.4.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (7.4.0)\n",
            "Requirement already satisfied: nes-py>=8.1.4 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym-super-mario-bros==7.4.0) (8.2.1)\n",
            "Requirement already satisfied: gym>=0.17.2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.23.5)\n",
            "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.5.21)\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.0.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (7.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from importlib-metadata>=4.8.0->gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (3.17.0)\n",
            "Requirement already satisfied: tensordict==0.3.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensordict==0.3.0) (2.2.0)\n",
            "Requirement already satisfied: numpy in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensordict==0.3.0) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from tensordict==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: filelock in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->tensordict==0.3.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->tensordict==0.3.0) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->tensordict==0.3.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->tensordict==0.3.0) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from jinja2->torch>=2.1.0->tensordict==0.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from sympy->torch>=2.1.0->tensordict==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: torchrl==0.3.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (0.3.0)\n",
            "Requirement already satisfied: torch>=2.1.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torchrl==0.3.0) (2.2.0)\n",
            "Requirement already satisfied: numpy in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torchrl==0.3.0) (1.23.5)\n",
            "Requirement already satisfied: packaging in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torchrl==0.3.0) (23.2)\n",
            "Requirement already satisfied: cloudpickle in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torchrl==0.3.0) (3.0.0)\n",
            "Requirement already satisfied: tensordict>=0.3.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torchrl==0.3.0) (0.3.0)\n",
            "Requirement already satisfied: filelock in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->torchrl==0.3.0) (4.9.0)\n",
            "Requirement already satisfied: sympy in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->torchrl==0.3.0) (1.12)\n",
            "Requirement already satisfied: networkx in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->torchrl==0.3.0) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from torch>=2.1.0->torchrl==0.3.0) (2024.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from jinja2->torch>=2.1.0->torchrl==0.3.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages (from sympy->torch>=2.1.0->torchrl==0.3.0) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0\n",
        "pip install tensordict==0.3.0\n",
        "pip install torchrl==0.3.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os\n",
        "\n",
        "# Gym is an OpenAI toolkit for RL\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "\n",
        "from tensordict import TensorDict\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RL Definitions\n",
        "\n",
        "**Environment** The world that an agent interacts with and learns from.\n",
        "\n",
        "**Action** $a$ : How the Agent responds to the Environment. The\n",
        "set of all possible Actions is called *action-space*.\n",
        "\n",
        "**State** $s$ : The current characteristic of the Environment. The\n",
        "set of all possible States the Environment can be in is called\n",
        "*state-space*.\n",
        "\n",
        "**Reward** $r$ : Reward is the key feedback from Environment to\n",
        "Agent. It is what drives the Agent to learn and to change its future\n",
        "action. An aggregation of rewards over multiple time steps is called\n",
        "**Return**.\n",
        "\n",
        "**Optimal Action-Value function** $Q^*(s,a)$ : Gives the expected\n",
        "return if you start in state $s$, take an arbitrary action\n",
        "$a$, and then for each future time step take the action that\n",
        "maximizes returns. $Q$ can be said to stand for the “quality” of\n",
        "the action in a state. We try to approximate this function.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment\n",
        "\n",
        "### Initialize Environment\n",
        "\n",
        "In Mario, the environment consists of tubes, mushrooms and other\n",
        "components.\n",
        "\n",
        "When Mario makes an action, the environment responds with the changed\n",
        "(next) state, reward and other info.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(240, 256, 3),\n",
            " 0.0,\n",
            " False,\n",
            " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'y_pos': 79}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/Users/marinus/miniconda3/envs/mario/lib/python3.8/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
            "  logger.warn(\n"
          ]
        }
      ],
      "source": [
        "# Initialize Super Mario environment (in v0.26 change render mode to 'human' to see results on the screen)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", new_step_api=True)\n",
        "else:\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-1-1-v0\", render_mode='rgb', apply_api_compatibility=True)\n",
        "\n",
        "# Limit the action-space to\n",
        "#   0. walk right\n",
        "#   1. jump right\n",
        "env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "\n",
        "env.reset()\n",
        "next_state, reward, done, trunc, info = env.step(action=0)\n",
        "print(f\"{next_state.shape},\\n {reward},\\n {done},\\n {info}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocess Environment\n",
        "\n",
        "Environment data is returned to the agent in ``next_state``. As you saw\n",
        "above, each state is represented by a ``[3, 240, 256]`` size array.\n",
        "Often that is more information than our agent needs; for instance,\n",
        "Mario’s actions do not depend on the color of the pipes or the sky!\n",
        "\n",
        "We use **Wrappers** to preprocess environment data before sending it to\n",
        "the agent.\n",
        "\n",
        "``GrayScaleObservation`` is a common wrapper to transform an RGB image\n",
        "to grayscale; doing so reduces the size of the state representation\n",
        "without losing useful information. Now the size of each state:\n",
        "``[1, 240, 256]``\n",
        "\n",
        "``ResizeObservation`` downsamples each observation into a square image.\n",
        "New size: ``[1, 84, 84]``\n",
        "\n",
        "``SkipFrame`` is a custom wrapper that inherits from ``gym.Wrapper`` and\n",
        "implements the ``step()`` function. Because consecutive frames don’t\n",
        "vary much, we can skip n-intermediate frames without losing much\n",
        "information. The n-th frame aggregates rewards accumulated over each\n",
        "skipped frame.\n",
        "\n",
        "``FrameStack`` is a wrapper that allows us to squash consecutive frames\n",
        "of the environment into a single observation point to feed to our\n",
        "learning model. This way, we can identify if Mario was landing or\n",
        "jumping based on the direction of his movement in the previous several\n",
        "frames.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class SkipFrame(gym.Wrapper):\n",
        "    def __init__(self, env, skip):\n",
        "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
        "        super().__init__(env)\n",
        "        self._skip = skip\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Repeat action, and sum reward\"\"\"\n",
        "        total_reward = 0.0\n",
        "        for i in range(self._skip):\n",
        "            # Accumulate reward and repeat the same action\n",
        "            obs, reward, done, trunk, info = self.env.step(action)\n",
        "            total_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "        return obs, total_reward, done, trunk, info\n",
        "\n",
        "\n",
        "class GrayScaleObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs_shape = self.observation_space.shape[:2]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def permute_orientation(self, observation):\n",
        "        # permute [H, W, C] array to [C, H, W] tensor\n",
        "        observation = np.transpose(observation, (2, 0, 1))\n",
        "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
        "        return observation\n",
        "\n",
        "    def observation(self, observation):\n",
        "        observation = self.permute_orientation(observation)\n",
        "        transform = T.Grayscale()\n",
        "        observation = transform(observation)\n",
        "        return observation\n",
        "\n",
        "\n",
        "class ResizeObservation(gym.ObservationWrapper):\n",
        "    def __init__(self, env, shape):\n",
        "        super().__init__(env)\n",
        "        if isinstance(shape, int):\n",
        "            self.shape = (shape, shape)\n",
        "        else:\n",
        "            self.shape = tuple(shape)\n",
        "\n",
        "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
        "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        transforms = T.Compose(\n",
        "            [T.Resize(self.shape, antialias=True), T.Normalize(0, 255)]\n",
        "        )\n",
        "        observation = transforms(observation).squeeze(0)\n",
        "        return observation\n",
        "\n",
        "\n",
        "# Apply Wrappers to environment\n",
        "env = SkipFrame(env, skip=4)\n",
        "env = GrayScaleObservation(env)\n",
        "env = ResizeObservation(env, shape=84)\n",
        "if gym.__version__ < '0.26':\n",
        "    env = FrameStack(env, num_stack=4, new_step_api=True)\n",
        "else:\n",
        "    env = FrameStack(env, num_stack=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After applying the above wrappers to the environment, the final wrapped\n",
        "state consists of 4 gray-scaled consecutive frames stacked together, as\n",
        "shown above in the image on the left. Each time Mario makes an action,\n",
        "the environment responds with a state of this structure. The structure\n",
        "is represented by a 3-D array of size ``[4, 84, 84]``.\n",
        "\n",
        ".. figure:: /_static/img/mario_env.png\n",
        "   :alt: picture\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agent\n",
        "\n",
        "We create a class ``Mario`` to represent our agent in the game. Mario\n",
        "should be able to:\n",
        "\n",
        "-  **Act** according to the optimal action policy based on the current\n",
        "   state (of the environment).\n",
        "\n",
        "-  **Remember** experiences. Experience = (current state, current\n",
        "   action, reward, next state). Mario *caches* and later *recalls* his\n",
        "   experiences to update his action policy.\n",
        "\n",
        "-  **Learn** a better action policy over time\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__():\n",
        "        pass\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
        "        pass\n",
        "\n",
        "    def cache(self, experience):\n",
        "        \"\"\"Add the experience to memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"Sample experiences from memory\"\"\"\n",
        "        pass\n",
        "\n",
        "    def learn(self):\n",
        "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In the following sections, we will populate Mario’s parameters and\n",
        "define his functions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Act\n",
        "\n",
        "For any given state, an agent can choose to do the most optimal action\n",
        "(**exploit**) or a random action (**explore**).\n",
        "\n",
        "Mario randomly explores with a chance of ``self.exploration_rate``; when\n",
        "he chooses to exploit, he relies on ``MarioNet`` (implemented in\n",
        "``Learn`` section) to provide the most optimal action.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        self.exploration_rate = 1.0\n",
        "        self.exploration_rate_decay = 0.02\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 10_000  # no. of experiences between saving Mario Net\n",
        "\n",
        "    def decay(self, initial_epsilon, decay_rate, episode, num_episodes, minimum):\n",
        "        return  max((initial_epsilon * (decay_rate ** (episode/num_episodes))), minimum)\n",
        "    \n",
        "    def act(self, state):\n",
        "        \"\"\"\n",
        "    Given a state, choose an epsilon-greedy action and update value of step.\n",
        "\n",
        "    Inputs:\n",
        "    state(``LazyFrame``): A single observation of the current state, dimension is (state_dim)\n",
        "    Outputs:\n",
        "    ``action_idx`` (``int``): An integer representing which action Mario will perform\n",
        "    \"\"\"\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        # self.exploration_rate *= self.exploration_rate_decay\n",
        "        # self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cache and Recall\n",
        "\n",
        "These two functions serve as Mario’s “memory” process.\n",
        "\n",
        "``cache()``: Each time Mario performs an action, he stores the\n",
        "``experience`` to his memory. His experience includes the current\n",
        "*state*, *action* performed, *reward* from the action, the *next state*,\n",
        "and whether the game is *done*.\n",
        "\n",
        "``recall()``: Mario randomly samples a batch of experiences from his\n",
        "memory, and uses that to learn the game.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):  # subclassing for continuity\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 128\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        \"\"\"\n",
        "        Store the experience to self.memory (replay buffer)\n",
        "\n",
        "        Inputs:\n",
        "        state (``LazyFrame``),\n",
        "        next_state (``LazyFrame``),\n",
        "        action (``int``),\n",
        "        reward (``float``),\n",
        "        done(``bool``))\n",
        "        \"\"\"\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        # self.memory.append((state, next_state, action, reward, done,))\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        \"\"\"\n",
        "        Retrieve a batch of experiences from memory\n",
        "        \"\"\"\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learn\n",
        "\n",
        "Mario uses the [DDQN algorithm](https://arxiv.org/pdf/1509.06461)_\n",
        "under the hood. DDQN uses two ConvNets - $Q_{online}$ and\n",
        "$Q_{target}$ - that independently approximate the optimal\n",
        "action-value function.\n",
        "\n",
        "In our implementation, we share feature generator ``features`` across\n",
        "$Q_{online}$ and $Q_{target}$, but maintain separate FC\n",
        "classifiers for each. $\\theta_{target}$ (the parameters of\n",
        "$Q_{target}$) is frozen to prevent updating by backprop. Instead,\n",
        "it is periodically synced with $\\theta_{online}$ (more on this\n",
        "later).\n",
        "\n",
        "#### Neural Network\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "    \"\"\"mini CNN structure\n",
        "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        c, h, w = input_dim\n",
        "\n",
        "        if h != 84:\n",
        "            raise ValueError(f\"Expecting input height: 84, got: {h}\")\n",
        "        if w != 84:\n",
        "            raise ValueError(f\"Expecting input width: 84, got: {w}\")\n",
        "\n",
        "        self.online = self.__build_cnn(c, output_dim)\n",
        "\n",
        "        self.target = self.__build_cnn(c, output_dim)\n",
        "        self.target.load_state_dict(self.online.state_dict())\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)\n",
        "\n",
        "    def __build_cnn(self, c, output_dim):\n",
        "        model = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=c, out_channels=32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "        # Apply He initialization to the model\n",
        "        for m in model.modules():\n",
        "            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "        return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### TD Estimate & TD Target\n",
        "\n",
        "Two values are involved in learning:\n",
        "\n",
        "**TD Estimate** - the predicted optimal $Q^*$ for a given state\n",
        "$s$\n",
        "\n",
        "\\begin{align}{TD}_e = Q_{online}^*(s,a)\\end{align}\n",
        "\n",
        "**TD Target** - aggregation of current reward and the estimated\n",
        "$Q^*$ in the next state $s'$\n",
        "\n",
        "\\begin{align}a' = argmax_{a} Q_{online}(s', a)\\end{align}\n",
        "\n",
        "\\begin{align}{TD}_t = r + \\gamma Q_{target}^*(s',a')\\end{align}\n",
        "\n",
        "Because we don’t know what next action $a'$ will be, we use the\n",
        "action $a'$ maximizes $Q_{online}$ in the next state\n",
        "$s'$.\n",
        "\n",
        "Notice we use the\n",
        "[@torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad)_\n",
        "decorator on ``td_target()`` to disable gradient calculations here\n",
        "(because we don’t need to backpropagate on $\\theta_{target}$).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.gamma = 0.9\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Updating the model\n",
        "\n",
        "As Mario samples inputs from his replay buffer, we compute $TD_t$\n",
        "and $TD_e$ and backpropagate this loss down $Q_{online}$ to\n",
        "update its parameters $\\theta_{online}$ ($\\alpha$ is the\n",
        "learning rate ``lr`` passed to the ``optimizer``)\n",
        "\n",
        "\\begin{align}\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\\end{align}\n",
        "\n",
        "$\\theta_{target}$ does not update through backpropagation.\n",
        "Instead, we periodically copy $\\theta_{online}$ to\n",
        "$\\theta_{target}$\n",
        "\n",
        "\\begin{align}\\theta_{target} \\leftarrow \\theta_{online}\\end{align}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Save checkpoint\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Putting it all together\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "class Mario(Mario):\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        super().__init__(state_dim, action_dim, save_dir)\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 32  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 6000  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logging\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time, datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, save_dir):\n",
        "        self.save_log = save_dir / \"log\"\n",
        "        with open(self.save_log, \"w\") as f:\n",
        "            f.write(\n",
        "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
        "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
        "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
        "            )\n",
        "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
        "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
        "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
        "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
        "\n",
        "        # History metrics\n",
        "        self.ep_rewards = []\n",
        "        self.ep_lengths = []\n",
        "        self.ep_avg_losses = []\n",
        "        self.ep_avg_qs = []\n",
        "\n",
        "        # Moving averages, added for every call to record()\n",
        "        self.moving_avg_ep_rewards = []\n",
        "        self.moving_avg_ep_lengths = []\n",
        "        self.moving_avg_ep_avg_losses = []\n",
        "        self.moving_avg_ep_avg_qs = []\n",
        "\n",
        "        # Current episode metric\n",
        "        self.init_episode()\n",
        "\n",
        "        # Timing\n",
        "        self.record_time = time.time()\n",
        "\n",
        "    def log_step(self, reward, loss, q):\n",
        "        self.curr_ep_reward += reward\n",
        "        self.curr_ep_length += 1\n",
        "        if loss:\n",
        "            self.curr_ep_loss += loss\n",
        "            self.curr_ep_q += q\n",
        "            self.curr_ep_loss_length += 1\n",
        "\n",
        "    def log_episode(self):\n",
        "        \"Mark end of episode\"\n",
        "        self.ep_rewards.append(self.curr_ep_reward)\n",
        "        self.ep_lengths.append(self.curr_ep_length)\n",
        "        if self.curr_ep_loss_length == 0:\n",
        "            ep_avg_loss = 0\n",
        "            ep_avg_q = 0\n",
        "        else:\n",
        "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
        "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
        "        self.ep_avg_losses.append(ep_avg_loss)\n",
        "        self.ep_avg_qs.append(ep_avg_q)\n",
        "\n",
        "        self.init_episode()\n",
        "\n",
        "    def init_episode(self):\n",
        "        self.curr_ep_reward = 0.0\n",
        "        self.curr_ep_length = 0\n",
        "        self.curr_ep_loss = 0.0\n",
        "        self.curr_ep_q = 0.0\n",
        "        self.curr_ep_loss_length = 0\n",
        "\n",
        "    def record(self, episode, epsilon, step):\n",
        "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
        "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
        "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
        "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
        "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
        "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
        "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
        "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
        "\n",
        "        last_record_time = self.record_time\n",
        "        self.record_time = time.time()\n",
        "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
        "\n",
        "        print(\n",
        "            f\"Episode {episode} - \"\n",
        "            f\"Step {step} - \"\n",
        "            f\"Epsilon {round(epsilon, 4)} - \"\n",
        "            f\"Mean Reward {mean_ep_reward} - \"\n",
        "            f\"Mean Length {mean_ep_length} - \"\n",
        "            f\"Mean Loss {mean_ep_loss} - \"\n",
        "            f\"Mean Q Value {mean_ep_q} - \"\n",
        "            f\"Time Delta {time_since_last_record} - \"\n",
        "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
        "        )\n",
        "\n",
        "        with open(self.save_log, \"a\") as f:\n",
        "            f.write(\n",
        "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
        "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
        "                f\"{time_since_last_record:15.3f}\"\n",
        "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
        "            )\n",
        "\n",
        "        for metric in [\"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\", \"ep_rewards\"]:\n",
        "            plt.clf()\n",
        "            plt.plot(getattr(self, f\"moving_avg_{metric}\"), label=f\"moving_avg_{metric}\")\n",
        "            plt.legend()\n",
        "            plt.savefig(getattr(self, f\"{metric}_plot\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Let’s play!\n",
        "\n",
        "In this example we run the training loop for 40 episodes, but for Mario to truly learn the ways of\n",
        "his world, we suggest running the loop for at least 40,000 episodes!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using CUDA: False\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode 0 - Step 44 - Epsilon 1.0 - Mean Reward 223.0 - Mean Length 44.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 0.365 - Time 2024-02-21T22:45:07\n",
            "Episode 20 - Step 4656 - Epsilon 0.9845 - Mean Reward 646.19 - Mean Length 221.714 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 25.691 - Time 2024-02-21T22:45:33\n",
            "Episode 40 - Step 9784 - Epsilon 0.9692 - Mean Reward 706.0 - Mean Length 238.634 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 27.983 - Time 2024-02-21T22:46:01\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_1.chkpt at step 10000\n",
            "Episode 60 - Step 13199 - Epsilon 0.9541 - Mean Reward 671.787 - Mean Length 216.377 - Mean Loss 0.873 - Mean Q Value 0.615 - Time Delta 51.348 - Time 2024-02-21T22:46:52\n",
            "Episode 80 - Step 17573 - Epsilon 0.9393 - Mean Reward 687.506 - Mean Length 216.951 - Mean Loss 1.229 - Mean Q Value 1.321 - Time Delta 69.553 - Time 2024-02-21T22:48:02\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_2.chkpt at step 20000\n",
            "Episode 100 - Step 22090 - Epsilon 0.9247 - Mean Reward 717.45 - Mean Length 220.46 - Mean Loss 1.198 - Mean Q Value 2.299 - Time Delta 71.348 - Time 2024-02-21T22:49:13\n",
            "Episode 120 - Step 25420 - Epsilon 0.9104 - Mean Reward 708.47 - Mean Length 207.64 - Mean Loss 1.385 - Mean Q Value 3.933 - Time Delta 52.348 - Time 2024-02-21T22:50:06\n",
            "Episode 140 - Step 29066 - Epsilon 0.8962 - Mean Reward 682.31 - Mean Length 192.82 - Mean Loss 1.542 - Mean Q Value 5.821 - Time Delta 57.291 - Time 2024-02-21T22:51:03\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_3.chkpt at step 30000\n",
            "Episode 160 - Step 32974 - Epsilon 0.8823 - Mean Reward 676.07 - Mean Length 197.75 - Mean Loss 1.197 - Mean Q Value 7.646 - Time Delta 63.54 - Time 2024-02-21T22:52:06\n",
            "Episode 180 - Step 37672 - Epsilon 0.8686 - Mean Reward 651.2 - Mean Length 200.99 - Mean Loss 0.908 - Mean Q Value 9.447 - Time Delta 75.579 - Time 2024-02-21T22:53:22\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_4.chkpt at step 40000\n",
            "Episode 200 - Step 41407 - Epsilon 0.8551 - Mean Reward 621.71 - Mean Length 193.17 - Mean Loss 0.887 - Mean Q Value 11.026 - Time Delta 59.8 - Time 2024-02-21T22:54:22\n",
            "Episode 220 - Step 44960 - Epsilon 0.8419 - Mean Reward 614.84 - Mean Length 195.4 - Mean Loss 0.946 - Mean Q Value 12.542 - Time Delta 57.109 - Time 2024-02-21T22:55:19\n",
            "Episode 240 - Step 48899 - Epsilon 0.8288 - Mean Reward 622.98 - Mean Length 198.33 - Mean Loss 1.015 - Mean Q Value 14.061 - Time Delta 63.216 - Time 2024-02-21T22:56:22\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_5.chkpt at step 50000\n",
            "Episode 260 - Step 52008 - Epsilon 0.8159 - Mean Reward 629.79 - Mean Length 190.34 - Mean Loss 1.062 - Mean Q Value 15.497 - Time Delta 51.361 - Time 2024-02-21T22:57:13\n",
            "Episode 280 - Step 56649 - Epsilon 0.8033 - Mean Reward 650.89 - Mean Length 189.77 - Mean Loss 1.137 - Mean Q Value 16.94 - Time Delta 78.248 - Time 2024-02-21T22:58:32\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_6.chkpt at step 60000\n",
            "Episode 300 - Step 61190 - Epsilon 0.7908 - Mean Reward 639.62 - Mean Length 197.83 - Mean Loss 1.2 - Mean Q Value 18.115 - Time Delta 74.071 - Time 2024-02-21T22:59:46\n",
            "Episode 320 - Step 64172 - Epsilon 0.7785 - Mean Reward 650.22 - Mean Length 192.12 - Mean Loss 1.212 - Mean Q Value 19.128 - Time Delta 49.302 - Time 2024-02-21T23:00:35\n",
            "Episode 340 - Step 69343 - Epsilon 0.7664 - Mean Reward 650.74 - Mean Length 204.44 - Mean Loss 1.261 - Mean Q Value 20.015 - Time Delta 84.761 - Time 2024-02-21T23:02:00\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_7.chkpt at step 70000\n",
            "Episode 360 - Step 75101 - Epsilon 0.7545 - Mean Reward 683.13 - Mean Length 230.93 - Mean Loss 1.303 - Mean Q Value 20.688 - Time Delta 96.131 - Time 2024-02-21T23:03:36\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_8.chkpt at step 80000\n",
            "Episode 380 - Step 81974 - Epsilon 0.7428 - Mean Reward 675.51 - Mean Length 253.25 - Mean Loss 1.351 - Mean Q Value 21.172 - Time Delta 113.66 - Time 2024-02-21T23:05:30\n",
            "Episode 400 - Step 86141 - Epsilon 0.7313 - Mean Reward 696.89 - Mean Length 249.51 - Mean Loss 1.398 - Mean Q Value 21.589 - Time Delta 68.783 - Time 2024-02-21T23:06:38\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_9.chkpt at step 90000\n",
            "Episode 420 - Step 90440 - Epsilon 0.7199 - Mean Reward 715.32 - Mean Length 262.68 - Mean Loss 1.435 - Mean Q Value 22.14 - Time Delta 71.037 - Time 2024-02-21T23:07:49\n",
            "Episode 440 - Step 93635 - Epsilon 0.7087 - Mean Reward 706.01 - Mean Length 242.92 - Mean Loss 1.474 - Mean Q Value 22.616 - Time Delta 52.81 - Time 2024-02-21T23:08:42\n",
            "Episode 460 - Step 96188 - Epsilon 0.6977 - Mean Reward 659.31 - Mean Length 210.87 - Mean Loss 1.476 - Mean Q Value 23.17 - Time Delta 42.656 - Time 2024-02-21T23:09:25\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_10.chkpt at step 100000\n",
            "Episode 480 - Step 100894 - Epsilon 0.6869 - Mean Reward 685.94 - Mean Length 189.2 - Mean Loss 1.47 - Mean Q Value 23.707 - Time Delta 78.4 - Time 2024-02-21T23:10:43\n",
            "Episode 500 - Step 103505 - Epsilon 0.6762 - Mean Reward 663.13 - Mean Length 173.64 - Mean Loss 1.481 - Mean Q Value 24.352 - Time Delta 44.542 - Time 2024-02-21T23:11:28\n",
            "Episode 520 - Step 107957 - Epsilon 0.6657 - Mean Reward 652.78 - Mean Length 175.17 - Mean Loss 1.526 - Mean Q Value 24.729 - Time Delta 76.106 - Time 2024-02-21T23:12:44\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_11.chkpt at step 110000\n",
            "Episode 540 - Step 114190 - Epsilon 0.6554 - Mean Reward 697.67 - Mean Length 205.55 - Mean Loss 1.521 - Mean Q Value 25.326 - Time Delta 105.601 - Time 2024-02-21T23:14:30\n",
            "Episode 560 - Step 117001 - Epsilon 0.6452 - Mean Reward 712.78 - Mean Length 208.13 - Mean Loss 1.55 - Mean Q Value 25.705 - Time Delta 48.101 - Time 2024-02-21T23:15:18\n",
            "Episode 580 - Step 119968 - Epsilon 0.6352 - Mean Reward 671.39 - Mean Length 190.74 - Mean Loss 1.574 - Mean Q Value 26.058 - Time Delta 50.615 - Time 2024-02-21T23:16:08\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_12.chkpt at step 120000\n",
            "Episode 600 - Step 124476 - Epsilon 0.6254 - Mean Reward 703.76 - Mean Length 209.71 - Mean Loss 1.583 - Mean Q Value 26.513 - Time Delta 418.606 - Time 2024-02-21T23:23:07\n",
            "Episode 620 - Step 129037 - Epsilon 0.6156 - Mean Reward 711.33 - Mean Length 210.8 - Mean Loss 1.555 - Mean Q Value 26.92 - Time Delta 79.294 - Time 2024-02-21T23:24:26\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_13.chkpt at step 130000\n",
            "Episode 640 - Step 133137 - Epsilon 0.6061 - Mean Reward 679.64 - Mean Length 189.47 - Mean Loss 1.583 - Mean Q Value 26.984 - Time Delta 70.439 - Time 2024-02-21T23:25:37\n",
            "Episode 660 - Step 136684 - Epsilon 0.5967 - Mean Reward 699.31 - Mean Length 196.83 - Mean Loss 1.591 - Mean Q Value 27.349 - Time Delta 62.025 - Time 2024-02-21T23:26:39\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_14.chkpt at step 140000\n",
            "Episode 680 - Step 140557 - Epsilon 0.5874 - Mean Reward 708.21 - Mean Length 205.89 - Mean Loss 1.603 - Mean Q Value 27.73 - Time Delta 66.7 - Time 2024-02-21T23:27:45\n",
            "Episode 700 - Step 144963 - Epsilon 0.5783 - Mean Reward 705.26 - Mean Length 204.87 - Mean Loss 1.614 - Mean Q Value 28.001 - Time Delta 76.201 - Time 2024-02-21T23:29:02\n",
            "Episode 720 - Step 149193 - Epsilon 0.5693 - Mean Reward 685.01 - Mean Length 201.56 - Mean Loss 1.621 - Mean Q Value 28.239 - Time Delta 72.804 - Time 2024-02-21T23:30:14\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_15.chkpt at step 150000\n",
            "Episode 740 - Step 153416 - Epsilon 0.5605 - Mean Reward 673.26 - Mean Length 202.79 - Mean Loss 1.593 - Mean Q Value 28.343 - Time Delta 73.802 - Time 2024-02-21T23:31:28\n",
            "Episode 760 - Step 159252 - Epsilon 0.5518 - Mean Reward 671.38 - Mean Length 225.68 - Mean Loss 1.597 - Mean Q Value 28.38 - Time Delta 100.248 - Time 2024-02-21T23:33:08\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_16.chkpt at step 160000\n",
            "Episode 780 - Step 162871 - Epsilon 0.5432 - Mean Reward 709.16 - Mean Length 223.14 - Mean Loss 1.556 - Mean Q Value 28.323 - Time Delta 62.316 - Time 2024-02-21T23:34:11\n",
            "Episode 800 - Step 165980 - Epsilon 0.5348 - Mean Reward 685.08 - Mean Length 210.17 - Mean Loss 1.546 - Mean Q Value 28.262 - Time Delta 54.936 - Time 2024-02-21T23:35:06\n",
            "Episode 820 - Step 169075 - Epsilon 0.5265 - Mean Reward 683.72 - Mean Length 198.82 - Mean Loss 1.545 - Mean Q Value 28.177 - Time Delta 53.812 - Time 2024-02-21T23:35:59\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_17.chkpt at step 170000\n",
            "Episode 840 - Step 172461 - Epsilon 0.5183 - Mean Reward 711.7 - Mean Length 190.45 - Mean Loss 1.553 - Mean Q Value 28.456 - Time Delta 60.092 - Time 2024-02-21T23:37:00\n",
            "Episode 860 - Step 176299 - Epsilon 0.5102 - Mean Reward 732.37 - Mean Length 170.47 - Mean Loss 1.546 - Mean Q Value 28.74 - Time Delta 66.217 - Time 2024-02-21T23:38:06\n",
            "Episode 880 - Step 179582 - Epsilon 0.5023 - Mean Reward 705.35 - Mean Length 167.11 - Mean Loss 1.587 - Mean Q Value 29.059 - Time Delta 56.839 - Time 2024-02-21T23:39:03\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_18.chkpt at step 180000\n",
            "Episode 900 - Step 183331 - Epsilon 0.4945 - Mean Reward 721.02 - Mean Length 173.51 - Mean Loss 1.623 - Mean Q Value 29.478 - Time Delta 65.49 - Time 2024-02-21T23:40:08\n",
            "Episode 920 - Step 185857 - Epsilon 0.4868 - Mean Reward 709.13 - Mean Length 167.82 - Mean Loss 1.625 - Mean Q Value 29.958 - Time Delta 44.062 - Time 2024-02-21T23:40:52\n",
            "Episode 940 - Step 188432 - Epsilon 0.4793 - Mean Reward 666.9 - Mean Length 159.71 - Mean Loss 1.641 - Mean Q Value 30.392 - Time Delta 44.849 - Time 2024-02-21T23:41:37\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_19.chkpt at step 190000\n",
            "Episode 960 - Step 191646 - Epsilon 0.4718 - Mean Reward 621.12 - Mean Length 153.47 - Mean Loss 1.663 - Mean Q Value 30.763 - Time Delta 58.667 - Time 2024-02-21T23:42:36\n",
            "Episode 980 - Step 196520 - Epsilon 0.4645 - Mean Reward 605.56 - Mean Length 169.38 - Mean Loss 1.663 - Mean Q Value 30.966 - Time Delta 88.418 - Time 2024-02-21T23:44:04\n",
            "Episode 1000 - Step 198906 - Epsilon 0.4573 - Mean Reward 586.36 - Mean Length 155.75 - Mean Loss 1.656 - Mean Q Value 30.917 - Time Delta 42.075 - Time 2024-02-21T23:44:46\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_20.chkpt at step 200000\n",
            "Episode 1020 - Step 202153 - Epsilon 0.4502 - Mean Reward 638.32 - Mean Length 162.96 - Mean Loss 1.683 - Mean Q Value 31.125 - Time Delta 57.132 - Time 2024-02-21T23:45:43\n",
            "Episode 1040 - Step 204258 - Epsilon 0.4432 - Mean Reward 630.54 - Mean Length 158.26 - Mean Loss 1.715 - Mean Q Value 31.244 - Time Delta 37.191 - Time 2024-02-21T23:46:20\n",
            "Episode 1060 - Step 207514 - Epsilon 0.4363 - Mean Reward 659.72 - Mean Length 158.68 - Mean Loss 1.719 - Mean Q Value 31.208 - Time Delta 56.759 - Time 2024-02-21T23:47:17\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_21.chkpt at step 210000\n",
            "Episode 1080 - Step 211733 - Epsilon 0.4296 - Mean Reward 648.73 - Mean Length 152.13 - Mean Loss 1.753 - Mean Q Value 31.313 - Time Delta 74.028 - Time 2024-02-21T23:48:31\n",
            "Episode 1100 - Step 215464 - Epsilon 0.4229 - Mean Reward 675.41 - Mean Length 165.58 - Mean Loss 1.739 - Mean Q Value 31.649 - Time Delta 65.241 - Time 2024-02-21T23:49:37\n",
            "Episode 1120 - Step 218456 - Epsilon 0.4163 - Mean Reward 644.05 - Mean Length 163.03 - Mean Loss 1.717 - Mean Q Value 31.783 - Time Delta 52.333 - Time 2024-02-21T23:50:29\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_22.chkpt at step 220000\n",
            "Episode 1140 - Step 221712 - Epsilon 0.4099 - Mean Reward 691.64 - Mean Length 174.54 - Mean Loss 1.682 - Mean Q Value 32.091 - Time Delta 57.337 - Time 2024-02-21T23:51:26\n",
            "Episode 1160 - Step 223838 - Epsilon 0.4035 - Mean Reward 651.38 - Mean Length 163.24 - Mean Loss 1.688 - Mean Q Value 32.397 - Time Delta 37.225 - Time 2024-02-21T23:52:03\n",
            "Episode 1180 - Step 226238 - Epsilon 0.3972 - Mean Reward 649.4 - Mean Length 145.05 - Mean Loss 1.652 - Mean Q Value 32.821 - Time Delta 42.21 - Time 2024-02-21T23:52:46\n",
            "Episode 1200 - Step 228399 - Epsilon 0.3911 - Mean Reward 608.12 - Mean Length 129.35 - Mean Loss 1.676 - Mean Q Value 33.088 - Time Delta 38.123 - Time 2024-02-21T23:53:24\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_23.chkpt at step 230000\n",
            "Episode 1220 - Step 232391 - Epsilon 0.385 - Mean Reward 628.21 - Mean Length 139.35 - Mean Loss 1.696 - Mean Q Value 33.402 - Time Delta 69.85 - Time 2024-02-21T23:54:34\n",
            "Episode 1240 - Step 234599 - Epsilon 0.379 - Mean Reward 573.33 - Mean Length 128.87 - Mean Loss 1.683 - Mean Q Value 33.567 - Time Delta 38.726 - Time 2024-02-21T23:55:12\n",
            "Episode 1260 - Step 237271 - Epsilon 0.3731 - Mean Reward 584.7 - Mean Length 134.33 - Mean Loss 1.701 - Mean Q Value 33.681 - Time Delta 46.513 - Time 2024-02-21T23:55:59\n",
            "Episode 1280 - Step 239850 - Epsilon 0.3673 - Mean Reward 581.68 - Mean Length 136.12 - Mean Loss 1.742 - Mean Q Value 33.745 - Time Delta 45.28 - Time 2024-02-21T23:56:44\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_24.chkpt at step 240000\n",
            "Episode 1300 - Step 242663 - Epsilon 0.3616 - Mean Reward 615.83 - Mean Length 142.64 - Mean Loss 1.761 - Mean Q Value 33.939 - Time Delta 49.95 - Time 2024-02-21T23:57:34\n",
            "Episode 1320 - Step 245772 - Epsilon 0.356 - Mean Reward 608.12 - Mean Length 133.81 - Mean Loss 1.776 - Mean Q Value 33.893 - Time Delta 56.308 - Time 2024-02-21T23:58:30\n",
            "Episode 1340 - Step 247976 - Epsilon 0.3505 - Mean Reward 612.61 - Mean Length 133.77 - Mean Loss 1.78 - Mean Q Value 34.096 - Time Delta 39.33 - Time 2024-02-21T23:59:10\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_25.chkpt at step 250000\n",
            "Episode 1360 - Step 250618 - Epsilon 0.345 - Mean Reward 626.34 - Mean Length 133.47 - Mean Loss 1.777 - Mean Q Value 34.53 - Time Delta 47.279 - Time 2024-02-21T23:59:57\n",
            "Episode 1380 - Step 253756 - Epsilon 0.3397 - Mean Reward 677.9 - Mean Length 139.06 - Mean Loss 1.783 - Mean Q Value 34.847 - Time Delta 55.92 - Time 2024-02-22T00:00:53\n",
            "Episode 1400 - Step 257147 - Epsilon 0.3344 - Mean Reward 681.1 - Mean Length 144.84 - Mean Loss 1.758 - Mean Q Value 35.226 - Time Delta 59.525 - Time 2024-02-22T00:01:52\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_26.chkpt at step 260000\n",
            "Episode 1420 - Step 260667 - Epsilon 0.3292 - Mean Reward 668.48 - Mean Length 148.95 - Mean Loss 1.76 - Mean Q Value 36.007 - Time Delta 62.734 - Time 2024-02-22T00:02:55\n",
            "Episode 1440 - Step 262928 - Epsilon 0.3241 - Mean Reward 684.88 - Mean Length 149.52 - Mean Loss 1.81 - Mean Q Value 36.587 - Time Delta 40.298 - Time 2024-02-22T00:03:35\n",
            "Episode 1460 - Step 266139 - Epsilon 0.3191 - Mean Reward 695.26 - Mean Length 155.21 - Mean Loss 1.829 - Mean Q Value 37.109 - Time Delta 56.924 - Time 2024-02-22T00:04:32\n",
            "Episode 1480 - Step 269260 - Epsilon 0.3141 - Mean Reward 668.45 - Mean Length 155.04 - Mean Loss 1.829 - Mean Q Value 37.715 - Time Delta 56.296 - Time 2024-02-22T00:05:29\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_27.chkpt at step 270000\n",
            "Episode 1500 - Step 271373 - Epsilon 0.3092 - Mean Reward 632.54 - Mean Length 142.26 - Mean Loss 1.845 - Mean Q Value 38.095 - Time Delta 37.767 - Time 2024-02-22T00:06:06\n",
            "Episode 1520 - Step 273572 - Epsilon 0.3044 - Mean Reward 611.22 - Mean Length 129.05 - Mean Loss 1.833 - Mean Q Value 38.416 - Time Delta 39.168 - Time 2024-02-22T00:06:46\n",
            "Episode 1540 - Step 276459 - Epsilon 0.2997 - Mean Reward 620.19 - Mean Length 135.31 - Mean Loss 1.833 - Mean Q Value 38.553 - Time Delta 51.191 - Time 2024-02-22T00:07:37\n",
            "Episode 1560 - Step 279017 - Epsilon 0.2951 - Mean Reward 612.13 - Mean Length 128.78 - Mean Loss 1.823 - Mean Q Value 38.531 - Time Delta 45.53 - Time 2024-02-22T00:08:22\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_28.chkpt at step 280000\n",
            "Episode 1580 - Step 281219 - Epsilon 0.2905 - Mean Reward 593.47 - Mean Length 119.59 - Mean Loss 1.811 - Mean Q Value 38.826 - Time Delta 39.278 - Time 2024-02-22T00:09:02\n",
            "Episode 1600 - Step 284278 - Epsilon 0.286 - Mean Reward 599.34 - Mean Length 129.05 - Mean Loss 1.797 - Mean Q Value 39.077 - Time Delta 54.225 - Time 2024-02-22T00:09:56\n",
            "Episode 1620 - Step 287560 - Epsilon 0.2815 - Mean Reward 617.79 - Mean Length 139.88 - Mean Loss 1.78 - Mean Q Value 39.08 - Time Delta 58.086 - Time 2024-02-22T00:10:54\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_29.chkpt at step 290000\n",
            "Episode 1640 - Step 292240 - Epsilon 0.2772 - Mean Reward 624.92 - Mean Length 157.81 - Mean Loss 1.772 - Mean Q Value 39.256 - Time Delta 83.341 - Time 2024-02-22T00:12:17\n",
            "Episode 1660 - Step 294504 - Epsilon 0.2729 - Mean Reward 608.61 - Mean Length 154.87 - Mean Loss 1.755 - Mean Q Value 39.604 - Time Delta 41.487 - Time 2024-02-22T00:12:59\n",
            "Episode 1680 - Step 297743 - Epsilon 0.2686 - Mean Reward 643.1 - Mean Length 165.24 - Mean Loss 1.749 - Mean Q Value 39.669 - Time Delta 58.872 - Time 2024-02-22T00:13:58\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_30.chkpt at step 300000\n",
            "Episode 1700 - Step 300141 - Epsilon 0.2645 - Mean Reward 642.5 - Mean Length 158.63 - Mean Loss 1.775 - Mean Q Value 39.844 - Time Delta 42.856 - Time 2024-02-22T00:14:40\n",
            "Episode 1720 - Step 302143 - Epsilon 0.2603 - Mean Reward 624.31 - Mean Length 145.83 - Mean Loss 1.775 - Mean Q Value 39.958 - Time Delta 36.124 - Time 2024-02-22T00:15:17\n",
            "Episode 1740 - Step 304165 - Epsilon 0.2563 - Mean Reward 599.99 - Mean Length 119.25 - Mean Loss 1.775 - Mean Q Value 39.982 - Time Delta 36.992 - Time 2024-02-22T00:15:54\n",
            "Episode 1760 - Step 306751 - Epsilon 0.2523 - Mean Reward 620.0 - Mean Length 122.47 - Mean Loss 1.803 - Mean Q Value 40.108 - Time Delta 46.945 - Time 2024-02-22T00:16:41\n",
            "Episode 1780 - Step 309207 - Epsilon 0.2484 - Mean Reward 603.19 - Mean Length 114.64 - Mean Loss 1.84 - Mean Q Value 40.145 - Time Delta 45.413 - Time 2024-02-22T00:17:26\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_31.chkpt at step 310000\n",
            "Episode 1800 - Step 311831 - Epsilon 0.2446 - Mean Reward 628.5 - Mean Length 116.9 - Mean Loss 1.833 - Mean Q Value 40.365 - Time Delta 50.456 - Time 2024-02-22T00:18:16\n",
            "Episode 1820 - Step 314765 - Epsilon 0.2408 - Mean Reward 661.4 - Mean Length 126.22 - Mean Loss 1.886 - Mean Q Value 40.719 - Time Delta 52.873 - Time 2024-02-22T00:19:09\n",
            "Episode 1840 - Step 317190 - Epsilon 0.237 - Mean Reward 675.49 - Mean Length 130.25 - Mean Loss 1.914 - Mean Q Value 41.001 - Time Delta 44.388 - Time 2024-02-22T00:19:54\n",
            "Episode 1860 - Step 319323 - Epsilon 0.2333 - Mean Reward 653.67 - Mean Length 125.72 - Mean Loss 1.893 - Mean Q Value 41.23 - Time Delta 38.119 - Time 2024-02-22T00:20:32\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_32.chkpt at step 320000\n",
            "Episode 1880 - Step 321490 - Epsilon 0.2297 - Mean Reward 633.06 - Mean Length 122.83 - Mean Loss 1.858 - Mean Q Value 41.486 - Time Delta 39.703 - Time 2024-02-22T00:21:11\n",
            "Episode 1900 - Step 323407 - Epsilon 0.2261 - Mean Reward 593.28 - Mean Length 115.76 - Mean Loss 1.888 - Mean Q Value 41.531 - Time Delta 35.223 - Time 2024-02-22T00:21:47\n",
            "Episode 1920 - Step 326322 - Epsilon 0.2226 - Mean Reward 597.3 - Mean Length 115.57 - Mean Loss 1.836 - Mean Q Value 41.651 - Time Delta 52.1 - Time 2024-02-22T00:22:39\n",
            "Episode 1940 - Step 328946 - Epsilon 0.2192 - Mean Reward 606.98 - Mean Length 117.56 - Mean Loss 1.82 - Mean Q Value 41.882 - Time Delta 47.692 - Time 2024-02-22T00:23:27\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_33.chkpt at step 330000\n",
            "Episode 1960 - Step 330723 - Epsilon 0.2158 - Mean Reward 586.68 - Mean Length 114.0 - Mean Loss 1.811 - Mean Q Value 42.143 - Time Delta 32.106 - Time 2024-02-22T00:23:59\n",
            "Episode 1980 - Step 332478 - Epsilon 0.2124 - Mean Reward 567.55 - Mean Length 109.88 - Mean Loss 1.833 - Mean Q Value 42.457 - Time Delta 31.824 - Time 2024-02-22T00:24:30\n",
            "Episode 2000 - Step 335196 - Epsilon 0.2091 - Mean Reward 589.12 - Mean Length 117.89 - Mean Loss 1.788 - Mean Q Value 42.714 - Time Delta 50.051 - Time 2024-02-22T00:25:20\n",
            "Episode 2020 - Step 337466 - Epsilon 0.2059 - Mean Reward 560.41 - Mean Length 111.44 - Mean Loss 1.809 - Mean Q Value 42.94 - Time Delta 41.897 - Time 2024-02-22T00:26:02\n",
            "Episode 2040 - Step 339424 - Epsilon 0.2027 - Mean Reward 527.56 - Mean Length 104.78 - Mean Loss 1.789 - Mean Q Value 43.326 - Time Delta 35.818 - Time 2024-02-22T00:26:38\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_34.chkpt at step 340000\n",
            "Episode 2060 - Step 341399 - Epsilon 0.1995 - Mean Reward 540.61 - Mean Length 106.76 - Mean Loss 1.836 - Mean Q Value 43.609 - Time Delta 35.541 - Time 2024-02-22T00:27:14\n",
            "Episode 2080 - Step 344692 - Epsilon 0.1964 - Mean Reward 596.16 - Mean Length 122.14 - Mean Loss 1.829 - Mean Q Value 43.796 - Time Delta 59.429 - Time 2024-02-22T00:28:13\n",
            "Episode 2100 - Step 347389 - Epsilon 0.1934 - Mean Reward 608.36 - Mean Length 121.93 - Mean Loss 1.853 - Mean Q Value 43.93 - Time Delta 48.411 - Time 2024-02-22T00:29:02\n",
            "Episode 2120 - Step 349907 - Epsilon 0.1904 - Mean Reward 620.95 - Mean Length 124.41 - Mean Loss 1.891 - Mean Q Value 44.02 - Time Delta 45.241 - Time 2024-02-22T00:29:47\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_35.chkpt at step 350000\n",
            "Episode 2140 - Step 352616 - Epsilon 0.1874 - Mean Reward 649.52 - Mean Length 131.92 - Mean Loss 1.899 - Mean Q Value 43.985 - Time Delta 48.971 - Time 2024-02-22T00:30:36\n",
            "Episode 2160 - Step 354557 - Epsilon 0.1845 - Mean Reward 646.29 - Mean Length 131.58 - Mean Loss 1.848 - Mean Q Value 43.89 - Time Delta 35.175 - Time 2024-02-22T00:31:11\n",
            "Episode 2180 - Step 356633 - Epsilon 0.1817 - Mean Reward 604.01 - Mean Length 119.41 - Mean Loss 1.883 - Mean Q Value 44.006 - Time Delta 38.69 - Time 2024-02-22T00:31:50\n",
            "Episode 2200 - Step 359944 - Epsilon 0.1788 - Mean Reward 617.24 - Mean Length 125.55 - Mean Loss 1.845 - Mean Q Value 44.253 - Time Delta 59.535 - Time 2024-02-22T00:32:49\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_36.chkpt at step 360000\n",
            "Episode 2220 - Step 362301 - Epsilon 0.1761 - Mean Reward 611.92 - Mean Length 123.94 - Mean Loss 1.798 - Mean Q Value 44.567 - Time Delta 43.04 - Time 2024-02-22T00:33:32\n",
            "Episode 2240 - Step 364878 - Epsilon 0.1733 - Mean Reward 615.23 - Mean Length 122.62 - Mean Loss 1.796 - Mean Q Value 44.663 - Time Delta 46.655 - Time 2024-02-22T00:34:19\n",
            "Episode 2260 - Step 367448 - Epsilon 0.1706 - Mean Reward 642.33 - Mean Length 128.91 - Mean Loss 1.818 - Mean Q Value 44.875 - Time Delta 46.275 - Time 2024-02-22T00:35:05\n",
            "Episode 2280 - Step 369693 - Epsilon 0.168 - Mean Reward 653.89 - Mean Length 130.6 - Mean Loss 1.768 - Mean Q Value 44.838 - Time Delta 39.996 - Time 2024-02-22T00:35:45\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_37.chkpt at step 370000\n",
            "Episode 2300 - Step 372442 - Epsilon 0.1654 - Mean Reward 639.21 - Mean Length 124.98 - Mean Loss 1.8 - Mean Q Value 44.86 - Time Delta 49.346 - Time 2024-02-22T00:36:35\n",
            "Episode 2320 - Step 376368 - Epsilon 0.1628 - Mean Reward 635.5 - Mean Length 140.67 - Mean Loss 1.824 - Mean Q Value 44.599 - Time Delta 70.023 - Time 2024-02-22T00:37:45\n",
            "Episode 2340 - Step 379170 - Epsilon 0.1603 - Mean Reward 642.14 - Mean Length 142.92 - Mean Loss 1.827 - Mean Q Value 44.604 - Time Delta 50.301 - Time 2024-02-22T00:38:35\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_38.chkpt at step 380000\n",
            "Episode 2360 - Step 381366 - Epsilon 0.1578 - Mean Reward 623.25 - Mean Length 139.18 - Mean Loss 1.836 - Mean Q Value 44.51 - Time Delta 40.221 - Time 2024-02-22T00:39:15\n",
            "Episode 2380 - Step 384091 - Epsilon 0.1553 - Mean Reward 652.76 - Mean Length 143.98 - Mean Loss 1.835 - Mean Q Value 44.558 - Time Delta 49.577 - Time 2024-02-22T00:40:05\n",
            "Episode 2400 - Step 386711 - Epsilon 0.1529 - Mean Reward 649.62 - Mean Length 142.69 - Mean Loss 1.83 - Mean Q Value 44.683 - Time Delta 48.189 - Time 2024-02-22T00:40:53\n",
            "Episode 2420 - Step 389231 - Epsilon 0.1506 - Mean Reward 661.81 - Mean Length 128.63 - Mean Loss 1.833 - Mean Q Value 45.074 - Time Delta 47.2 - Time 2024-02-22T00:41:40\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_39.chkpt at step 390000\n",
            "Episode 2440 - Step 391670 - Epsilon 0.1482 - Mean Reward 650.04 - Mean Length 125.0 - Mean Loss 1.852 - Mean Q Value 45.45 - Time Delta 44.356 - Time 2024-02-22T00:42:24\n",
            "Episode 2460 - Step 396139 - Epsilon 0.1459 - Mean Reward 699.79 - Mean Length 147.73 - Mean Loss 1.859 - Mean Q Value 45.926 - Time Delta 80.882 - Time 2024-02-22T00:43:45\n",
            "Episode 2480 - Step 398553 - Epsilon 0.1437 - Mean Reward 675.26 - Mean Length 144.62 - Mean Loss 1.858 - Mean Q Value 46.227 - Time Delta 43.365 - Time 2024-02-22T00:44:29\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_40.chkpt at step 400000\n",
            "Episode 2500 - Step 400967 - Epsilon 0.1414 - Mean Reward 665.57 - Mean Length 142.56 - Mean Loss 1.872 - Mean Q Value 46.412 - Time Delta 43.772 - Time 2024-02-22T00:45:12\n",
            "Episode 2520 - Step 403122 - Epsilon 0.1392 - Mean Reward 646.38 - Mean Length 138.91 - Mean Loss 1.884 - Mean Q Value 46.367 - Time Delta 38.523 - Time 2024-02-22T00:45:51\n",
            "Episode 2540 - Step 405715 - Epsilon 0.1371 - Mean Reward 652.55 - Mean Length 140.45 - Mean Loss 1.859 - Mean Q Value 46.171 - Time Delta 45.409 - Time 2024-02-22T00:46:36\n",
            "Episode 2560 - Step 407406 - Epsilon 0.1349 - Mean Reward 581.94 - Mean Length 112.67 - Mean Loss 1.835 - Mean Q Value 45.982 - Time Delta 30.135 - Time 2024-02-22T00:47:06\n",
            "Episode 2580 - Step 409814 - Epsilon 0.1328 - Mean Reward 566.5 - Mean Length 112.61 - Mean Loss 1.846 - Mean Q Value 45.827 - Time Delta 41.848 - Time 2024-02-22T00:47:48\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_41.chkpt at step 410000\n",
            "Episode 2600 - Step 412263 - Epsilon 0.1308 - Mean Reward 553.58 - Mean Length 112.96 - Mean Loss 1.822 - Mean Q Value 45.804 - Time Delta 43.21 - Time 2024-02-22T00:48:32\n",
            "Episode 2620 - Step 414332 - Epsilon 0.1287 - Mean Reward 543.79 - Mean Length 112.1 - Mean Loss 1.803 - Mean Q Value 45.851 - Time Delta 36.051 - Time 2024-02-22T00:49:08\n",
            "Episode 2640 - Step 416685 - Epsilon 0.1267 - Mean Reward 531.68 - Mean Length 109.7 - Mean Loss 1.816 - Mean Q Value 46.178 - Time Delta 41.437 - Time 2024-02-22T00:49:49\n",
            "Episode 2660 - Step 419508 - Epsilon 0.1248 - Mean Reward 585.82 - Mean Length 121.02 - Mean Loss 1.828 - Mean Q Value 46.332 - Time Delta 48.846 - Time 2024-02-22T00:50:38\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_42.chkpt at step 420000\n",
            "Episode 2680 - Step 422802 - Epsilon 0.1228 - Mean Reward 627.83 - Mean Length 129.88 - Mean Loss 1.813 - Mean Q Value 46.742 - Time Delta 57.902 - Time 2024-02-22T00:51:36\n",
            "Episode 2700 - Step 424765 - Epsilon 0.1209 - Mean Reward 622.65 - Mean Length 125.02 - Mean Loss 1.808 - Mean Q Value 46.96 - Time Delta 35.811 - Time 2024-02-22T00:52:12\n",
            "Episode 2720 - Step 426856 - Epsilon 0.1191 - Mean Reward 619.33 - Mean Length 125.24 - Mean Loss 1.81 - Mean Q Value 47.216 - Time Delta 36.839 - Time 2024-02-22T00:52:48\n",
            "Episode 2740 - Step 429838 - Epsilon 0.1172 - Mean Reward 616.25 - Mean Length 131.53 - Mean Loss 1.829 - Mean Q Value 47.094 - Time Delta 208.435 - Time 2024-02-22T00:56:17\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_43.chkpt at step 430000\n",
            "Episode 2760 - Step 432146 - Epsilon 0.1154 - Mean Reward 594.62 - Mean Length 126.38 - Mean Loss 1.836 - Mean Q Value 47.137 - Time Delta 328.597 - Time 2024-02-22T01:01:45\n",
            "Episode 2780 - Step 434516 - Epsilon 0.1136 - Mean Reward 568.77 - Mean Length 117.14 - Mean Loss 1.866 - Mean Q Value 47.131 - Time Delta 312.138 - Time 2024-02-22T01:06:58\n",
            "Episode 2800 - Step 436564 - Epsilon 0.1118 - Mean Reward 577.46 - Mean Length 117.99 - Mean Loss 1.875 - Mean Q Value 47.054 - Time Delta 36.061 - Time 2024-02-22T01:07:34\n",
            "Episode 2820 - Step 439118 - Epsilon 0.1101 - Mean Reward 587.21 - Mean Length 122.62 - Mean Loss 1.832 - Mean Q Value 47.27 - Time Delta 139.045 - Time 2024-02-22T01:09:53\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_44.chkpt at step 440000\n",
            "Episode 2840 - Step 442042 - Epsilon 0.1084 - Mean Reward 594.78 - Mean Length 122.04 - Mean Loss 1.815 - Mean Q Value 47.429 - Time Delta 1069.983 - Time 2024-02-22T01:27:43\n",
            "Episode 2860 - Step 445235 - Epsilon 0.1067 - Mean Reward 599.46 - Mean Length 130.89 - Mean Loss 1.795 - Mean Q Value 47.561 - Time Delta 56.112 - Time 2024-02-22T01:28:39\n",
            "Episode 2880 - Step 448244 - Epsilon 0.105 - Mean Reward 603.79 - Mean Length 137.28 - Mean Loss 1.726 - Mean Q Value 47.524 - Time Delta 52.189 - Time 2024-02-22T01:29:31\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_45.chkpt at step 450000\n",
            "Episode 2900 - Step 451353 - Epsilon 0.1034 - Mean Reward 632.09 - Mean Length 147.89 - Mean Loss 1.709 - Mean Q Value 47.51 - Time Delta 957.921 - Time 2024-02-22T01:45:29\n",
            "Episode 2920 - Step 453189 - Epsilon 0.1018 - Mean Reward 621.03 - Mean Length 140.71 - Mean Loss 1.72 - Mean Q Value 47.218 - Time Delta 32.792 - Time 2024-02-22T01:46:02\n",
            "Episode 2940 - Step 456303 - Epsilon 0.1002 - Mean Reward 641.92 - Mean Length 142.61 - Mean Loss 1.684 - Mean Q Value 47.144 - Time Delta 54.451 - Time 2024-02-22T01:46:56\n",
            "Episode 2960 - Step 458614 - Epsilon 0.1 - Mean Reward 621.94 - Mean Length 133.79 - Mean Loss 1.65 - Mean Q Value 46.939 - Time Delta 40.19 - Time 2024-02-22T01:47:36\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_46.chkpt at step 460000\n",
            "Episode 2980 - Step 461869 - Epsilon 0.1 - Mean Reward 664.73 - Mean Length 136.25 - Mean Loss 1.705 - Mean Q Value 46.891 - Time Delta 57.292 - Time 2024-02-22T01:48:34\n",
            "Episode 3000 - Step 464960 - Epsilon 0.1 - Mean Reward 679.65 - Mean Length 136.07 - Mean Loss 1.714 - Mean Q Value 46.841 - Time Delta 56.812 - Time 2024-02-22T01:49:30\n",
            "Episode 3020 - Step 468235 - Epsilon 0.1 - Mean Reward 727.8 - Mean Length 150.46 - Mean Loss 1.722 - Mean Q Value 46.729 - Time Delta 61.869 - Time 2024-02-22T01:50:32\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_47.chkpt at step 470000\n",
            "Episode 3040 - Step 471534 - Epsilon 0.1 - Mean Reward 737.34 - Mean Length 152.31 - Mean Loss 1.781 - Mean Q Value 46.542 - Time Delta 67.466 - Time 2024-02-22T01:51:40\n",
            "Episode 3060 - Step 474107 - Epsilon 0.1 - Mean Reward 753.52 - Mean Length 154.93 - Mean Loss 1.812 - Mean Q Value 46.522 - Time Delta 52.0 - Time 2024-02-22T01:52:32\n",
            "Episode 3080 - Step 477162 - Epsilon 0.1 - Mean Reward 732.82 - Mean Length 152.93 - Mean Loss 1.814 - Mean Q Value 46.463 - Time Delta 62.889 - Time 2024-02-22T01:53:35\n",
            "Episode 3100 - Step 479774 - Epsilon 0.1 - Mean Reward 719.22 - Mean Length 148.14 - Mean Loss 1.814 - Mean Q Value 46.521 - Time Delta 53.542 - Time 2024-02-22T01:54:28\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_48.chkpt at step 480000\n",
            "Episode 3120 - Step 482812 - Epsilon 0.1 - Mean Reward 728.19 - Mean Length 145.77 - Mean Loss 1.825 - Mean Q Value 46.585 - Time Delta 63.002 - Time 2024-02-22T01:55:31\n",
            "Episode 3140 - Step 485741 - Epsilon 0.1 - Mean Reward 728.93 - Mean Length 142.07 - Mean Loss 1.779 - Mean Q Value 46.716 - Time Delta 60.735 - Time 2024-02-22T01:56:32\n",
            "Episode 3160 - Step 487911 - Epsilon 0.1 - Mean Reward 726.87 - Mean Length 138.04 - Mean Loss 1.773 - Mean Q Value 46.678 - Time Delta 46.327 - Time 2024-02-22T01:57:18\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_49.chkpt at step 490000\n",
            "Episode 3180 - Step 490612 - Epsilon 0.1 - Mean Reward 708.93 - Mean Length 134.5 - Mean Loss 1.752 - Mean Q Value 46.587 - Time Delta 58.283 - Time 2024-02-22T01:58:17\n",
            "Episode 3200 - Step 493808 - Epsilon 0.1 - Mean Reward 737.7 - Mean Length 140.34 - Mean Loss 1.754 - Mean Q Value 46.507 - Time Delta 70.319 - Time 2024-02-22T01:59:27\n",
            "Episode 3220 - Step 496231 - Epsilon 0.1 - Mean Reward 697.04 - Mean Length 134.19 - Mean Loss 1.751 - Mean Q Value 46.594 - Time Delta 52.158 - Time 2024-02-22T02:00:19\n",
            "Episode 3240 - Step 499454 - Epsilon 0.1 - Mean Reward 688.42 - Mean Length 137.13 - Mean Loss 1.739 - Mean Q Value 46.834 - Time Delta 67.759 - Time 2024-02-22T02:01:27\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_50.chkpt at step 500000\n",
            "Episode 3260 - Step 501151 - Epsilon 0.1 - Mean Reward 664.05 - Mean Length 132.4 - Mean Loss 1.733 - Mean Q Value 47.148 - Time Delta 37.971 - Time 2024-02-22T02:02:05\n",
            "Episode 3280 - Step 505957 - Epsilon 0.1 - Mean Reward 675.02 - Mean Length 153.45 - Mean Loss 1.738 - Mean Q Value 47.363 - Time Delta 99.171 - Time 2024-02-22T02:03:44\n",
            "Episode 3300 - Step 508363 - Epsilon 0.1 - Mean Reward 635.2 - Mean Length 145.55 - Mean Loss 1.731 - Mean Q Value 47.355 - Time Delta 49.739 - Time 2024-02-22T02:04:34\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_51.chkpt at step 510000\n",
            "Episode 3320 - Step 510926 - Epsilon 0.1 - Mean Reward 646.97 - Mean Length 146.95 - Mean Loss 1.711 - Mean Q Value 47.382 - Time Delta 54.394 - Time 2024-02-22T02:05:28\n",
            "Episode 3340 - Step 513862 - Epsilon 0.1 - Mean Reward 647.53 - Mean Length 144.08 - Mean Loss 1.728 - Mean Q Value 47.21 - Time Delta 1029.082 - Time 2024-02-22T02:22:37\n",
            "Episode 3360 - Step 516308 - Epsilon 0.1 - Mean Reward 680.81 - Mean Length 151.57 - Mean Loss 1.719 - Mean Q Value 46.994 - Time Delta 1028.306 - Time 2024-02-22T02:39:45\n",
            "Episode 3380 - Step 519338 - Epsilon 0.1 - Mean Reward 684.48 - Mean Length 133.81 - Mean Loss 1.678 - Mean Q Value 46.863 - Time Delta 814.809 - Time 2024-02-22T02:53:20\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_52.chkpt at step 520000\n",
            "Episode 3400 - Step 522321 - Epsilon 0.1 - Mean Reward 708.81 - Mean Length 139.58 - Mean Loss 1.684 - Mean Q Value 46.879 - Time Delta 53.204 - Time 2024-02-22T02:54:13\n",
            "Episode 3420 - Step 525109 - Epsilon 0.1 - Mean Reward 720.59 - Mean Length 141.83 - Mean Loss 1.71 - Mean Q Value 46.956 - Time Delta 997.015 - Time 2024-02-22T03:10:50\n",
            "Episode 3440 - Step 528341 - Epsilon 0.1 - Mean Reward 727.18 - Mean Length 144.79 - Mean Loss 1.705 - Mean Q Value 47.01 - Time Delta 956.635 - Time 2024-02-22T03:26:47\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_53.chkpt at step 530000\n",
            "Episode 3460 - Step 531614 - Epsilon 0.1 - Mean Reward 737.31 - Mean Length 153.06 - Mean Loss 1.723 - Mean Q Value 47.226 - Time Delta 966.231 - Time 2024-02-22T03:42:53\n",
            "Episode 3480 - Step 534159 - Epsilon 0.1 - Mean Reward 728.19 - Mean Length 148.21 - Mean Loss 1.78 - Mean Q Value 47.286 - Time Delta 44.978 - Time 2024-02-22T03:43:38\n",
            "Episode 3500 - Step 536019 - Epsilon 0.1 - Mean Reward 677.28 - Mean Length 136.98 - Mean Loss 1.769 - Mean Q Value 47.415 - Time Delta 32.573 - Time 2024-02-22T03:44:11\n",
            "Episode 3520 - Step 538606 - Epsilon 0.1 - Mean Reward 677.75 - Mean Length 134.97 - Mean Loss 1.754 - Mean Q Value 47.389 - Time Delta 44.898 - Time 2024-02-22T03:44:56\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_54.chkpt at step 540000\n",
            "Episode 3540 - Step 541438 - Epsilon 0.1 - Mean Reward 663.06 - Mean Length 130.97 - Mean Loss 1.751 - Mean Q Value 47.56 - Time Delta 49.56 - Time 2024-02-22T03:45:45\n",
            "Episode 3560 - Step 544428 - Epsilon 0.1 - Mean Reward 674.35 - Mean Length 128.14 - Mean Loss 1.738 - Mean Q Value 47.498 - Time Delta 53.474 - Time 2024-02-22T03:46:39\n",
            "Episode 3580 - Step 547199 - Epsilon 0.1 - Mean Reward 671.99 - Mean Length 130.4 - Mean Loss 1.708 - Mean Q Value 47.692 - Time Delta 49.453 - Time 2024-02-22T03:47:28\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_55.chkpt at step 550000\n",
            "Episode 3600 - Step 550214 - Epsilon 0.1 - Mean Reward 721.89 - Mean Length 141.95 - Mean Loss 1.726 - Mean Q Value 47.857 - Time Delta 55.588 - Time 2024-02-22T03:48:24\n",
            "Episode 3620 - Step 553235 - Epsilon 0.1 - Mean Reward 735.74 - Mean Length 146.29 - Mean Loss 1.751 - Mean Q Value 48.066 - Time Delta 57.169 - Time 2024-02-22T03:49:21\n",
            "Episode 3640 - Step 556157 - Epsilon 0.1 - Mean Reward 747.19 - Mean Length 147.19 - Mean Loss 1.788 - Mean Q Value 48.195 - Time Delta 54.82 - Time 2024-02-22T03:50:16\n",
            "Episode 3660 - Step 558736 - Epsilon 0.1 - Mean Reward 730.87 - Mean Length 143.08 - Mean Loss 1.82 - Mean Q Value 48.534 - Time Delta 49.42 - Time 2024-02-22T03:51:05\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_56.chkpt at step 560000\n",
            "Episode 3680 - Step 561421 - Epsilon 0.1 - Mean Reward 730.42 - Mean Length 142.22 - Mean Loss 1.842 - Mean Q Value 48.624 - Time Delta 53.013 - Time 2024-02-22T03:51:58\n",
            "Episode 3700 - Step 563715 - Epsilon 0.1 - Mean Reward 703.56 - Mean Length 135.01 - Mean Loss 1.828 - Mean Q Value 48.936 - Time Delta 45.897 - Time 2024-02-22T03:52:44\n",
            "Episode 3720 - Step 566699 - Epsilon 0.1 - Mean Reward 694.57 - Mean Length 134.64 - Mean Loss 1.81 - Mean Q Value 49.051 - Time Delta 60.677 - Time 2024-02-22T03:53:45\n",
            "Episode 3740 - Step 568712 - Epsilon 0.1 - Mean Reward 644.03 - Mean Length 125.55 - Mean Loss 1.799 - Mean Q Value 49.119 - Time Delta 43.136 - Time 2024-02-22T03:54:28\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_57.chkpt at step 570000\n",
            "Episode 3760 - Step 572290 - Epsilon 0.1 - Mean Reward 661.07 - Mean Length 135.54 - Mean Loss 1.8 - Mean Q Value 49.122 - Time Delta 74.925 - Time 2024-02-22T03:55:43\n",
            "Episode 3780 - Step 575168 - Epsilon 0.1 - Mean Reward 673.72 - Mean Length 137.47 - Mean Loss 1.803 - Mean Q Value 49.334 - Time Delta 58.077 - Time 2024-02-22T03:56:41\n",
            "Episode 3800 - Step 577835 - Epsilon 0.1 - Mean Reward 681.6 - Mean Length 141.2 - Mean Loss 1.837 - Mean Q Value 49.365 - Time Delta 954.591 - Time 2024-02-22T04:12:36\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_58.chkpt at step 580000\n",
            "Episode 3820 - Step 580443 - Epsilon 0.1 - Mean Reward 665.13 - Mean Length 137.44 - Mean Loss 1.856 - Mean Q Value 49.493 - Time Delta 46.319 - Time 2024-02-22T04:13:22\n",
            "Episode 3840 - Step 583515 - Epsilon 0.1 - Mean Reward 721.3 - Mean Length 148.03 - Mean Loss 1.864 - Mean Q Value 49.589 - Time Delta 54.996 - Time 2024-02-22T04:14:17\n",
            "Episode 3860 - Step 586595 - Epsilon 0.1 - Mean Reward 729.18 - Mean Length 143.05 - Mean Loss 1.897 - Mean Q Value 49.721 - Time Delta 54.034 - Time 2024-02-22T04:15:11\n",
            "Episode 3880 - Step 589361 - Epsilon 0.1 - Mean Reward 722.18 - Mean Length 141.93 - Mean Loss 1.9 - Mean Q Value 49.676 - Time Delta 1050.711 - Time 2024-02-22T04:32:42\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_59.chkpt at step 590000\n",
            "Episode 3900 - Step 592601 - Epsilon 0.1 - Mean Reward 755.07 - Mean Length 147.66 - Mean Loss 1.91 - Mean Q Value 49.564 - Time Delta 56.729 - Time 2024-02-22T04:33:38\n",
            "Episode 3920 - Step 595741 - Epsilon 0.1 - Mean Reward 784.97 - Mean Length 152.98 - Mean Loss 1.919 - Mean Q Value 49.435 - Time Delta 55.449 - Time 2024-02-22T04:34:34\n",
            "Episode 3940 - Step 597837 - Epsilon 0.1 - Mean Reward 734.77 - Mean Length 143.22 - Mean Loss 1.934 - Mean Q Value 49.131 - Time Delta 1085.464 - Time 2024-02-22T04:52:39\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_60.chkpt at step 600000\n",
            "Episode 3960 - Step 600627 - Epsilon 0.1 - Mean Reward 723.96 - Mean Length 140.32 - Mean Loss 1.943 - Mean Q Value 48.938 - Time Delta 49.077 - Time 2024-02-22T04:53:28\n",
            "Episode 3980 - Step 602810 - Epsilon 0.1 - Mean Reward 695.97 - Mean Length 134.49 - Mean Loss 1.97 - Mean Q Value 48.871 - Time Delta 38.221 - Time 2024-02-22T04:54:07\n",
            "Episode 4000 - Step 606104 - Epsilon 0.1 - Mean Reward 699.73 - Mean Length 135.03 - Mean Loss 1.962 - Mean Q Value 49.018 - Time Delta 57.065 - Time 2024-02-22T04:55:04\n",
            "Episode 4020 - Step 608206 - Epsilon 0.1 - Mean Reward 650.73 - Mean Length 124.65 - Mean Loss 2.012 - Mean Q Value 49.212 - Time Delta 1016.288 - Time 2024-02-22T05:12:00\n",
            "Episode 4040 - Step 609853 - Epsilon 0.1 - Mean Reward 630.69 - Mean Length 120.16 - Mean Loss 2.0 - Mean Q Value 49.597 - Time Delta 28.868 - Time 2024-02-22T05:12:29\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_61.chkpt at step 610000\n",
            "Episode 4060 - Step 612071 - Epsilon 0.1 - Mean Reward 594.09 - Mean Length 114.44 - Mean Loss 1.99 - Mean Q Value 49.81 - Time Delta 39.402 - Time 2024-02-22T05:13:08\n",
            "Episode 4080 - Step 614572 - Epsilon 0.1 - Mean Reward 608.65 - Mean Length 117.62 - Mean Loss 1.997 - Mean Q Value 50.039 - Time Delta 43.644 - Time 2024-02-22T05:13:52\n",
            "Episode 4100 - Step 617085 - Epsilon 0.1 - Mean Reward 574.99 - Mean Length 109.81 - Mean Loss 2.02 - Mean Q Value 50.115 - Time Delta 43.652 - Time 2024-02-22T05:14:35\n",
            "Episode 4120 - Step 619576 - Epsilon 0.1 - Mean Reward 599.47 - Mean Length 113.7 - Mean Loss 1.991 - Mean Q Value 50.139 - Time Delta 1001.297 - Time 2024-02-22T05:31:17\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_62.chkpt at step 620000\n",
            "Episode 4140 - Step 622210 - Epsilon 0.1 - Mean Reward 649.86 - Mean Length 123.57 - Mean Loss 2.02 - Mean Q Value 50.199 - Time Delta 46.3 - Time 2024-02-22T05:32:03\n",
            "Episode 4160 - Step 624424 - Epsilon 0.1 - Mean Reward 662.75 - Mean Length 123.53 - Mean Loss 1.986 - Mean Q Value 50.404 - Time Delta 38.59 - Time 2024-02-22T05:32:42\n",
            "Episode 4180 - Step 626903 - Epsilon 0.1 - Mean Reward 675.3 - Mean Length 123.31 - Mean Loss 1.989 - Mean Q Value 50.651 - Time Delta 43.038 - Time 2024-02-22T05:33:25\n",
            "Episode 4200 - Step 629036 - Epsilon 0.1 - Mean Reward 651.95 - Mean Length 119.51 - Mean Loss 1.962 - Mean Q Value 50.913 - Time Delta 37.427 - Time 2024-02-22T05:34:02\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_63.chkpt at step 630000\n",
            "Episode 4220 - Step 631525 - Epsilon 0.1 - Mean Reward 640.79 - Mean Length 119.49 - Mean Loss 1.941 - Mean Q Value 51.19 - Time Delta 44.01 - Time 2024-02-22T05:34:46\n",
            "Episode 4240 - Step 633900 - Epsilon 0.1 - Mean Reward 613.7 - Mean Length 116.9 - Mean Loss 1.968 - Mean Q Value 51.427 - Time Delta 41.335 - Time 2024-02-22T05:35:27\n",
            "Episode 4260 - Step 635726 - Epsilon 0.1 - Mean Reward 585.86 - Mean Length 113.02 - Mean Loss 2.0 - Mean Q Value 51.389 - Time Delta 32.93 - Time 2024-02-22T05:36:00\n",
            "Episode 4280 - Step 637377 - Epsilon 0.1 - Mean Reward 535.23 - Mean Length 104.74 - Mean Loss 2.032 - Mean Q Value 51.565 - Time Delta 30.776 - Time 2024-02-22T05:36:31\n",
            "Episode 4300 - Step 639002 - Epsilon 0.1 - Mean Reward 510.29 - Mean Length 99.66 - Mean Loss 2.045 - Mean Q Value 51.559 - Time Delta 994.326 - Time 2024-02-22T05:53:06\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_64.chkpt at step 640000\n",
            "Episode 4320 - Step 640514 - Epsilon 0.1 - Mean Reward 472.26 - Mean Length 89.89 - Mean Loss 2.069 - Mean Q Value 51.586 - Time Delta 27.245 - Time 2024-02-22T05:53:33\n",
            "Episode 4340 - Step 642420 - Epsilon 0.1 - Mean Reward 456.68 - Mean Length 85.2 - Mean Loss 2.059 - Mean Q Value 51.58 - Time Delta 33.392 - Time 2024-02-22T05:54:06\n",
            "Episode 4360 - Step 644927 - Epsilon 0.1 - Mean Reward 487.62 - Mean Length 92.01 - Mean Loss 2.067 - Mean Q Value 51.574 - Time Delta 44.072 - Time 2024-02-22T05:54:50\n",
            "Episode 4380 - Step 647500 - Epsilon 0.1 - Mean Reward 526.86 - Mean Length 101.23 - Mean Loss 2.009 - Mean Q Value 51.41 - Time Delta 44.737 - Time 2024-02-22T05:55:35\n",
            "Episode 4400 - Step 649976 - Epsilon 0.1 - Mean Reward 568.61 - Mean Length 109.74 - Mean Loss 2.015 - Mean Q Value 51.271 - Time Delta 1068.904 - Time 2024-02-22T06:13:24\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_65.chkpt at step 650000\n",
            "Episode 4420 - Step 652771 - Epsilon 0.1 - Mean Reward 628.51 - Mean Length 122.57 - Mean Loss 1.993 - Mean Q Value 51.174 - Time Delta 49.466 - Time 2024-02-22T06:14:13\n",
            "Episode 4440 - Step 655281 - Epsilon 0.1 - Mean Reward 665.0 - Mean Length 128.61 - Mean Loss 1.969 - Mean Q Value 51.22 - Time Delta 43.502 - Time 2024-02-22T06:14:57\n",
            "Episode 4460 - Step 657548 - Epsilon 0.1 - Mean Reward 655.33 - Mean Length 126.21 - Mean Loss 2.009 - Mean Q Value 51.405 - Time Delta 39.515 - Time 2024-02-22T06:15:36\n",
            "Episode 4480 - Step 659530 - Epsilon 0.1 - Mean Reward 636.21 - Mean Length 120.3 - Mean Loss 2.032 - Mean Q Value 51.47 - Time Delta 34.74 - Time 2024-02-22T06:16:11\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_66.chkpt at step 660000\n",
            "Episode 4500 - Step 661974 - Epsilon 0.1 - Mean Reward 628.03 - Mean Length 119.98 - Mean Loss 2.063 - Mean Q Value 51.627 - Time Delta 43.572 - Time 2024-02-22T06:16:55\n",
            "Episode 4520 - Step 664269 - Epsilon 0.1 - Mean Reward 594.34 - Mean Length 114.98 - Mean Loss 2.092 - Mean Q Value 51.834 - Time Delta 40.429 - Time 2024-02-22T06:17:35\n",
            "Episode 4540 - Step 666356 - Epsilon 0.1 - Mean Reward 571.84 - Mean Length 110.75 - Mean Loss 2.108 - Mean Q Value 51.973 - Time Delta 37.415 - Time 2024-02-22T06:18:13\n",
            "Episode 4560 - Step 668250 - Epsilon 0.1 - Mean Reward 550.85 - Mean Length 107.02 - Mean Loss 2.088 - Mean Q Value 52.212 - Time Delta 34.97 - Time 2024-02-22T06:18:47\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_67.chkpt at step 670000\n",
            "Episode 4580 - Step 670528 - Epsilon 0.1 - Mean Reward 557.82 - Mean Length 109.98 - Mean Loss 2.088 - Mean Q Value 52.435 - Time Delta 951.696 - Time 2024-02-22T06:34:39\n",
            "Episode 4600 - Step 672470 - Epsilon 0.1 - Mean Reward 542.26 - Mean Length 104.96 - Mean Loss 2.087 - Mean Q Value 52.667 - Time Delta 34.346 - Time 2024-02-22T06:35:14\n",
            "Episode 4620 - Step 674716 - Epsilon 0.1 - Mean Reward 545.89 - Mean Length 104.47 - Mean Loss 2.122 - Mean Q Value 52.78 - Time Delta 39.129 - Time 2024-02-22T06:35:53\n",
            "Episode 4640 - Step 676991 - Epsilon 0.1 - Mean Reward 552.56 - Mean Length 106.35 - Mean Loss 2.1 - Mean Q Value 53.012 - Time Delta 39.762 - Time 2024-02-22T06:36:32\n",
            "Episode 4660 - Step 679602 - Epsilon 0.1 - Mean Reward 601.97 - Mean Length 113.52 - Mean Loss 2.1 - Mean Q Value 53.023 - Time Delta 472.14 - Time 2024-02-22T06:44:25\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_68.chkpt at step 680000\n",
            "Episode 4680 - Step 682349 - Epsilon 0.1 - Mean Reward 634.69 - Mean Length 118.21 - Mean Loss 2.136 - Mean Q Value 53.127 - Time Delta 48.37 - Time 2024-02-22T06:45:13\n",
            "Episode 4700 - Step 685033 - Epsilon 0.1 - Mean Reward 672.03 - Mean Length 125.63 - Mean Loss 2.143 - Mean Q Value 53.274 - Time Delta 47.305 - Time 2024-02-22T06:46:00\n",
            "Episode 4720 - Step 687579 - Epsilon 0.1 - Mean Reward 696.46 - Mean Length 128.63 - Mean Loss 2.128 - Mean Q Value 53.366 - Time Delta 44.281 - Time 2024-02-22T06:46:45\n",
            "Episode 4740 - Step 689985 - Epsilon 0.1 - Mean Reward 703.81 - Mean Length 129.94 - Mean Loss 2.137 - Mean Q Value 53.296 - Time Delta 42.252 - Time 2024-02-22T06:47:27\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_69.chkpt at step 690000\n",
            "Episode 4760 - Step 692666 - Epsilon 0.1 - Mean Reward 699.29 - Mean Length 130.64 - Mean Loss 2.108 - Mean Q Value 53.383 - Time Delta 47.063 - Time 2024-02-22T06:48:14\n",
            "Episode 4780 - Step 694530 - Epsilon 0.1 - Mean Reward 651.83 - Mean Length 121.81 - Mean Loss 2.096 - Mean Q Value 53.337 - Time Delta 34.02 - Time 2024-02-22T06:48:48\n",
            "Episode 4800 - Step 697208 - Epsilon 0.1 - Mean Reward 654.3 - Mean Length 121.75 - Mean Loss 2.099 - Mean Q Value 53.397 - Time Delta 48.678 - Time 2024-02-22T06:49:37\n",
            "Episode 4820 - Step 698999 - Epsilon 0.1 - Mean Reward 611.9 - Mean Length 114.2 - Mean Loss 2.082 - Mean Q Value 53.537 - Time Delta 34.119 - Time 2024-02-22T06:50:11\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_70.chkpt at step 700000\n",
            "Episode 4840 - Step 701100 - Epsilon 0.1 - Mean Reward 599.3 - Mean Length 111.15 - Mean Loss 2.124 - Mean Q Value 53.709 - Time Delta 982.597 - Time 2024-02-22T07:06:33\n",
            "Episode 4860 - Step 702857 - Epsilon 0.1 - Mean Reward 556.59 - Mean Length 101.91 - Mean Loss 2.117 - Mean Q Value 53.783 - Time Delta 31.179 - Time 2024-02-22T07:07:04\n",
            "Episode 4880 - Step 704999 - Epsilon 0.1 - Mean Reward 570.21 - Mean Length 104.69 - Mean Loss 2.137 - Mean Q Value 53.822 - Time Delta 37.91 - Time 2024-02-22T07:07:42\n",
            "Episode 4900 - Step 706612 - Epsilon 0.1 - Mean Reward 513.97 - Mean Length 94.04 - Mean Loss 2.128 - Mean Q Value 53.789 - Time Delta 28.211 - Time 2024-02-22T07:08:11\n",
            "Episode 4920 - Step 707959 - Epsilon 0.1 - Mean Reward 491.54 - Mean Length 89.6 - Mean Loss 2.147 - Mean Q Value 53.752 - Time Delta 23.649 - Time 2024-02-22T07:08:34\n",
            "MarioNet saved to checkpoints/2024-02-21T22-45-07/mario_net_71.chkpt at step 710000\n",
            "Episode 4940 - Step 710100 - Epsilon 0.1 - Mean Reward 496.93 - Mean Length 90.0 - Mean Loss 2.123 - Mean Q Value 53.525 - Time Delta 37.966 - Time 2024-02-22T07:09:12\n",
            "Episode 4960 - Step 712222 - Epsilon 0.1 - Mean Reward 507.68 - Mean Length 93.65 - Mean Loss 2.182 - Mean Q Value 53.349 - Time Delta 37.698 - Time 2024-02-22T07:09:50\n",
            "Episode 4980 - Step 714151 - Epsilon 0.1 - Mean Reward 497.91 - Mean Length 91.52 - Mean Loss 2.122 - Mean Q Value 53.233 - Time Delta 35.422 - Time 2024-02-22T07:10:25\n",
            "Episode 4999 - Step 715802 - Epsilon 0.1 - Mean Reward 501.52 - Mean Length 92.3 - Mean Loss 2.128 - Mean Q Value 53.229 - Time Delta 31.177 - Time 2024-02-22T07:10:56\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "print(f\"Using CUDA: {use_cuda}\")\n",
        "print()\n",
        "\n",
        "save_dir = Path(\"checkpoints\") / datetime.datetime.now().strftime(\"%Y-%m-%dT%H-%M-%S\")\n",
        "save_dir.mkdir(parents=True)\n",
        "\n",
        "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
        "\n",
        "logger = MetricLogger(save_dir)\n",
        "\n",
        "episodes = 5_000\n",
        "for e in range(episodes):\n",
        "\n",
        "    state = env.reset()\n",
        "\n",
        "    # Play the game!\n",
        "    while True:\n",
        "\n",
        "        # Run agent on the state\n",
        "        action = mario.act(state)\n",
        "\n",
        "        # Agent performs action\n",
        "        next_state, reward, done, trunc, info = env.step(action)\n",
        "\n",
        "        # Remember\n",
        "        mario.cache(state, next_state, action, reward, done)\n",
        "\n",
        "        # Learn\n",
        "        q, loss = mario.learn()\n",
        "\n",
        "        # Logging\n",
        "        logger.log_step(reward, loss, q)\n",
        "\n",
        "        # Update state\n",
        "        state = next_state\n",
        "\n",
        "        # Check if end of game\n",
        "        if done or info[\"flag_get\"]:\n",
        "            break\n",
        "\n",
        "    logger.log_episode()\n",
        "    mario.exploration_rate = mario.decay(1.0, mario.exploration_rate_decay, e, episodes, mario.exploration_rate_min)\n",
        "    if (e % 20 == 0) or (e == episodes - 1):\n",
        "        logger.record(episode=e, epsilon=mario.exploration_rate, step=mario.curr_step)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we saw how we can use PyTorch to train a game-playing AI. You can use the same methods\n",
        "to train an AI to play any of the games at the [OpenAI gym](https://gym.openai.com/)_. Hope you enjoyed this tutorial, feel free to reach us at\n",
        "[our github](https://github.com/yuansongFeng/MadMario/)_!\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
