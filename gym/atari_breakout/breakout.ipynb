{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://learn.microsoft.com/en-us/windows/ai/directml/gpu-tensorflow-plugin\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Add, Conv2D, Flatten, BatchNormalization, Dropout, Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "# import torch\n",
    "# import tensorflow_probability as tfp\n",
    "import gymnasium as gym\n",
    "from collections import deque\n",
    "import random\n",
    "# from skimage.color import rgb2gray\n",
    "# from skimage.transform import resize\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "# import threading\n",
    "# from queue import Queue\n",
    "# import heapq\n",
    "import cv2\n",
    "# import math\n",
    "import os\n",
    "from send_email import send_email\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "load_dotenv()\n",
    "\n",
    "# https://chat.openai.com/c/2b12934e-de73-4813-a047-8db8e8aa87a7\n",
    "# Hyperparameters\n",
    "NUM_ACTIONS = 3\n",
    "ACTIONS = [0, 2, 3]\n",
    "GAMMA = 0.99\n",
    "ALPHA = 0.60 # alpha = 0 -> uniform; alpha = 1 -> purely based on priority\n",
    "BETA = 0.40 # beta = 0 -> no correction; beta = 1 -> weights fully correct bias\n",
    "BUFFER_SIZE = 300_000\n",
    "MIN_BUFFER_SIZE = 150_000\n",
    "START_PRIORITY_SAMPLING = 300\n",
    "RESET_PRIORITIES = 500\n",
    "NUM_FRAMES = 4\n",
    "FRAME_WIDTH = 84\n",
    "FRAME_HEIGHT = 64\n",
    "N_STEPS = 3\n",
    "BATCH_SIZE = 512\n",
    "TRAINING_FREQ = 32\n",
    "SHOW_FRAME = 200\n",
    "TARGET_UPDATE_FREQ = 2_500\n",
    "TARGET_UPDATE_RATE = 150\n",
    "MAX_TARGET_UPDATE_FREQ = 10_000\n",
    "ADJUST_TARGET_UPDATE_FREQ_EVERY = 50_000\n",
    "INITIAL_LEARNING_RATE = 0.002 #0.00025 #0.002\n",
    "LEARNING_RATE_DECAY = 0.05   #0.05 for 5_000, 0.005 for 20_000\n",
    "MIN_LEARNING_RATE = 0.0001\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.04          #0.04 for 5_000, 0.01 for 20_000\n",
    "EPSILON_MIN = 0.10\n",
    "SEED=42\n",
    "\n",
    "NUM_EPISODES = 10_000\n",
    "SAVE_FREQ = 100\n",
    "EMAIL_FREQUENCY = 1000\n",
    "# RUN = \"per_n_steps_47_5_000_n\"\n",
    "RUN = \"per_59_10_000_control_he_init_bn\"\n",
    "SAVE_PATH = f\"G:/Coding/breakout/testing_prioritized/{RUN}/\"\n",
    "\n",
    "# GAME = \"Pong-v4\"\n",
    "GAME = \"Breakout-v4\"\n",
    "# GAME = \"BreakoutDeterministic-v4\"\n",
    "# GAME = \"BreakoutNoFrameskip-v4\"\n",
    "# GAME = \"ALE/Breakout-v5\"\n",
    "\n",
    "\n",
    "RENDER = False\n",
    "PRETRAINED = False\n",
    "TRAINING = True\n",
    "MODEL = \"\"\n",
    "LOGGING = True\n",
    "\n",
    "DOUBLE_DQN                    = True\n",
    "PRIORITIZED_EXPERIENCE_REPLAY = False\n",
    "N_STEPS_IMPLEMENTED           = False\n",
    "DUELING_DQN                   = False\n",
    "DISTIBUTIONAL_RL              = False\n",
    "NOISY_NETS                    = False\n",
    "\n",
    "\n",
    "\n",
    "# Declare a DQN network that can learn to play atari breakout using the rainbow algorithm\n",
    "# https://arxiv.org/pdf/1710.02298.pdf\n",
    "#\n",
    "\n",
    "def decay(initial_epsilon, decay_rate, episode, num_episodes, minimum):\n",
    "    return  max((initial_epsilon * (decay_rate ** (episode/num_episodes))), minimum)\n",
    "\n",
    "# def decay(initial, episode, decay_rate):\n",
    "#     return initial * (decay_rate ** (episode/NUM_EPISODES))\n",
    "def save_ep_replay_to_file(ep_replay, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(ep_replay, f)\n",
    "def estimate_score(total_episodes, scores):\n",
    "    x = np.arange(len(scores))\n",
    "    coefficients = np.polyfit(x, scores, 1)\n",
    "    fitted_line = np.poly1d(coefficients)\n",
    "    remaining_episodes = np.arange(len(scores), total_episodes)\n",
    "    predicted_scores = fitted_line(remaining_episodes)\n",
    "    return round(np.mean(predicted_scores[-100:]),1)\n",
    "\n",
    "def estimate_remaining_time(total_episodes, times):\n",
    "    x = np.arange(len(times))\n",
    "    coefficients = np.polyfit(x, times, 1)\n",
    "    fitted_line = np.poly1d(coefficients)\n",
    "    remaining_episodes = np.arange(len(times), total_episodes)\n",
    "    predicted_times = fitted_line(remaining_episodes)\n",
    "    remaining_time = np.sum(predicted_times)\n",
    "    return remaining_time\n",
    "\n",
    "def read_file():\n",
    "    try:\n",
    "        with open(\"C:/Users/Mar/OneDrive/Desktop/Development/Playground/gym/atari_breakout/breakout.ipynb\", \"r\") as f:\n",
    "            return f.read()\n",
    "    except:\n",
    "        return \"\"\n",
    "def write_to_file(output_str, file_name=f\"{RUN}.txt\", type = \"a\"):\n",
    "    if TRAINING:\n",
    "        try:\n",
    "            with open(f\"{SAVE_PATH}{file_name}\", type) as f:\n",
    "                f.write(output_str)\n",
    "        except:\n",
    "            print(\"error writing to file\")\n",
    "            pass\n",
    "        \n",
    "def send_email_notification(all_rewards, output_str):\n",
    "    try:\n",
    "        def get_averages(rewards):\n",
    "          averages = []\n",
    "          chunk_size = 100\n",
    "          for i in range(0, len(rewards), chunk_size):\n",
    "              chunk = rewards[i:i + chunk_size]\n",
    "              average = sum(chunk) / len(chunk)\n",
    "              averages.append(average)\n",
    "          return averages\n",
    "\n",
    "        average_per_100 = get_averages(all_rewards)\n",
    "\n",
    "        email_subject = SAVE_PATH\n",
    "        email_body = f\"\"\"{output_str}\\n\n",
    "Average per 100: {average_per_100}\\n\"\"\"\n",
    "        write_to_file(email_body)\n",
    "        send_email(email_subject, f\"{email_body}\\nAll Rewards: {all_rewards}\")\n",
    "        if LOGGING:\n",
    "            print(\"\\n\")\n",
    "    except Exception as e:\n",
    "        print(\"email error\", e)\n",
    "        pass\n",
    "    \n",
    "# Prioritized experience replay buffer\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, buffer_size, alpha):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.priorities = deque(maxlen=buffer_size)\n",
    "        self.alpha = alpha\n",
    "        self.max_priority = 1\n",
    "        self.sample_num = 1\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "        self.priorities.append(self.max_priority)\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        probs = np.array(self.priorities) ** self.alpha\n",
    "        # probs = np.array([-priority for priority in self.priorities]) ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probs, replace=False) # Won't select the same index more than once\n",
    "        # indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
    "        samples = [self.memory[i] for i in indices]\n",
    "\n",
    "        with open(f\"{SAVE_PATH}/indices/indices{self.sample_num}.txt\", \"a\") as f:\n",
    "            f.write(f\"{','.join(map(str, np.sort(np.array(indices))))}\\n\")\n",
    "        self.sample_num += 1\n",
    "        # weights = (self.priorities ** (-beta)) / np.max(self.priorities) # Proportional prioritization\n",
    "        weights = (len(self.memory) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        # print(weights)\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "        \n",
    "        return samples, indices, weights\n",
    "        # return samples, indices, weights\n",
    "\n",
    "    def update_priorities(self, indices, priorities):\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            self.priorities[idx] = priority\n",
    "        self.max_priority = max(self.max_priority, max(priorities))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        # Convolutional layers with He initialization and separate activation layers\n",
    "        self.conv1 = Conv2D(32, 8, strides=4, kernel_initializer=HeNormal())\n",
    "        self.bn1 = BatchNormalization()  # Added batch normalization layer\n",
    "        self.act1 = Activation('relu')\n",
    "        self.conv2 = Conv2D(64, 4, strides=2, kernel_initializer=HeNormal())\n",
    "        self.bn2 = BatchNormalization()  # Added batch normalization layer\n",
    "        self.act2 = Activation('relu')\n",
    "        self.conv3 = Conv2D(64, 3, strides=1, kernel_initializer=HeNormal())\n",
    "        self.bn3 = BatchNormalization()  # Added batch normalization layer\n",
    "        self.act3 = Activation('relu')\n",
    "\n",
    "        # Flattening layer\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "        # Dueling DQN Layers\n",
    "        if DUELING_DQN:\n",
    "            self.dense1_adv = Dense(512, activation='relu', kernel_initializer=HeNormal())\n",
    "            self.dense2_adv = Dense(NUM_ACTIONS, kernel_initializer=HeNormal())\n",
    "            self.dense1_val = Dense(512, activation='relu', kernel_initializer=HeNormal())\n",
    "            self.dense2_val = Dense(1, kernel_initializer=HeNormal())\n",
    "        # Standard DQN Layers\n",
    "        else:\n",
    "            self.dense1 = Dense(512, activation='relu', kernel_initializer=HeNormal())\n",
    "            self.dense2 = Dense(NUM_ACTIONS, kernel_initializer=HeNormal())\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x)  # Apply batch normalization\n",
    "        x = self.act1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)  # Apply batch normalization\n",
    "        x = self.act2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)  # Apply batch normalization\n",
    "        x = self.act3(x)\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        # Dueling DQN logic\n",
    "        if DUELING_DQN:\n",
    "            adv = self.dense1_adv(x)\n",
    "            adv = self.dense2_adv(adv)\n",
    "\n",
    "            val = self.dense1_val(x)\n",
    "            val = self.dense2_val(val)\n",
    "\n",
    "            # Combine advantage and value streams\n",
    "            q_values = val + (adv - tf.reduce_mean(adv, axis=1, keepdims=True))\n",
    "        # Standard DQN logic\n",
    "        else:\n",
    "            x = self.dense1(x)\n",
    "            q_values = self.dense2(x)\n",
    "        return q_values\n",
    "\n",
    "class RainbowAgent:\n",
    "    def __init__(self):\n",
    "        self.frame_buffer = deque(maxlen=NUM_FRAMES)\n",
    "        self.num_actions = NUM_ACTIONS\n",
    "        self.epsilon = EPSILON_START\n",
    "        self.target_update_freq = TARGET_UPDATE_FREQ\n",
    "        if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "            self.buffer = PrioritizedReplayBuffer(BUFFER_SIZE, ALPHA)\n",
    "            self.beta = BETA\n",
    "            self.beta_increment = (1.0 - BETA) / NUM_EPISODES\n",
    "        else:\n",
    "            self.buffer = deque(maxlen=BUFFER_SIZE)\n",
    "        if N_STEPS_IMPLEMENTED:\n",
    "            self.n_step_buffer = deque(maxlen=N_STEPS)\n",
    "            # self.n_step_buffer = ReversedDeque(maxlen=N_STEPS)\n",
    "        self.fig_frame = None\n",
    "        self.q_network = DQN()\n",
    "        self.target_network = DQN()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=INITIAL_LEARNING_RATE)\n",
    "        self.loss_function = tf.keras.losses.MeanSquaredError()\n",
    "        self.q_network.compile(optimizer=self.optimizer, loss=self.loss_function)\n",
    "        self.target_network.compile(optimizer=self.optimizer, loss=self.loss_function)\n",
    "        if RENDER:\n",
    "            self.env = gym.make(GAME, render_mode=\"human\")\n",
    "        else:\n",
    "            self.env = gym.make(GAME)\n",
    "        self.env.seed(SEED)\n",
    "        if PRETRAINED:\n",
    "            if LOGGING:\n",
    "                print(f\"..................Pretrained model..................\\nMODEL: {MODEL}\")\n",
    "            dummy_input = np.zeros((1, FRAME_WIDTH, FRAME_HEIGHT, NUM_FRAMES))\n",
    "            self.q_network(dummy_input)\n",
    "            self.q_network.load_weights(MODEL)\n",
    "            self.target_network(dummy_input)\n",
    "\n",
    "        human_readable_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        if LOGGING:\n",
    "            print(f\"Starting training at {human_readable_time}\")\n",
    "        write_to_file(f\"Starting training at {human_readable_time}\\n\\n\")\n",
    "\n",
    "        model=f\"\"\"Rainbow Algorithm: https://arxiv.org/pdf/1710.02298.pdf\n",
    "  - Double DQN                        - {DOUBLE_DQN}\n",
    "  - Prioritized Experience Replay     - {PRIORITIZED_EXPERIENCE_REPLAY}\n",
    "  - Multi-step returns                - {N_STEPS_IMPLEMENTED}\n",
    "  - Dueling DQN                       - {DUELING_DQN}\n",
    "  - Noisy Nets                        - {NOISY_NETS}\n",
    "  - Distributional RL                 - {DISTIBUTIONAL_RL}\\n\\n\"\"\"\n",
    "        write_to_file(model)\n",
    "        if LOGGING:\n",
    "            print(model)\n",
    "        \n",
    "        params = f\"\"\"Hyperparameters:\n",
    "NUM_ACTIONS = {NUM_ACTIONS}\n",
    "ACTIONS = {ACTIONS}\n",
    "GAMMA = {GAMMA}\n",
    "ALPHA = {ALPHA}\n",
    "BETA = {BETA}\n",
    "BUFFER_SIZE = {BUFFER_SIZE}\n",
    "N_STEPS = {N_STEPS}\n",
    "BATCH_SIZE = {BATCH_SIZE}\n",
    "TARGET_UPDATE_FREQ = {TARGET_UPDATE_FREQ}\n",
    "TARGET_UPDATE_RATE = {TARGET_UPDATE_RATE}\n",
    "MAX_TARGET_UPDATE_FREQ = {MAX_TARGET_UPDATE_FREQ}\n",
    "ADJUST_TARGET_UPDATE_FREQ_EVERY = {ADJUST_TARGET_UPDATE_FREQ_EVERY}\n",
    "TRAINING_FREQ = {TRAINING_FREQ}\n",
    "SHOW_FRAME = {SHOW_FRAME}\n",
    "FRAME_WIDTH = {FRAME_WIDTH}\n",
    "FRAME_HEIGHT = {FRAME_HEIGHT}\n",
    "NUM_FRAMES = {NUM_FRAMES}\n",
    "INITIAL_LEARNING_RATE = {INITIAL_LEARNING_RATE}\n",
    "LEARNING_RATE_DECAY = {LEARNING_RATE_DECAY}\n",
    "MIN_LEARNING_RATE = {MIN_LEARNING_RATE}\n",
    "SEED = {SEED}\n",
    "GAME = {GAME}\n",
    "EPSILON_START = {EPSILON_START}\n",
    "EPSILON_DECAY = {EPSILON_DECAY}\n",
    "NUM_EPISODES = {NUM_EPISODES}\n",
    "SAVE_FREQ = {SAVE_FREQ}\n",
    "SAVE_PATH = {SAVE_PATH}\n",
    "RENDER = {RENDER}\n",
    "PRETRAINED = {PRETRAINED}\n",
    "TRAINING = {TRAINING}\n",
    "MODEL = {MODEL}\\n\\n\"\"\"\n",
    "        if LOGGING:\n",
    "            print(params)\n",
    "        write_to_file(f\"{read_file()}\", \"code.ipynb\", type=\"w\")\n",
    "        self.update_target_network()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        if TRAINING:\n",
    "            if LOGGING:\n",
    "                print(\"updating target network\")\n",
    "            write_to_file(\"updating target network\\n\")\n",
    "            self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        if N_STEPS_IMPLEMENTED:\n",
    "            self.n_step_buffer.append((state, action, reward, next_state, done))\n",
    "            if len(self.n_step_buffer) < N_STEPS:\n",
    "                return\n",
    "            state, action, n_step_reward, n_step_state, n_step_done = self.calculate_n_step_info()\n",
    "            # state, action, _, _, _ = self.n_step_buffer[0]\n",
    "            if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "                self.buffer.add(state, action, n_step_reward, n_step_state, n_step_done)\n",
    "            else:\n",
    "                self.buffer.append((state, action, n_step_reward, n_step_state, n_step_done))\n",
    "        else:\n",
    "            if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "                self.buffer.add(state, action, reward, next_state, done)\n",
    "            else:\n",
    "                self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def calculate_n_step_info(self):\n",
    "        n_step_reward = 0\n",
    "        n_step_state = self.n_step_buffer[-1][-2]  # default to the last state in the buffer\n",
    "        n_step_done = self.n_step_buffer[-1][-1]  # default to the last done flag in the buffer\n",
    "\n",
    "        for idx, (_, _, reward, next, done) in enumerate(self.n_step_buffer):\n",
    "            n_step_reward += (GAMMA ** idx) * reward\n",
    "            if done:\n",
    "                n_step_done = True # if a terminal state is encountered, update the n_step_done flag\n",
    "                n_step_state = next # set the state to the last state before the terminal state\n",
    "                break\n",
    "        return self.n_step_buffer[0][0], self.n_step_buffer[0][1], n_step_reward, n_step_state, n_step_done\n",
    "    \n",
    "    # Use tf.function to speed up the computation graph\n",
    "    @tf.function\n",
    "    def compute_loss_and_gradients(self, states, actions, rewards, next_states, dones, weights=None):\n",
    "        # print(\"computing loss and gradients\")\n",
    "        # print(type(states))\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            q_values = tf.reduce_sum(q_values * tf.one_hot(actions, self.num_actions), axis=1)\n",
    "\n",
    "            next_q_values = self.target_network(next_states)\n",
    "            next_actions = tf.argmax(self.q_network(next_states), axis=1)\n",
    "\n",
    "            next_q_values = tf.reduce_sum(next_q_values * tf.one_hot(next_actions, self.num_actions), axis=1)\n",
    "\n",
    "            if N_STEPS_IMPLEMENTED:\n",
    "                target_q_values = rewards + (GAMMA ** N_STEPS) * (1 - dones) * next_q_values\n",
    "            else:\n",
    "                target_q_values = rewards + GAMMA * (1 - dones) * next_q_values\n",
    "            target_q_values = tf.stop_gradient(target_q_values)\n",
    "            td_errors = target_q_values - q_values\n",
    "            if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "                loss = tf.keras.losses.MSE(target_q_values, q_values)\n",
    "                loss = tf.reduce_mean(weights * loss)\n",
    "            else:\n",
    "                loss = tf.keras.losses.MSE(target_q_values, q_values)\n",
    "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        return loss, gradients, td_errors\n",
    "\n",
    "    def learn(self, episode, steps, last10rewards):\n",
    "        if len(self.buffer) < BATCH_SIZE:\n",
    "            return 0, 0\n",
    "        start = time.time()\n",
    "        if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "            minibatch, indices, weights = self.buffer.sample(BATCH_SIZE, self.beta)\n",
    "        else:\n",
    "            minibatch = random.sample(self.buffer, BATCH_SIZE)\n",
    "        states, actions, rewards, next_states, dones = zip(*minibatch)\n",
    "        # states, actions, rewards, next_states, dones = map(tf.stack, zip(*minibatch))\n",
    "\n",
    "        states      = np.array(states,      dtype=np.float32)\n",
    "        actions     = np.array(actions,     dtype=np.int32)\n",
    "        rewards     = np.array(rewards,     dtype=np.float32)\n",
    "        next_states = np.array(next_states, dtype=np.float32)\n",
    "        dones       = np.array(dones,       dtype=np.float32)\n",
    "\n",
    "        if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "            loss, gradients, td_errors = self.compute_loss_and_gradients(states, actions, rewards, next_states, dones, weights)\n",
    "            priorities = tf.abs(td_errors) + 1e-6\n",
    "            if episode >= START_PRIORITY_SAMPLING:\n",
    "                self.buffer.update_priorities(indices, priorities.numpy())\n",
    "        else:\n",
    "            loss, gradients, td_errors = self.compute_loss_and_gradients(states, actions, rewards, next_states, dones, None)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables))\n",
    "        end = time.time()\n",
    "        log_start = time.time()\n",
    "        if steps > 10_000 and steps % (TRAINING_FREQ*100):\n",
    "            with file_writer.as_default():\n",
    "                tf.summary.scalar('Loss', loss.numpy(), step=steps)\n",
    "                tf.summary.scalar('Learning Duration', end - start, step=steps)\n",
    "                tf.summary.scalar('Epsilon', self.epsilon, step=steps)\n",
    "                tf.summary.scalar('Learning Rate', self.optimizer.learning_rate, step=steps)\n",
    "                tf.summary.scalar('Avg last 10 rewards', last10rewards, step=steps)\n",
    "                file_writer.flush()\n",
    "        return round(end - start, 2), loss.numpy()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon and TRAINING:\n",
    "            return np.random.choice(ACTIONS)\n",
    "        else:\n",
    "            q_values = self.q_network(np.array([state], dtype=np.float32))\n",
    "            action = ACTIONS[np.argmax(q_values.numpy())]\n",
    "        return action\n",
    "    \n",
    "    def show_frame(self, state):\n",
    "        if self.fig_frame is None or not plt.fignum_exists(self.fig_frame.number):\n",
    "            self.fig_frame = plt.figure(SAVE_PATH.split('/')[-2]) \n",
    "            self.ax_frame = self.fig_frame.add_subplot(111)\n",
    "        self.ax_frame.clear()\n",
    "        # print(state.shape)\n",
    "        self.ax_frame.imshow(state, cmap='gray')\n",
    "        self.fig_frame.canvas.draw()\n",
    "        plt.pause(0.01)\n",
    "    \n",
    "    def preprocess_state(self, state):\n",
    "        start = time.time()\n",
    "        try:\n",
    "            state.shape\n",
    "        except:\n",
    "            state = state[0]\n",
    "            pass\n",
    "        # Custom weights for R, G, B\n",
    "        # processed_observe = 0.8 * state[:,:,2] + 0.1 * state[:,:,1] + 0.1 * state[:,:,0]\n",
    "        processed_observe = cv2.cvtColor(state[17:,:], cv2.COLOR_RGB2GRAY)\n",
    "        _, processed_observe = cv2.threshold(processed_observe, 0, 128, cv2.THRESH_BINARY)\n",
    "        processed_observe = cv2.resize(processed_observe, (FRAME_WIDTH, FRAME_HEIGHT), interpolation=cv2.INTER_NEAREST)\n",
    "        return processed_observe, time.time() - start\n",
    "    \n",
    "    def get_frames(self, frames = MIN_BUFFER_SIZE):\n",
    "        state = self.env.reset()\n",
    "        self.env.step(1)\n",
    "        steps = 0\n",
    "        lives = 5\n",
    "        for _ in range(NUM_FRAMES):\n",
    "            processed_state, proc_time = self.preprocess_state(state)\n",
    "            self.frame_buffer.append(processed_state)\n",
    "        state = np.stack(self.frame_buffer, axis=-1)\n",
    "\n",
    "        pbar = tqdm(total=frames)\n",
    "        while steps < frames:\n",
    "            # action = np.random.choice(ACTIONS)\n",
    "            action = self.choose_action(state)\n",
    "            next_state, reward, done, _, info = self.env.step(action)\n",
    "            if reward > 0:\n",
    "                reward = 1\n",
    "            processed_state, proc_time = self.preprocess_state(next_state)\n",
    "            self.frame_buffer.append(processed_state)\n",
    "            next_state = np.stack(self.frame_buffer, axis=-1)\n",
    "\n",
    "            if action != 0:\n",
    "                action -= 1\n",
    "            if (info[\"lives\"] < lives):\n",
    "                lives = info[\"lives\"]\n",
    "                # reward = -1\n",
    "                self.env.step(1)\n",
    "                self.remember(state, action, reward, next_state, True)\n",
    "            else:\n",
    "                self.remember(state, action, reward, next_state, done)\n",
    "            if(lives ==  0):\n",
    "                state = self.env.reset()\n",
    "                lives = 5\n",
    "                self.env.step(1)\n",
    "\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            pbar.update(1)\n",
    "        pbar.close()\n",
    "        write_to_file(f\"Finished getting {steps} frames\\n\")\n",
    "        if LOGGING:\n",
    "            print(f\"Finished getting {steps} frames\")\n",
    "\n",
    "    def train(self):\n",
    "        steps = 0\n",
    "        all_rewards = []\n",
    "        ep_times = [0.5]\n",
    "        training_loss = []\n",
    "        total_actions = {0: 0, 2: 0, 3: 0}\n",
    "        highscore = 0\n",
    "        self.get_frames()\n",
    "        start_time = time.time()\n",
    "        human_readable_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "        print(f\"Starting training at {human_readable_time}\")\n",
    "        write_to_file(f\"Starting training at {human_readable_time}\\n\")\n",
    "        for episode in range(1, NUM_EPISODES + 1):\n",
    "            # if PRIORITIZED_EXPERIENCE_REPLAY and episode != NUM_EPISODES and episode % RESET_PRIORITIES == 0:\n",
    "            #     self.buffer.priorities = deque([1] * len(self.buffer.priorities), maxlen=BUFFER_SIZE)\n",
    "            #     self.buffer.max_priority = 1\n",
    "\n",
    "            ep_start = time.time()\n",
    "            ep_replay = []\n",
    "            actions = {0: 0, 2: 0, 3: 0}\n",
    "            done = False\n",
    "            ep_reward = 0\n",
    "            lives = 5\n",
    "            state = self.env.reset()\n",
    "            for _ in range(NUM_FRAMES):\n",
    "                processed_state, proc_time = self.preprocess_state(state)\n",
    "                self.frame_buffer.append(processed_state)\n",
    "            state = np.stack(self.frame_buffer, axis=-1)\n",
    "            training_time = 0\n",
    "            total_actions_time = 0\n",
    "            step_times = 0\n",
    "            procs_time = 0\n",
    "            self.env.step(1)\n",
    "            while not done:\n",
    "                action_time = time.time()\n",
    "                action = self.choose_action(state)\n",
    "                total_actions_time += time.time() - action_time\n",
    "                actions[action] += 1\n",
    "                total_actions[action] += 1\n",
    "                step_time = time.time()\n",
    "                next_state, reward, done, _, info = self.env.step(action)\n",
    "                step_times += time.time() - step_time\n",
    " \n",
    "                if action != 0:\n",
    "                    action -= 1\n",
    "                processed_state, proc_time = self.preprocess_state(next_state)\n",
    "                procs_time += proc_time\n",
    "                self.frame_buffer.append(processed_state)\n",
    "                next_state = np.stack(self.frame_buffer, axis=-1)\n",
    "                steps += 1\n",
    "                # if steps % SHOW_FRAME == 0:\n",
    "                #     self.show_frame(processed_state)\n",
    "                    # time.sleep(0.5)\n",
    "                if steps % TRAINING_FREQ == 0 and TRAINING:\n",
    "                    learn_time, loss = self.learn(episode, steps, np.round(np.mean(all_rewards[-10:]), 1))\n",
    "                    training_time += learn_time\n",
    "                    if steps > TARGET_UPDATE_FREQ*2:\n",
    "                        training_loss.append(loss)\n",
    "                ep_reward += reward\n",
    "                if reward > 0:\n",
    "                    reward = 1\n",
    "                    # maybe take the log of the reward instead\n",
    "                    # reward = np.log(reward)\n",
    "                if (info[\"lives\"] < lives):\n",
    "                    lives = info[\"lives\"]\n",
    "                    # print(reward)\n",
    "                    # reward = -1\n",
    "                    self.env.step(1)\n",
    "                    self.remember(state, action, reward, next_state, True)\n",
    "                elif TRAINING:\n",
    "                    self.remember(state, action, reward, next_state, done)\n",
    "                ep_replay.append((state, action, reward, next_state, done))\n",
    "                state = next_state\n",
    "\n",
    "                if steps % self.target_update_freq == 0:\n",
    "                    self.update_target_network()\n",
    "                if steps % ADJUST_TARGET_UPDATE_FREQ_EVERY == 0 and self.target_update_freq < MAX_TARGET_UPDATE_FREQ:\n",
    "                    self.target_update_freq += TARGET_UPDATE_RATE\n",
    "                if done:\n",
    "                    logging_time = time.time()\n",
    "                    all_rewards.append(ep_reward)\n",
    "                    if ep_reward > highscore:\n",
    "                        highscore = ep_reward\n",
    "                        # for i in range(5):\n",
    "                        #     for exp in ep_replay:\n",
    "                        #         self.remember(*exp)\n",
    "                        save_ep_replay_to_file(ep_replay, f'{SAVE_PATH}replays/ep_replay_{episode}_score_{highscore}.pkl')\n",
    "                    seconds = round(time.time() - start_time)\n",
    "                    elapsed_time = round(time.time() - start_time, 1)\n",
    "                    ep_times.append(time.time() - ep_start)\n",
    "                    \n",
    "                    hours = seconds // 3600\n",
    "                    minutes = (seconds % 3600) // 60\n",
    "                    seconds = round(seconds % 60)// 1\n",
    "\n",
    "                    avgTime = round(elapsed_time / episode, 1)\n",
    "                    ep_time = round(time.time() - ep_start, 1)\n",
    "                    avg100time = np.round(np.mean(ep_times[-100:]), 1)\n",
    "                    avg1000time = np.round(np.mean(ep_times[-1000:]), 1)\n",
    "\n",
    "                    avg10 = np.round(np.mean(all_rewards[-10:]), 1)\n",
    "                    avg50 = np.round(np.mean(all_rewards[-50:]), 1)\n",
    "                    avg100 = np.round(np.mean(all_rewards[-100:]), 1)\n",
    "                    avg300 = np.round(np.mean(all_rewards[-300:]), 1)\n",
    "                    avg500 = np.round(np.mean(all_rewards[-500:]), 1)\n",
    "                    avg1000 = np.round(np.mean(all_rewards[-1000:]), 1)\n",
    "                    avg5000 = np.round(np.mean(all_rewards[-5000:]), 1)\n",
    "                    avg = np.round(np.mean(all_rewards), 1)\n",
    "\n",
    "                    lossAvg = np.mean(training_loss)\n",
    "                    loss100 = np.mean(training_loss[-100:])\n",
    "                    loss1K = np.mean(training_loss[-1_000:])\n",
    "                    loss10K = np.mean(training_loss[-10_000:])\n",
    "                    loss50K = np.mean(training_loss[-50_000:])\n",
    "                    loss100K = np.mean(training_loss[-100_000:])\n",
    "\n",
    "                    actions_per_episode = actions[0] + actions[2] + actions[3]\n",
    "                    actions_per_training = total_actions[0] + total_actions[2] + total_actions[3]\n",
    "                    if total_actions_time != 0:\n",
    "                        actions_per_second = round(actions_per_episode / total_actions_time)\n",
    "                    else:\n",
    "                        actions_per_second = 'lots'\n",
    "\n",
    "                    output_str = f\"Episode {episode}/{NUM_EPISODES}, Highscore: {highscore}, Reward: {ep_reward}, Epsilon: {round(self.epsilon, 3)}, LR: {round(float(self.optimizer.learning_rate), 6)}, NAME: {SAVE_PATH}\\n\"\n",
    "                    output_str += f\"Avg10: {avg10}, Avg50: {avg50}, Avg100: {avg100}, Avg300: {avg300}, Avg500: {avg500}, avg1000: {avg1000}, avg5000: {avg5000}, Average: {avg}\\n\"\n",
    "                    output_str += f\"beta: {getattr(self, 'beta', 'N/A')}, alpha: {getattr(self.buffer, 'alpha', 'N/A')}, target_update_freq: {self.target_update_freq}\\n\"\n",
    "                    output_str += f\"loss100: {loss100:.6f}, loss1K: {loss1K:.6f}, loss10K: {loss10K:.6f}, loss50K: {loss50K:.6f}, loss100K: {loss100K:.6f}, lossAvg: {lossAvg:.6f}\\n\"\n",
    "                    output_str += f\"Total time: {hours:02d}:{minutes:02d}:{seconds:02d}, Episode time: {ep_time}s, Avg100: {avg100time}s, avg1000time: {avg1000time}s, Average time: {avgTime}s\\n\"\n",
    "                    output_str += f\"No op: {actions[0]}/{total_actions[0]}, Left: {actions[3]}/{total_actions[3]}, Right: {actions[2]}/{total_actions[2]}, Total: {actions_per_episode}/{actions_per_training}, memory size: {len(self.buffer)}\\n\"\n",
    "                    output_str += f\"Training time: {round(training_time, 1)}s, \"\n",
    "                    output_str += f\"Action time: {round(total_actions_time, 1)}s, Actions per second: {actions_per_second}, \"\n",
    "                    output_str += f\"Step time: {round(step_times, 1)}s, \"\n",
    "                    output_str += f\"Preprocessing time: {round(procs_time, 1)}s, \"\n",
    "                    output_str += f\"Total time: {round(training_time + total_actions_time + step_times + procs_time, 1)}/{round(time.time() - ep_start, 1)}s\\n\"\n",
    "                    if (episode % SAVE_FREQ == 0 or episode == 10) and TRAINING:\n",
    "                        if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "                            output_str += str(np.array(self.buffer.priorities)[-300:]) + \"\\n\"\n",
    "                            output_str += f\"max_priority: {self.buffer.max_priority}\\n\"\n",
    "                        \n",
    "                        output_str += f\"Model weights saved at episode {episode}\\n\"\n",
    "                        self.q_network.save_weights(f\"{SAVE_PATH}models/model_{episode}.h5\")\n",
    "                        time_remaining = estimate_remaining_time(NUM_EPISODES, ep_times)\n",
    "                        estimated_score = estimate_score(NUM_EPISODES, all_rewards)\n",
    "                        finish_time = datetime.now() + timedelta(seconds=time_remaining)\n",
    "                        finish_time = finish_time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        output_str += f\"\\nEstimated finish time: {finish_time}, which is in {round(time_remaining/60)}mins\\n\"\n",
    "                        output_str += f\"Estimated avg100 score: {estimated_score}\\n\"\n",
    "\n",
    "                        with open(f\"{SAVE_PATH}rewards.txt\", \"w\") as f:\n",
    "                            f.write(str(all_rewards))\n",
    "                        with open(f\"{SAVE_PATH}loss.txt\", \"w\") as f:\n",
    "                            f.write(str(training_loss))\n",
    "                        with open(f\"{SAVE_PATH}ep_times.txt\", \"w\") as f:\n",
    "                            f.write(str(ep_times))\n",
    "                    if episode % EMAIL_FREQUENCY == 0:\n",
    "                        send_email_notification(all_rewards, output_str)\n",
    "                    write_to_file(f\"{output_str}\\n\")\n",
    "                    if LOGGING:\n",
    "                        print(output_str, end='')\n",
    "                        print(f\"Logging time: {round(time.time() - logging_time, 1)}s\\n\")\n",
    "\n",
    "            if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "                self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "            if episode > 10_000:\n",
    "                self.epsilon = decay(0.30, EPSILON_DECAY, episode - 10_000, 2_500, EPSILON_MIN)\n",
    "            elif episode > 7_500:\n",
    "                self.epsilon = decay(0.25, EPSILON_DECAY, episode - 7_500, 2_500, EPSILON_MIN)\n",
    "            elif episode > 5_000:\n",
    "                self.epsilon = decay(0.35, EPSILON_DECAY, episode - 5_000, 2_500, EPSILON_MIN)\n",
    "            elif episode > 2_500:\n",
    "                self.epsilon = decay(0.50, EPSILON_DECAY, episode - 2_500, 2_500, EPSILON_MIN)\n",
    "            else:\n",
    "                self.epsilon = decay(EPSILON_START, EPSILON_DECAY, episode, 2_500, EPSILON_MIN)\n",
    "            # self.epsilon = decay(EPSILON_START, EPSILON_DECAY, episode, NUM_EPISODES, EPSILON_MIN)\n",
    "            self.optimizer.learning_rate = decay(INITIAL_LEARNING_RATE, LEARNING_RATE_DECAY, episode, NUM_EPISODES, MIN_LEARNING_RATE)\n",
    "\n",
    "\n",
    "# Function to write the graph\n",
    "def write_graph(model, input_shape):\n",
    "    tf.summary.trace_on(graph=True, profiler=False)\n",
    "    # Dummy forward pass for graph tracing\n",
    "    dummy_input = tf.random.normal([1, *input_shape])\n",
    "    model(dummy_input)\n",
    "    with file_writer.as_default():\n",
    "        tf.summary.trace_export(name=\"model_trace\", step=0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tf.random.set_seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "\n",
    "    if not os.path.exists(SAVE_PATH):\n",
    "        os.makedirs(SAVE_PATH)\n",
    "        os.makedirs(f\"{SAVE_PATH}models\")\n",
    "        os.makedirs(f\"{SAVE_PATH}replays\")\n",
    "        if PRIORITIZED_EXPERIENCE_REPLAY:\n",
    "            os.makedirs(f\"{SAVE_PATH}indices\")\n",
    "    else:\n",
    "        print(f\"\\nThe directory {SAVE_PATH.split('/')[-2]} already exists.\\n\")\n",
    "        exit()\n",
    "\n",
    "    log_dir = \"G:/Coding/breakout/testing_prioritized\" + \"/tensorboard/\" + RUN\n",
    "    # tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1, write_graph=True)\n",
    "    file_writer = tf.summary.create_file_writer(log_dir)\n",
    "    file_writer.set_as_default()\n",
    "\n",
    "    np.set_printoptions(suppress=True)\n",
    "    tf.config.optimizer.set_jit(True)\n",
    "    devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "    print(devices)\n",
    "    # tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "    \n",
    "    agent = RainbowAgent()\n",
    "    write_graph(agent.q_network, input_shape=[FRAME_WIDTH, FRAME_HEIGHT, NUM_FRAMES]) \n",
    "    agent.train()\n",
    "    human_readable_time = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    print(f\"Finished training at {human_readable_time}\")\n",
    "    write_to_file(f\"Finished training at {human_readable_time}\\n\")\n",
    "    print(\"\\a\")\n",
    "\n",
    "    # Next steps\n",
    "    \"\"\" \n",
    "    - Run 1 with double the buffer size\n",
    "    - double the batch size\n",
    "    - quadruple the batch size\n",
    "    - reduce the max target update freq maybe to 7000\n",
    "    - use log base 3 for reward but first check what \n",
    "    - reduce buffer size to 300_000\"\"\"\n",
    "\n",
    "    # Notes\n",
    "    # Convert to notebook then I can check vars after and I can run it intermittently then check whats going on\n",
    "    # Change minbuffer to 150_000 thats when it got to 40 average after 5000 episodes run 56\n",
    "    # Use LeakyReLU instead of ReLU especially for the output layer so that negative rewards can make it through for \n",
    "    # when dying, then give -1 for dying\n",
    "    # Figure out how to monitor gradients in order to know if they are exploding or vanishing\n",
    "    # Add graph to tensorboard\n",
    "    # Check why regularization is not working\n",
    "    # In general tweak and play with Tensorboard\n",
    "    # For PER use GAMMA = 0.99 and for n_steps use GAMMA = 0.95\n",
    "    # For prioritized experience replay, remove the reset of the priorities\n",
    "    # Add highscore experiences to buffer multiple times\n",
    "    # no more logging just log to file and use tqdm\n",
    "    # Make the epsilon decay slower\n",
    "    # Make sure that the model I am using is actually the right one\n",
    "    # The rewards are 1, 4, and 7. Change that to 1, 2, and 3\n",
    "    # Multistep and dueling dqn\n",
    "    # Use tensorboard for logging and graphing the loss and rewards over time and the model architecture and the model weights and the replay buffer\n",
    "    # Try again a negative reward for losing a life\n",
    "    # Break up this file by having all non training or model stuff in a different file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
